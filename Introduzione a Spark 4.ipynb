{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e85a42",
   "metadata": {},
   "source": [
    "<font size=\"8\">**Introduzione a Spark 4 (con Python)**</font><br>\n",
    "\n",
    "> (c) 2025 Antonio Piemontese "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b81a63",
   "metadata": {},
   "source": [
    "---\n",
    "Significato delle **icone** del notebook:\n",
    "\n",
    "üü• Errore ‚Üí un quadrato rosso con ‚ùå\n",
    "\n",
    "üü© OK ‚Üí un cerchio verde con ‚úî\n",
    "\n",
    "üü¶ Suggerimento ‚Üí un fumetto blu üí¨\n",
    "\n",
    "üü® Da chiarire ‚Üí un triangolo giallo con ‚ùì\n",
    "\n",
    "üü™ Idea ‚Üí lampadina viola üí°\n",
    "\n",
    "üî¥ Punto importante ‚Üí un cerchio rosso con ‚ùó\n",
    "\n",
    "Set **numeri**: 0Ô∏è‚É£, 1Ô∏è‚É£, 2Ô∏è‚É£,  3Ô∏è‚É£,  4Ô∏è‚É£, 5Ô∏è‚É£, 6Ô∏è‚É£,  7Ô∏è‚É£, 8Ô∏è‚É£, 9Ô∏è‚É£, üîü \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f15f53",
   "metadata": {},
   "source": [
    "# I big data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f6854",
   "metadata": {},
   "source": [
    "Cosa sono i big data?\n",
    "* [La voce Wikipedia](https://it.wikipedia.org/wiki/Big_data)\n",
    "* [Le famose 3V](big_data_3V.png)\n",
    "\n",
    "Una definizione pratica a 3 livelli √® [questa](Table_1_1.png). E' una buona regola del pollice.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a721427",
   "metadata": {},
   "source": [
    "Python ed R scalano male al crescere dei dati da gestire.<br>\n",
    "Librerie come *pandas* non sono progettate per dataset che non stanno in memoria.<br>\n",
    "D'altra parte il [computing distribuito](https://en.wikipedia.org/wiki/Distributed_computing) non √® banale:\n",
    "* i dati devono essere partizionati tra i differenti nodi del cluster; quindi gli algoritmi che utilizzano molto i dati patiscono le **latenze di rete** e comunque i **network transfer rate** sono inferiori di diversi ordini di grandezza rispetto agli accessi in RAM;\n",
    "* mano a mano che il numero di nodi coinvolti in un processo cresce, la probabilit√† di **failure** di un nodo (e quindi del processo) cresce in modo proporzionale;\n",
    "* l'applicazione deve essere progettata sin dall'inizio in modo **parallelizzabile** (sia come codice che come dati)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca8c61",
   "metadata": {},
   "source": [
    "Un modo semplice per immaginare i sistemi distribuiti √® come un gruppo di computer indipendenti che appaiono all'utente finale come un singolo computer. Consentono lo **scaling orizzontale**. Ci√≤ significa aggiungere pi√π computer/cpu anzich√© fare upgrade del singolo sistema (**scaling verticale**). Quest'ultimo √® relativamente costoso e spesso insufficiente per grandi carichi di lavoro. I sistemi distribuiti sono ottimi in termini di scalabilit√† ed affidabilit√†, ma rendono **pi√π complesse** la progettazione, l'implementazione ed il debug del codice. E' importante comprendere **questo compromesso** prima di optare per uno strumento del genere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943df38",
   "metadata": {},
   "source": [
    "# Soluzioni per big data in Python\n",
    "\n",
    "![](cudf_vs_pyspark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beff9b70",
   "metadata": {},
   "source": [
    "# La framework Apache SPark\n",
    "\n",
    "Apache Spark √® una **framework open-source** nata a Berkeley, e poi donata alla fondazione Apache.<br>\n",
    "Il nome (`Spark` = *scintilla*) vuole rimarcare gli obiettivi iniziali del progetto: **computazione veloce ed interattiva**.\n",
    "\n",
    "---\n",
    "\n",
    "‚ùó Prima major release nel 2016, la major release **Spark** 3 √® uscita nel 2020, **a maggio del 2025 √® uscito Spark 4**, **a settembre 2025 √® uscito 4.0.1**, vedi [qui](https://spark.apache.org/downloads.html).<br>\n",
    "Sempre da settembre 2025 √® disponibile la **3.5.7.**, una evoluzione della consolidata serie 3.5.<br>\n",
    "Dal sito Spark (a novembre 2025):\n",
    "\n",
    "![](spark_releases.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c901e337",
   "metadata": {},
   "source": [
    "Apache Spark √® stata la **prima framework open-source** che ha reso la programmazione distribuita realmente fruibile ai data scientist.<br>\n",
    "Ecco le sue componenti:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f05d79",
   "metadata": {},
   "source": [
    "![](Figure_1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d95105",
   "metadata": {},
   "source": [
    "Vediamo l'architettura di *Spark* pi√π in dettaglio, con <u>l'aggiunta delle possibili sorgenti dati e dei possibili environment</u> (vedi pi√π avanti):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44cf4f6",
   "metadata": {},
   "source": [
    "![](spark_ecosystem3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614b369",
   "metadata": {},
   "source": [
    "dove:\n",
    "* *Spark SQL and DataFrames + Datasets*: il modulo per lavorare con dati strutturati (tabellari)\n",
    "* *MLlib*: una libreria di Machine Learning scalabile\n",
    "* *Structured Streaming*: per realizzare applicazioni in streaming scalabili e fault-tolerant\n",
    "* *GraphX (legacy)*: la libreria di Apache Spark per l'uso dei [grafi](https://it.wikipedia.org/wiki/Grafo). Per le applicazioni di [graph analytics](https://www.nvidia.com/en-us/glossary/data-science/graph-analytics/#:~:text=Graph%20analytics%2C%20or%20Graph%20algorithms,the%20graph%20as%20a%20whole.), comunque, si raccomanda l'uso di [*GraphFrames*](https://graphframes.github.io/graphframes/docs/_site/index.html) anzich√® di GraphX, che non √® mantenuto in modo attivo ed √® carente per Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d47c3",
   "metadata": {},
   "source": [
    "Altre fonti:<br>\n",
    "[Spark secondo Apache](https://spark.apache.org/)<br>\n",
    "[Spark secondo Wikipedia](https://it.wikipedia.org/wiki/Apache_Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cfa3cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Come riportato da Wikipedia IT:\n",
    "> A differenza del paradigma `MapReduce`, il predecessore di Spark, basato sul disco a due livelli di `Hadoop`, le **primitive \"in-memory\" multilivello** di Spark forniscono prestazioni fino **a 100 volte migliori** per talune applicazioni<sup>1</sup>.\n",
    "\n",
    "<small><sup>1</sup>*Reynold Xin, Josh Rosen, Matei Zaharia, Michael Franklin, Scott Shenker e Ion Stoica, Association for Computing Machinery, Shark: SQL and Rich Analytics at Scale (PDF), SIGMOD '13, New York, 22-27 giugno 2013. URL consultato il 29 agosto 2017 (archiviato dall'url originale il 9 agosto 2017).*</small>\n",
    "\n",
    "Spark vs [MapReduce](https://it.wikipedia.org/wiki/MapReduce):\n",
    "* [Hadoop](https://it.wikipedia.org/wiki/Apache_Hadoop) √® un <u>file system distribuito per big data</u>; MapReduce di Apache Hadoop ha rivoluzionato la elaborazione dei grandissimi dataset (*huge dataset*) offrendo un <u>modello</u> semplice e flessibile per scrivere programmi che possono essere **eseguiti in parallelo su centinaia o migliaia di macchine**. MapReduce divide il task in piccoli task che possono essere eseguiti su macchine differenti e la cui failure pu√≤ essere gestita comodamente.\n",
    "* Spark estende MapReduce per 3 aspetti: l'engine esegue un [grafo aciclico diretto](https://en.wikipedia.org/wiki/Directed_acyclic_graph) degli operatori (sul tipo di TensorFlow - vedi [questo esempio](DAG_example.png) relativo alla formula: [(a+b) x c] + [(a+b) + e]); offre un set di trasformazioni pi√π ampio; permette l'elaborazione **in memoria**\n",
    "* inoltre, Spark <u>disaccoppia lo storage e l'elaborazione</u> (a differenza di Hadoop); cio√®, possiamo usare Spark per leggere dati memorizzati su molte sorgenti: Apache Hadoop, [Apache Cassandra](https://it.wikipedia.org/wiki/Cassandra_(database)), [Apache Hbase](https://it.wikipedia.org/wiki/HBase), [MongoDB](https://it.wikipedia.org/wiki/MongoDB), [Apache Hive](https://it.wikipedia.org/wiki/Apache_Hive), RDBMS, ecc. Spark gestisce tutte queste sorgenti dati *in-memory*!\n",
    "* tramite le [API](https://it.wikipedia.org/wiki/Application_programming_interface) *DataFrameReader* e *DataFrameWriter*, inoltre, permettono di accedere anche altre sorgenti dati, come: [Apache Kafka](https://it.wikipedia.org/wiki/Apache_Kafka), [Amazon Kinesis](https://en.wikipedia.org/wiki/Amazon_Kinesis), Azure Storage, [Amazon S3](https://it.wikipedia.org/wiki/Amazon_S3). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c74804",
   "metadata": {},
   "source": [
    "# L'ecosistema Spark\n",
    "\n",
    "Intorno a Spark √® cresciuto negli anni un **ecosistema**, fatto di *community* di utenti e sviluppatori, di software di terze parti, di conferenze e meet-up locali, ecc. Seguono alcune slide di descrizione di tale ecosistema (per la parte software):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28edd1",
   "metadata": {},
   "source": [
    "![](spark_ecosystem2.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2146e3d7",
   "metadata": {},
   "source": [
    "![](spark_ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9ea4a",
   "metadata": {},
   "source": [
    "# Le release di Spark\n",
    "\n",
    "**Spark Release 3.0**:<br>\n",
    "* non introduce importanti cambiamenti funzionali alle API (rispetto alla versione 2);\n",
    "<br><br>\n",
    "* si focalizza sulla <u>performance ed usabilit√†</u>, che sono migliorate;\n",
    "<br><br>\n",
    "* il modulo maggiormente migliorato √® *SparkSQL*: l'esecuzione adattiva delle query ed il pruning dinamico di partizione permettono a Spark di adattare <u>in tempo reale il query plan</u> e <u>saltare i dati</u> che non sono richiesti come risultato della query;\n",
    "<br><br>\n",
    "* questi miglioramenti fanno risparmiare agli utenti parecchio tempo nel tuning della query ed ottimizzazione del query plan;\n",
    "<br><br>\n",
    "* Spark 3.0 √® circa <u>2 volte pi√π veloce di Spark 2.4</u> nell'esecuzione del benchmark di elaborazione analitica [TPC-DS](https://www.tpc.org/tpcds/);\n",
    "<br><br>\n",
    "* poich√® la maggior parte delle applicazioni Spark utilizzano l'engine SQL, ne traggono beneficio tutte le librerie di alto livello, includendo MLlib e lo streaming strutturato, e tutte le API, includendo SQL e DataFrames;\n",
    "<br><br>\n",
    "* Spark 3.0 √® conforme allo [standard ANSI SQL](https://blog.ansi.org/2018/10/sql-standard-iso-iec-9075-2016-ansi-x3-135/#gref), e cio√® lo rende pi√π usabile;\n",
    "<br><br>\n",
    "* Spark √® progettato per i big data, non per l'estetica. Molti output sono essenziali e poco attraenti dal punto di vista grafico. All'estremo opposto ci sono tool come [*Orange*](https://orangedatamining.com/), eccellenti per la parte grafica ma deboli con volumi di quantit√† medio-grandi;\n",
    "Spark & Java Virtual Machine\n",
    "<br><br>\n",
    "* *[Java Virtual Machine (JVM)](https://it.wikipedia.org/wiki/Macchina_virtuale_Java) can be considered as middleware between Java-based applications like Spark and operating systems where it is running. As Spark is written mostly in Scala, it can not work without JVM on the computer where it is running.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1d0871",
   "metadata": {},
   "source": [
    "**Spark Release 4.0.0**\n",
    "\n",
    "Apache Spark 4.0.0 segna **una pietra miliare** importante come primo rilascio della **serie 4.x**, che incarna lo sforzo collettivo della vivace community open-source. Questo rilascio √® la prova di una collaborazione straordinaria: sono stati risolti **oltre 5100 ticket con contributi da pi√π di 390 persone**.\n",
    "\n",
    "***Spark Connect*** continua ad avanzare rapidamente, offrendo miglioramenti significativi:\n",
    "\n",
    "* Un nuovo client Python leggero (**pyspark-client**) di soli 1,5 MB.\n",
    "* Un ulteriore pacchetto di rilascio con Spark Connect abilitato di default.\n",
    "* Piena compatibilit√† API per il client Java.\n",
    "* Una nuova configurazione `spark.api.mode` per attivare/disattivare facilmente Spark Connect nelle tue applicazioni.\n",
    "* Copertura API notevolmente ampliata.\n",
    "* ML su Spark Connect.\n",
    "* Una nuova implementazione di client per Swift.\n",
    "\n",
    "***Spark SQL*** √® stato arricchito in modo significativo con nuove potenti funzionalit√† progettate per aumentare l‚Äôespressivit√† e la versatilit√† dei carichi SQL, come il supporto al tipo di dato **VARIANT**, le funzioni SQL definite dall‚Äôutente, le variabili di sessione, la sintassi *pipe* e la collazione delle stringhe.\n",
    "\n",
    "***PySpark*** continua a ricevere attenzione sia in termini di ampiezza funzionale sia di esperienza d‚Äôuso per gli sviluppatori, introducendo **una API nativa per il plotting, una nuova Python Data Source API, il supporto alle UDTF Python e il profiling unificato per le UDF PySpark**, insieme a numerosi altri miglioramenti.\n",
    "\n",
    "***Structured Streaming*** evolve con aggiunte chiave che offrono maggiore controllo e facilit√† di debugging, in particolare l‚Äôintroduzione della **Arbitrary State API v2** per una gestione dello stato pi√π flessibile e della **State Data Source** per un debugging pi√π semplice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b683e8f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**E' meglio usare la 4.0.1 o la 3.5.7.?** (*chatGPT5*)\n",
    "\n",
    "Dipende da dove si mette Spark, ma ‚Äúin generale, oggi‚Äù la risposta √® questa:\n",
    "\n",
    "1. **Se si parte da zero e si pu√≤ scegliere liberamente ‚Üí 4.0.1.**\n",
    "   √à l‚Äôultima stabile del ramo nuovo, hanno gi√† fatto il primo giro di bugfix, e tutta la documentazione ‚Äúlatest‚Äù ora punta l√¨. <sup>1</sup>\n",
    "\n",
    "2. **Se si √® in un ambiente gi√† in produzione che √® rimasto su 3.5.x ‚Üí 3.5.7.**\n",
    "   √à il maintenance pi√π recente del ramo 3.5 e serve proprio a chi non pu√≤/pensa di non poter fare subito il salto al 4.x (compatibilit√†, vendor, cluster vecchi, ecc.).<sup>2</sup>\n",
    "\n",
    "3. **Cosa guardare per decidere davvero**\n",
    "\n",
    "   * connettori / librerie di terze parti: supportano gi√† 4.0?\n",
    "   * runtime dove lo si deve mettere (Dataproc, EMR, Kubernetes operator): hanno gi√† l‚Äôimmagine 4.0.x?\n",
    "   * team: user√† Spark Connect / le novit√† SQL di 4.0? Se s√¨, ha senso andare dritto l√¨. <sup>3</sup>\n",
    "\n",
    "Quindi versione consigliata ‚Äúoggi‚Äù: **Apache Spark 4.0.1**, salvo vincoli di piattaforma.\n",
    "\n",
    "<small><sup>1</sup>*https://spark.apache.org/downloads.html?utm_source=chatgpt.com*</small><br>\n",
    "<small><sup>2</sup>*https://spark.apache.org/releases/spark-release-3-5-7.html?utm_source=chatgpt.com*</small><br>\n",
    "<small><sup>3</sup>*https://spark.apache.org/releases/spark-release-4-0-1.html?utm_source=chatgpt.com*</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebbe6a",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "[**PySpark**](https://spark.apache.org/docs/latest/api/python/) √® l'API di Spark per Python, ed √® quella che useremo per il codice degli esempi di questo corso.<br>\n",
    "E' un <u>wrapper Python</u> della framework Spark, √® scritto principalmente in [Scala](https://it.wikipedia.org/wiki/Scala_(linguaggio_di_programmazione)), che, come noto, [gira sulla JVM](https://www.educative.io/courses/learn-scala-from-scratch/39BnN6DMZxr).<br>\n",
    "\n",
    "![](icone_pyspark.png)\n",
    "\n",
    "* Python √® il linguaggio pi√π usato nella Data Science e nel Machine Learning; di conseguenza, PySpark √® la API di Spark pi√π usata;\n",
    "<br><br>\n",
    "* su [PyPI](https://pypi.org/) (il Python Package Index, l'equivalente del [CRAN](https://cran.r-project.org/) in R), PySpark ha pi√π di 5 milioni di download al mese;\n",
    "<br><br>\n",
    "* le versioni Python prima della 3.6 sono state deprecate; da Spark 3.2, anche la versione Python 3.6 √® stata deprecata (come supporto); verificare la versione di Python installata sul proprio PC con il comando: *import sys; print(sys.version)*\n",
    "<br><br>\n",
    "* PySpark permette di gestire al meglio gli aspetti critici della analitica complessa su *huge data*, in particolare: il **pre-processing** (ne vedremo un esempio); **le iterazioni**, usate da molti algoritmi di Machine Learning e Deep Learning (ad esempio la [discesa stocastica del gradiente](https://it.wikipedia.org/wiki/Discesa_stocastica_del_gradiente)); la **pipeline** dei modelli (dal pre-processing alla valutazione del modello);\n",
    "<br><br>\n",
    "* si pu√≤ utilizzare un ambiente [REPL](https://it.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop), evitando l'andata-ritorno con le IDE (ad esempio *Jupyter* oppure *PyCharm*) oppure il loop edit-compile-run-debug. *Un read-eval-print loop √® un ambiente di programmazione dove gli input utente sono letti ed elaborati, e quindi i risultati sono restituiti all'utente*. Per REPL vs Interprete, vedi [qui](https://stackoverflow.com/questions/5451042/relation-between-repl-interpreter-and-compiler);\n",
    "<br><br>\n",
    "* il REPL si attiva da anaconda prompt con: *pyspark*; in questo corso, tuttavia, utilizzeremo prevalentemente Jupyter;\n",
    "<br><br>\n",
    "* PySpark fornisce **interoperabilit√† con *pandas*** (come vedremo pi√π avanti)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda10fe",
   "metadata": {},
   "source": [
    "***Spark*** vs ***PySpark*** vs ***SparkSQL***: non confondere (frequente!):\n",
    "* *Spark*: una framework di elaborazione distribuita scritta principalmente nel linguaggio Scala. La framework offre le [API](https://it.wikipedia.org/wiki/Application_programming_interface) per differenti linguaggi, costruite *on the top* di Spark\n",
    "* *PySpark*: la API di Spark per Python. E' un wrapper del core di Spark per Python\n",
    "* *SparkSQL*: un modulo Spark per l'elaborazione dei dati strutturati (tabellari), accedibile de tutte le API di Spark e dunque anche da PySpark (vedi il link precedente a *PySpark*). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6edfa0",
   "metadata": {},
   "source": [
    "# L'installazione di Spark e PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b332f9b",
   "metadata": {},
   "source": [
    "Ora **installiamo** *PySpark* sulla nostra macchina (da [qui](https://spark.apache.org/downloads.html)) e **configuriamolo** in modo da poter iniziare a eseguire l'analisi dei dati. E' un'attivit√† *una tantum*.\n",
    "\n",
    "Prima di installare spark, pyspark, python, java, ecc √® bene disinstallare e pulire tutto?<br>\n",
    "Non √® obbligatorio azzerare tutto prima di usare Spark/PySpark, ma se negli ultimi giorni ci sono stati molti tentativi, versioni miste di Python/Java, variabili d‚Äôambiente sovrascritte, una ‚Äúmezza formattazione‚Äù mirata aiuta molto a evitare problemi.\n",
    "\n",
    "Vedi Word \"Disinstallazione preliminare\".\n",
    "\n",
    "---\n",
    "1Ô∏è‚É£ **Le versioni di *Spark*** oggi disponibili:<br>\n",
    "\n",
    "![](spark_versions.png)\n",
    "\n",
    "‚ùó Scegliamo0 la **4.0.1.** per sfruttare i vantaggi della versione 4 gi√† assestata 6 mesi dopo nella 4.0.1.\n",
    "\n",
    "---\n",
    "\n",
    "2Ô∏è‚É£ **I tipi di package** disponibili (cio√® per quali versioni di hadoop Spark √® stato compilato):<br>\n",
    "\n",
    "![](package_types.png)\n",
    "\n",
    "Vediamo il loro significato:\n",
    "\n",
    "**1. Pre-built for Apache Hadoop 3.4 and later**\n",
    "\n",
    "Questo √® il pacchetto gi√† compilato e pronto per essere usato con Hadoop 3.4 o versioni successive.<br>\n",
    "L‚Äôutente lo pu√≤ scaricare e avviare senza dover compilare nulla. **√à l‚Äôopzione ‚Äústandard‚Äù** per chi non ha esigenze particolari.\n",
    "\n",
    "**Serve quando:** l‚Äôutente vuole far partire Spark rapidamente in un ambiente recente.\n",
    "\n",
    "**2. Pre-built for Apache Hadoop 3.4 and later with Spark Connect enabled**\n",
    "\n",
    "√à lo stesso pacchetto di prima, ma con **Spark Connect gi√† abilitato**.<br>\n",
    "In questo modo l‚Äôutente pu√≤ usare i client leggeri (Python, Swift, ecc.) senza dover cambiare configurazioni o ricompilare.\n",
    "\n",
    "**Serve quando:** l‚Äôutente sa gi√† che user√† Spark Connect e vuole tutto pronto.\n",
    "\n",
    "**3. Pre-built with user-provided Apache Hadoop**\n",
    "\n",
    "Anche questo √® precompilato, ma **non porta dentro le librerie Hadoop**: si aspetta che nel sistema siano gi√† presenti quelle del cluster/ambiente dell‚Äôutente (per esempio un Hadoop aziendale o di un vendor).<br>\n",
    "Cos√¨ si evitano conflitti di versione tra i jar di Spark e quelle del cluster.\n",
    "\n",
    "**Serve quando:** l‚Äôutente √® in un ambiente dove le librerie Hadoop le fornisce il cluster e non vanno sovrascritte.\n",
    "\n",
    "**4. Source Code**\n",
    "\n",
    "Qui viene fornito solo il **codice sorgente**. L‚Äôutente deve compilarlo (‚Äúbuildarlo‚Äù) per ottenere i pacchetti.<br>\n",
    "√à l‚Äôopzione pi√π flessibile, ma anche la pi√π tecnica.\n",
    "\n",
    "**Serve quando:** l‚Äôutente vuole una combinazione non standard (altra versione di Hadoop, feature particolari) o in azienda si richiede di costruire i binari internamente.\n",
    "\n",
    "**In pratica**:\n",
    "\n",
    "* **Non so / sto iniziando** ‚Üí *Pre-built for Apache Hadoop 3.4 and later*\n",
    "* **Voglio provare Spark Connect subito** ‚Üí *‚Ä¶with Spark Connect enabled*\n",
    "* **Ho un Hadoop ‚Äúdi casa‚Äù e non voglio conflitti** ‚Üí *Pre-built with user-provided Apache Hadoop*\n",
    "* **Sono un maniaco del build o devo allineare le versioni** ‚Üí *Source Code*\n",
    "\n",
    "\n",
    "‚ùó Scegliamo *Pre-built for Apache Hadoop 3.4 and later*.\n",
    "\n",
    "---\n",
    "\n",
    "3Ô∏è‚É£ Ora, scarichiamo il pacchetto Spark prima selezionato (√® nel formato compresso *tgz* <sup>1</sup>)<br>\n",
    "Quando il download √® terminato:\n",
    "1. copiamo il nome del pacchetto scaricato (ad esempio selezionando il pacchetto nella cartella *Downloads* --> tasto destro del mouse --> *Rinomina* --> Ctrl-C) - ad esempio: `spark-4.0.1-bin-hadoop3`\n",
    "2. sotto il path `C:/Utenti/<Utente>` - <u>o anche semplicemente sotto `C:/`</u> - creiamo una cartella di nome `spark-4.0.1-bin-hadoop3` (o quello che √®... <sup>2</sup>)\n",
    "3. nella cartella *Downloads* facciamo doppio click sul pacchetto TGZ scaricato (oppure apriamolo con *7-zip* oppure *WinRAR*) --> la finestra ci mostra ora una cartella con il pacchetto in formato *RAR*\n",
    "4. estraiamo il pacchetto nella cartella creata al passo 2\n",
    "5. in questa cartella facciamo ora doppio click sul pacchetto TAR (oppure apriamolo con *7-zip* oppure *WinRAR*) e quindi estraiamo i contenuti (nella cartella stessa). Dovremmo ottenere questo albero:<br>\n",
    "\n",
    "    <small><sup>1</sup>*E' un'estensione di file che indica un archivio compresso, comunemente usato in sistemi operativi come Linux e macOS. Se Windows non riconosce il tipo file: selezionare il file--> tasto destro del mouse --> \"Apri con\", scegliere *7-zip* oppure *WinRAR*, fare check su \"Usa sempre questa app per aprire i file .tgz\"*</small><br>\n",
    "    <small><sup>2</sup>*E' consigliabile che Spark sia installato in un percorso senza spazi, in genere all'inizio dell'albero. Gli script di Spark spesso soffrono con gli spazi.*</small><br>\n",
    "\n",
    "    ![](spark_tree.png)\n",
    "6. Puliamo il cammino, in questo modo (da Explorer):<br>\n",
    "\n",
    "    6.1. Rinominiamo la cartella pi√π interna in `spark`<br>\n",
    "    In questo momento il percorso potrebbe essere, ad esempio:\n",
    "\n",
    "    ```text\n",
    "    C:\\spark-4.0.1-bin-hadoop3\\spark\n",
    "    ```\n",
    "\n",
    "    6.2 Spostare la cartella in `C:\\`\n",
    "\n",
    "    - Selezionare la cartella `spark` appena rinominata.\n",
    "    - Clic destro ‚Üí \"Taglia\".\n",
    "    - Aprire \"Questo PC ‚Üí Disco locale (C:)\".\n",
    "    - Clic destro ‚Üí \"Incolla\".\n",
    "\n",
    "    La struttura attesa dovrebbe ora essere la seguente:\n",
    "    ```text\n",
    "    C:\\spark\n",
    "        bin\\\n",
    "        conf\\\n",
    "        data\\\n",
    "        ...\n",
    "    ```\n",
    "    In questo modo sar√† pi√π semplice impostare le variabili d‚Äôambiente e lanciare Spark.\n",
    "\n",
    "---\n",
    "\n",
    "4Ô∏è‚É£ **Verifica dell'integrit√† del file scaricato**\n",
    "- integrit√† **logica** tramite il *checksum*\n",
    "- integrit√† **logica** tramite le *signatures*\n",
    "\n",
    "Dalla pagina prima riportata vedi:<br>\n",
    "`Verify this release using the 4.0.1 signatures, checksums and project release KEYS by following **these procedures**`.\n",
    "\n",
    "In merito vedi [questa chat](https://chatgpt.com/share/6913a622-f948-8012-ad35-86387af37adb) pratica riassuntiva.\n",
    "\n",
    "---\n",
    "\n",
    "5Ô∏è‚É£ **Impostazione delle variabili di ambiente di Spark**\n",
    "\n",
    "E' importante adesso **dire a Windows dove √® Spark**.<br>\n",
    "Occorre aprire un prompt *cmd* dei comandi ed eseguire:<br>\n",
    "\n",
    "<pre>\n",
    "    set SPARK_HOME=C:\\spark<br>\n",
    "    set PATH=%SPARK_HOME%\\bin;%PATH%\n",
    "</pre>\n",
    "\n",
    "Ovviamente cambiare il percorso all'occorrenza.<br>\n",
    "La seonda riga aggiunge la cartella `C:\\spark\\bin` (cio√® la cartella con gli eseguibili di Spark) all‚Äôinizio della variabile `PATH` solo per quella finestra di comando. In pratica: dopo quella riga, quando nella stessa finestra si scrive `pyspark` o `spark-shell`, Windows riesce a trovarli perch√© guarda anche dentro `C:\\spark\\bin`. Senza quella riga, Windows direbbe ‚Äúcomando non riconosciuto‚Äù perch√© non sa dove stanno gli eseguibili di Spark.<br>\n",
    "Ovviamente √® meglio modificare la variabile di ambiente **in modo permanente** dalla finestra Windows delle **variabili di ambiente**.\n",
    "\n",
    "---\n",
    "\n",
    "6Ô∏è‚É£ **Installazione della Java Virtual Machine**\n",
    "\n",
    "Spark utilizza Java.<br>\n",
    "Infatti (da chatGPT 5):\n",
    "> Spark ha bisogno di Java perch√© **il suo motore √® JVM (Scala/Java)**; gli script di avvio non fanno altro che preparare il *classpath* e poi **invocare java ...**  . Python √® **solo un cliente della JVM**. Senza una JVM \"sensata\" (cio√® con la variabile di ambiente `JAVA_HOME` + *java* nel `PATH`) Spark non parte.\n",
    "\n",
    "Vediamo **la soluzione pulita**.\n",
    "1. Eliminare eventuali variabili d'ambiente (di sistema!) `JAVA_HOME` sbagliate (cio√® che puntano ad una cartella sbagliata oppure Java non √® proprio installato sul PC). Occcorre connettersi come *amministratore*.\n",
    "2. Installare un **JDK** (8, 11 o 17; oggi 11 o 17 va benissimo per Spark su Windows). I JDK di Oracle e Microsoft vanno bene, oppure una JDK open-source come [**Temurin 17 LTS di Adoptium**](https://adoptium.net/temurin/releases):\n",
    "   * **LTS** significa ‚ÄúLong-Term Support‚Äù: √® una versione pensata per restare stabile a lungo, senza cambiare di continuo le API.\n",
    "   * la build si chiama di solito **Eclipse Temurin 17**.\n",
    "   * √® una distribuzione **OpenJDK**, gratuita, molto usata anche in contesti aziendali/PA perch√© ha una licenza chiara.\n",
    "   * per far girare **Spark su Windows 10** o per sviluppo Java generale, Temurin 17 LTS va benissimo.\n",
    "   * di solito basta scaricare l‚Äô**installer MSI per Windows x64**, installarlo e poi puntare `JAVA_HOME` a quella cartella.\n",
    "   * Temurin 17 LTS di Adoptium √® una scelta ‚Äúclassica‚Äù, solida e compatibile.\n",
    "   * E' perfetta anche per **Windows 11**, l‚Äôinstallazione e la configurazione sono le stesse di Windows 10.<br>\n",
    "\n",
    "   ![](temurin_jdk.png)<br>\n",
    "\n",
    "   Richiede 304 MB, il wizard di Temurin installa in genere in `C:\\Users\\Utente\\AppData\\Local\\Programs\\Eclipse Adoptium\\jdk-17.0.17.10-hotspot\\`\n",
    "\n",
    "3. impostare la variabile di ambiente di <u>sistema</u> **JAVA_HOME** (quindi con i diritti di <u>amministratore</u>) a **quella** cartella, ad esempio:\n",
    "\n",
    "   ```text\n",
    "   JAVA_HOME = C:\\Users\\Utente\\AppData\\Local\\Programs\\Eclipse Adoptium\\jdk-17.0.17.10-hotspot\\\n",
    "   ```\n",
    "   oppure dalla finestra di sistema in modo permanente.\n",
    "\n",
    "4. nel PATH (di sistema o utente) aggiungere:\n",
    "\n",
    "```text\n",
    "      %JAVA_HOME%\\bin\n",
    "```\n",
    "\n",
    "5. aprire **un nuovo** `cmd` e controllare:\n",
    "\n",
    "   ```bat\n",
    "   java -version\n",
    "   echo %JAVA_HOME%\n",
    "   ```\n",
    "\n",
    "   devono entrambi rispondere con un **percorso reale** e **una versione Java**. Ad esempio:\n",
    "\n",
    "   ![](java_answer.png)\n",
    "\n",
    "   ---\n",
    "\n",
    "7Ô∏è‚É£ **L'installazione di Python**\n",
    "\n",
    "Serve la versione **3.11**, perch√® Spark ad oggi (novembre 2025) ha <u>problemi con le ultime versioni di Python, la 3.12, la 3.13 e la 3.14</u> (la 3.15 √® solo in preview).<br>\n",
    "> Spark 4.0.1 oggi dichiara ‚ÄúPython 3.9+‚Äù, ma in pratica il supporto reale si ferma alle versioni correnti stabili che il progetto ha testato (3.11, 3.12); con 3.13 su Windows molti stanno vedendo worker che esplodono o che non partono affatto. Ci sono gi√† discussioni su questo punto.\n",
    "\n",
    "Scaricare dal sito [python.org - Windows](https://www.python.org/downloads/windows/) scaricare **Python 3.11.9 - April 2, 2024 - Installer 64 bit**.\n",
    "\n",
    "Si ottiene il seguente *exe*: `python-3.11.9-amd64` --> doppio click --> mettere il check su **Add python.exe to PATH** --> premere l'opzione \"Install now\"\" (non customizzare!) --> l'installer installa python nella directory `C:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python311`.\n",
    "\n",
    "Ed ora dobbiamo **fissare le variabili <u>utente</u> di Windows** alla nuova versione di Python ora installata:\n",
    "\n",
    "```text\n",
    "   PYSPARK_PYTHON = C:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python311\\python.exe<br>\n",
    "   PYSPARK_DRIVER_PYTHON = C:\\Users\\Utente\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
    "```\n",
    "\n",
    "A cosa servono queste due variabili di ambiente?\n",
    "\n",
    "**1. PYSPARK_PYTHON**\n",
    "- Questa dice: ‚Äúquesto √® il Python che devono usare i worker‚Äù.\n",
    "- Quando Spark esegue un job (anche un semplice `sc.parallelize(...).count())`, fa partire dei processi Python separati. Se non gli si dice quale Python usare, lui prova a lanciare python ‚Äúalla cieca‚Äù e su Windows finisce spesso sul Python sbagliato (quello dello Store, quello di Anaconda non attivato, ecc.).\n",
    "Con `PYSPARK_PYTHON=...python.exe` invece i worker partono sempre con quel Python.\n",
    "\n",
    "**2. PYSPARK_DRIVER_PYTHON**\n",
    "- Questa dice: ‚Äúquesto √® il Python con cui voglio far girare il driver PySpark‚Äù cio√® la shell pyspark che vediamo noi (quella col prompt >>>)\n",
    "- Mettere lo stesso valore in entrambe le variabili fa s√¨ che:\n",
    "   - il driver usa Python 3.11\n",
    "   - i worker usano Python 3.11<br>\n",
    "\n",
    "   driver e worker parlano la stessa ‚Äúlingua‚Äù ‚Üí non si hanno tipici errori come: ‚ÄúPython worker exited‚Äù o \"Python version mismatch\".\n",
    "\n",
    "‚ùó Spark pretende che la versione Python del <u>driver e quella del worker</u> siano **le medesime, anche le minor**.<br>\n",
    "> **Ci√≤ √® essenziale per usare spark tramite CLI o submit** (ed evitare appunto i tipici errori visti prima).\n",
    "\n",
    "---\n",
    "\n",
    "8Ô∏è‚É£ **Le versioni Python con i notebook**\n",
    "\n",
    "Se <u>invece</u> usiamo **Python dal notebook** (Jupyter, VSC, ecc) le  due versioni di Python sono le seguenti:\n",
    "- i worker = Python lanciato dagli executors (**3.11**) - impostato al punto 7Ô∏è‚É£\n",
    "- il driver = il Python del **notebook/kernel** (**3.13**??), e **<u>NON</u>** quello impostato al passo 7Ô∏è‚É£ nella variabile `PYSPARK_DRIVER_PYTHON`\n",
    "\n",
    "> Perch√©??<br>\n",
    "> `PYSPARK_DRIVER_PYTHON` e `PYSPARK_PYTHON`:\n",
    "- in **CLI/spark-submit**: dicono a Spark quale Python usare per driver e per worker ‚Üí funzionano.\n",
    "- in notebook (Jupyter / VSC): il driver √® gi√† deciso (**√® il kernel**). Non possiamo cambiare la versione del kernel.\n",
    "\n",
    " Con i notebook c'√® quindi probabilmente un **mismatch**. \n",
    "\n",
    "**Soluzione classica**:<br>\n",
    "Occorre creare un **ambiente virtuale Python** sul PC (dalla CLI) e poi eseguire il notebook da l√¨, ad esempio in questo modo:\n",
    "```text\n",
    "   conda create -n py311 python=3.11 -y\n",
    "   conda activate py311\n",
    "   conda env list\n",
    "```\n",
    "\n",
    "e poi eseguire **dalla CLI, <u>nell'ambiente creato prima `py311`</u>**:\n",
    "```text\n",
    "   python.exe -m pip install --upgrade pip\n",
    "   pip install pyspark ipykernel\n",
    "   python -m ipykernel install --user --name py311 --display-name \"Python 3.11 (py311)\"\n",
    "   jupyter kernelspec list\n",
    "```\n",
    "\n",
    "‚ùó Attenzione, abbiamo creato **un kernel Jupyter**, disponibile in VSC (in alto a destra) sotto la voce *Select another kernel* --> *Jupyter kernel*\n",
    "\n",
    "---\n",
    "\n",
    "9Ô∏è‚É£ **L'attivazione di *Spark*, finalmente (da un terminale *cmd*)**:\n",
    "\n",
    "Ora **tutto √® stato  preparato** e possiamo attivare *pyspark/spark* da un **nuovo** *cmd* (come sempre quando si modificano le variabili di ambiente, come fatto al passo 6Ô∏è‚É£, in modo temporaneo con la *SET* o permanente):<br>\n",
    "\n",
    "   ```bat\n",
    "   cd C:\\spark\\bin\n",
    "   pyspark.cmd\n",
    "   ```\n",
    "\n",
    "   Si deve ottenere un messaggio come il seguente\n",
    "\n",
    "   ![](spark_first_message.png)\n",
    "\n",
    "   dove:\n",
    "   * `Banner Spark 4.0.1` ‚Üí significa che **la parte JVM √® partita e che JAVA_HOME √® quella giusta**.\n",
    "\n",
    "   * `‚ÄúUsing Python version 3.13.2 ‚Ä¶‚Äù` ‚Üí PySpark ha trovato Python e ha aperto la **REPL** (vedi dopo).\n",
    "\n",
    "   * ‚ÄúSpark context Web UI available at http://DESKTOP-‚Ä¶:4040‚Äù<br>\n",
    "      ‚Üí se si apre quel link dal browser della stessa macchina, si vede **l‚Äôinterfaccia web dei job**.\n",
    "\n",
    "   * ‚ÄúSpark context available as 'sc' ‚Ä¶ SparkSession available as 'spark'.‚Äù ‚Üí quindi si possono pu√≤ gi√† eseguire comandi.\n",
    "\n",
    "   * L‚Äôunico messaggio ‚Äústrano‚Äù √®:\n",
    "\n",
    "      `WARN NativeCodeLoader: Unable to load native-hadoop library for your platform...`\n",
    "\n",
    "      Su Windows √® normalissimo: **Spark non trova le librerie native Hadoop e usa le classi Java incluse. Per lavorare in locale si pu√≤ ignorare**.\n",
    "\n",
    "\n",
    "> **Cosa √® la *REPL***?<br>\n",
    "> √à il prompt interattivo di PySpark: una **REPL**.\n",
    "> **REPL** significa **Read ‚Äì Evaluate ‚Äì Print ‚Äì Loop**.\n",
    ">\n",
    ">  1. **Read**: legge ci√≤ che la persona digita (`spark.range(5).show()`).\n",
    ">  2. **Evaluate**: lo esegue nel motore Spark.\n",
    ">  3. **Print**: mostra il risultato.\n",
    ">  4. **Loop**: torna al prompt `>>>` per un altro comando.\n",
    ">\n",
    ">   Nel nostro caso √® una REPL **Python** che ha gi√† dentro un‚Äôistanza di Spark pronta:\n",
    ">\n",
    ">   * la variabile **`sc`** √® il `SparkContext` gi√† creato;\n",
    ">   * la variabile **`spark`** √® la `SparkSession` gi√† pronta.\n",
    ">\n",
    "> Quindi, invece di scrivere un file `.py` e lanciarlo, l'utente pu√≤ ‚Äúprovare‚Äù comandi Spark uno alla volta, vedere subito l‚Äôoutput, correggere, riprovare. > > **√à lo stesso concetto del prompt Python classico, ma con Spark gi√† avviato**.\n",
    ">\n",
    "> Ecco perch√©, quando √® partita, ha scritto:\n",
    "> ‚ÄúSpark context available as ‚Äòsc‚Äô ‚Ä¶ SparkSession available as ‚Äòspark‚Äô.‚Äù\n",
    "> cio√®: ‚Äúle ho gi√† preparato Spark, usi pure `sc` e `spark` qui dentro‚Äù.\n",
    "\n",
    "Come **VERIFICA**, dentro *pyspark*, eseguiamo alcuni comandi:\n",
    "- `spark` --> dovrebbe stampare qualcosa tipo `<pyspark.sql.session.SparkSession object ...>`. Significa che la sessione c‚Äô√®.\n",
    "- `spark.version` --> dovrebbe stampare la versione `4.0.1`.\n",
    "- `sc.master` --> dovrebbe dare `local[*]`. Vuol dire: esecuzione locale, usa tutti i core.\n",
    "- `spark.range(5).show()` √® un job minimo **Dataframe** --> deve stampare una tabellina con i numeri 0‚Äì4. √à il modo pi√π rapido per vedere che Spark SQL funziona. \n",
    "- `sc.parallelize(range(10)).count()` √® un job minimo **RDD** --> se risponde 10 (dopo qualche secondo di elaborazione), **la catena Python ‚Üí JVM ‚Üí esecuzione funziona**.\n",
    "\n",
    "Digitare `exit()` per uscire da *pyspark*.<br>\n",
    "> Quando si fa `exit()` dalla shell PySpark, Spark deve spegnere:\n",
    "- il driver JVM,\n",
    "- i processi Python di supporto,\n",
    "- e a volte anche i processi figli che abbiamo aperto (per la UI, per il gateway py4j, ecc.).\n",
    "\n",
    "Su Windows, se Spark vede che qualche processo figlio √® ancora vivo, lo chiude ‚Äúa mano‚Äù e il sistema stampa proprio quei messaggi:\n",
    "```text\n",
    "   OPERAZIONE RIUSCITA: il processo con PID ‚Ä¶ √® stato terminato.\n",
    "```\n",
    "Sono solo i processi aperti da Spark che vengono chiusi quando si esce.\n",
    "\n",
    "---\n",
    "\n",
    "**Se invece si desidera usare la JRE gi√† presente** (es. `jre1.8.0_371`), basta:\n",
    "\n",
    "1. trovare la cartella reale (con i due `dir`),\n",
    "2. mettere quella in `JAVA_HOME`,\n",
    "3. aggiungere `%JAVA_HOME%\\bin` al PATH,\n",
    "4. nuovo `cmd` ‚Üí `java -version` ‚Üí `pyspark.cmd`.\n",
    "\n",
    "‚ùó Ma, per un‚Äôinstallazione ordinata e ‚Äúda manuale‚Äù, √® meglio installare un JDK e puntare l√¨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fddfec",
   "metadata": {},
   "source": [
    "---\n",
    "**Cosa abbiamo installato sul PC??**\n",
    "\n",
    "In sintesi, **PySpark** √® il **modulo Python di Apache Spark**, cio√® l‚Äôinterfaccia che permette di usare **Spark direttamente da Python**.\n",
    "\n",
    "Per capire cosa significa, bastano <u>tre punti chiave</u>:\n",
    "\n",
    "1. üß† **Apache Spark** √® una piattaforma per l‚Äôelaborazione di grandi quantit√† di dati (‚Äúbig data‚Äù), distribuita su pi√π core o pi√π macchine, molto pi√π veloce e scalabile di Pandas o SQL tradizionali.\n",
    "\n",
    "   * Pu√≤ lavorare su file enormi (CSV, Parquet, JSON, database, ecc.).\n",
    "   * √à ottimizzato per elaborazioni parallele e in memoria.\n",
    "\n",
    "2. üêç **PySpark** √® il ‚Äúponte‚Äù tra Python e Spark: consente di scrivere codice Python (con sintassi simile a Pandas o SQL) che Spark poi esegue in modo distribuito su JVM.\n",
    "\n",
    "   * Esempio (commentato nella cella successiva):\n",
    "\n",
    "     ```python\n",
    "     from pyspark.sql import SparkSession\n",
    "     spark = SparkSession.builder.appName(\"Esempio\").getOrCreate()\n",
    "     df = spark.read.csv(r\"C:\\Users\\Utente\\Desktop\\salvataggi\\SALVATAGGIO DATI\\Documents\\Seminari\\Data Science (corsi)\\Corso Big Data con Spark\\persone.csv\", header=True, inferSchema=True)\n",
    "     df.show()\n",
    "     df.groupBy(\"eta\").count().show()\n",
    "     ```\n",
    "\n",
    "     Qui il codice sembra Python, ma in realt√† **Spark lo traduce in operazioni parallele Java/Scala dietro le quinte**.\n",
    "\n",
    "3. ‚öôÔ∏è **Installare PySpark su PC** significa aver installato:\n",
    "\n",
    "   * la libreria Python `pyspark` (che fornisce le API),\n",
    "   * e il motore Spark vero e proprio (che gira su Java, da qui la necessit√† di **JAVA_HOME**).\n",
    "\n",
    "üëâ In pratica: **PySpark = Spark + interfaccia Python**\n",
    "Serve per fare analisi dati su grandi volumi, elaborazioni ETL, o machine learning distribuito, ma usando un linguaggio comodo e leggibile come Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2190c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Testiamo nella CLI l'esempio di codice sopra riportato al punto 2**, riportato qui sotto con i commenti.\n",
    "\n",
    "```python\n",
    "    # Importa la classe SparkSession, necessaria per creare e gestire una sessione Spark\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    # Crea (o riutilizza) una sessione Spark chiamata \"Esempio\" - vedi sotto per ulteriori commenti\n",
    "    spark = SparkSession.builder.appName(\"Esempio\").getOrCreate()\n",
    "\n",
    "    # Legge il file CSV indicato dal percorso, con:\n",
    "    # - header=True ‚Üí la prima riga contiene i nomi delle colonne\n",
    "    # - inferSchema=True ‚Üí Spark deduce automaticamente i tipi di dato (int, string, ecc.)\n",
    "    # - attenzione al CAMMINO ASSOLUTO del file --> ADATTARE AL VOSTRO CASO\n",
    "    df = spark.read.csv(\n",
    "        r\"C:\\Users\\Utente\\Desktop\\salvataggi\\SALVATAGGIO DATI\\Documents\\Seminari\\Data Science (corsi)\\Corso Big Data con Spark\\persone.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "\n",
    "    # Mostra a schermo le prime righe del DataFrame (di default 20)\n",
    "    df.show()\n",
    "\n",
    "    # Raggruppa i dati per valore della colonna \"eta\" e conta quante righe ha ciascun gruppo\n",
    "    # poi mostra il risultato a video\n",
    "    df.groupBy(\"eta\").count().show()\n",
    "```\n",
    "\n",
    "Esaminiamo meglio il secondo statement (creazione / riutilizzo della sessione spark).<br>\n",
    "üëâ Crea o riusa una sessione Spark **chiamata ‚ÄúEsempio‚Äù**, eseguendola **in locale su tutti i core della CPU** (`local[*]`).<br>\n",
    "√à il comando che inizializza PySpark e permette di leggere, trasformare e analizzare i dati.\n",
    "\n",
    "Se otteniamo questo warning:<br>\n",
    "` WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.`\n",
    "\n",
    "√® normale: \n",
    "- significa che il metodo `getOrCreate()` ha trovato una **`SparkSession` gi√† aperta e quindi riusa quella**. In tal caso:\n",
    "- le nuove impostazioni passate al builder (es. `.appName(\"Esempio\")`, `.master(...)`, `.config(\"spark.executor.memory\", ...)`) non vengono applicate;\n",
    "- solo le configurazioni SQL ‚Äúruntime‚Äù impostate dopo via `spark.conf.set(...) avranno effetto`.\n",
    "\n",
    "In questo caso eseguiamo da CLI questo codice:\n",
    "```python\n",
    "    spark.stop()                                # chiude la sessione corrente (e lo SparkContext)\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = (SparkSession.builder\n",
    "            .master(\"local[*]\")\n",
    "            .appName(\"Esempio\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "            .getOrCreate())\n",
    "```\n",
    "\n",
    "Ecco cosa fa l'ultimo comando (`spark ( (SparkSession.builder ...))`):<br>\n",
    "esegue (o riusa) una SparkSession **locale** con alcune impostazioni:\n",
    "- `.master(\"local[*]\")` ‚Üí esecuzione in locale, usando **tutti i core CPU disponibili**.\n",
    "- `.appName(\"Esempio\")` ‚Üí assegna il **nome applicazione** (‚ÄúEsempio‚Äù).\n",
    "- `.config(\"spark.sql.shuffle.partitions\",\"8\")` ‚Üí imposta a **8 le partizioni di shuffle** (join/aggregate/shuffle saranno suddivisi in 8 partizioni; utile per ridurre overhead su dataset piccoli/medi).\n",
    "- `.getOrCreate()` ‚Üí **crea** la sessione se non esiste; altrimenti **riutilizza** quella gi√† attiva (le nuove config non sovrascrivono quelle del contesto gi√† avviato, salvo le runtime SQL).\n",
    "\n",
    "In sintesi: prepara una **sessione Spark locale ‚Äúleggera‚Äù**, ottimizzata per test/mini job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f13384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifica del numero di processori logici disponibili (non sono i core fisici!)\n",
    "import os\n",
    "os.cpu_count()  # numero di processori LOGICI (thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3dcd0",
   "metadata": {},
   "source": [
    "---\n",
    "La **CLI** (Command Line Interface) che si lancia **dal terminale** ‚Äî cio√® quando si scrive ad esempio `pyspark.cmd` o `spark-shell.cmd` ‚Äî non √® PySpark ‚Äúin s√©‚Äù, ma **il programma che avvia l‚Äôambiente Spark sul PC**.\n",
    "\n",
    "Ecco in breve cosa succede ‚Äúdietro le quinte‚Äù:\n",
    "\n",
    "---\n",
    "\n",
    "1Ô∏è‚É£ **Cos‚Äô√® la CLI di Spark**\n",
    "\n",
    "La **CLI** √® un insieme di **script batch** (`.cmd` su Windows, `.sh` su Linux/macOS) contenuti nella cartella `C:\\spark\\bin`.\n",
    "Servono per **avviare Spark** in modalit√† interattiva o per lanciare job.\n",
    "I principali sono:\n",
    "\n",
    "| Comando            | Descrizione                                                                              |\n",
    "| ------------------ | ---------------------------------------------------------------------------------------- |\n",
    "| `pyspark.cmd`      | apre una **shell Python interattiva** con Spark gi√† caricato (cio√® l‚Äôinterprete PySpark) |\n",
    "| `spark-shell.cmd`  | apre la shell Spark in **Scala**, il linguaggio nativo di Spark                          |\n",
    "| `spark-submit.cmd` | serve per **eseguire script o job Spark** gi√† scritti (Python, Scala, Java)              |\n",
    "| `spark-sql.cmd`    | apre una shell per eseguire **query SQL** direttamente su Spark                          |\n",
    "\n",
    "---\n",
    "\n",
    "2Ô∏è‚É£ **Cosa fa quando si lancia**\n",
    "\n",
    "Quando si esegue ad esempio:\n",
    "\n",
    "```cmd\n",
    "    C:\\spark\\bin>pyspark.cmd\n",
    "```\n",
    "\n",
    "succede questo:\n",
    "\n",
    "1. lo script `.cmd` prepara le **variabili d‚Äôambiente** (SPARK_HOME, JAVA_HOME, PYSPARK_PYTHON‚Ä¶);\n",
    "2. avvia un piccolo **processo Java** che rappresenta il ‚Äúmotore Spark‚Äù (JVM);\n",
    "3. apre una **sessione Python** gi√† collegata a quel motore;\n",
    "4. lei si ritrova in un prompt tipo:\n",
    "\n",
    "   ```\n",
    "   >>> from pyspark.sql import SparkSession\n",
    "   >>> spark = SparkSession.builder.getOrCreate()\n",
    "   ```\n",
    "\n",
    "   cio√® un interprete Python che comanda Spark ‚Äúdietro‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "üí° **In sintesi**\n",
    "\n",
    "La CLI non √® Spark in s√©, ma:\n",
    "\n",
    "> un **ambiente di avvio** che collega Python (PySpark) al motore Spark vero e proprio, basato su Java.\n",
    "\n",
    "√à utile per testare comandi, fare debug o lanciare job manualmente.\n",
    "In produzione invece si usa quasi sempre `spark-submit`, che lancia script completi.\n",
    "\n",
    "Quui sotto lo **schema del flusso logico** (cio√® cosa parla con cosa: Python ‚Üí JVM ‚Üí Spark core ‚Üí dati)?\n",
    "![](schema_logico_flusso.png)\n",
    "\n",
    "NB. Gli **executors** in Spark sono **processi JVM** (uno o pi√π) che eseguono i task dell‚Äôapplicazione, conservano in memoria/disk i dati in cache e comunicano i risultati al Driver. Vedi pi√π avanti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0de30",
   "metadata": {},
   "source": [
    "---\n",
    "**Ancora sulla CLI** (aka *REPL*)\n",
    "\n",
    "In alternativa alla **app Spark** (questo notebook!), l'avviamento di Spark pu√≤ avvenire anche in **modalit√† REPL**, e cio√® da un anaconda prompt (non da un terminale DOS normale), tramite la shell *pyspark* (un REPL per Python, appunto), ad esempio cos√¨:<br><br>\n",
    "    *pyspark --master local[asterisco]*<br><br>\n",
    "Questo comando crea un cluster locale che, come indicato da \"asterisco\" (in Jupyter l'asterisco √® un carattere speciale di controllo della formattazione - attenzione!), usa tutti i core della macchina (6 nel mio caso, 12 i processori logici). In questo corso useremo sempre l'opzione di lancio *master*.<br>\n",
    "Per verificare il numero di core del processore in Windows 10, premere CTRL + MAIUSC + ESC per aprire Gestione attivit√†, selezionare la scheda prestazioni e poi la CPU. Per la differenza tra processore fisico, core e processore logico, vedi [qui](https://learn.microsoft.com/it-it/windows/win32/procthread/processor-groups).<br>\n",
    "In modalit√† REPL si pu√≤ impostare la memoria da allocare alla sessione Spark con questo avviamento:<br><br>\n",
    "*pyspark --driver-memory 2g*.<br><br>\n",
    "Dentro la shell REPL: *help()* per aiuto. Si esce dalla shell REPL con *quit()*. \n",
    "\n",
    "L'oggetto *spark* ha i suoi metodi, visualizzabili in modalit√† REPL con *spark.* seguito da tab, in modo gerarchico (ad es., *spark.Builder* seguito da tab, *spark.Builder.config* seguito da tab, ecc).<br>\n",
    "\n",
    "NB. Nella modalit√† REPL non √® necessario che l'utente crei esplicitamente la sessione; essa √® eseguita e mostrata automaticamente all'avvio di Spark, subito dopo la *Ascii art*, insieme al suo indirizzo esadecimale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed385bf",
   "metadata": {},
   "source": [
    "# La submit\n",
    "\n",
    "Una differente modalit√† di esecuzione √® la *submit*.\n",
    "\n",
    "Ecco un esempio **minimale ma adatto per la `spark-submit`.<br>\n",
    "Fa un word-count con le DataFrame API; accetta --input e --output.<br>\n",
    "Se non si passa nulla, usa un piccolo esempio in memoria e stampa a video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d5275",
   "metadata": {},
   "source": [
    "```python\n",
    "    # file: wordcount_df.py\n",
    "    import argparse, sys\n",
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    def parse_args(argv=None):\n",
    "        p = argparse.ArgumentParser(description=\"Word Count (DataFrame API)\")\n",
    "        p.add_argument(\"--input\", help=\"Percorso file di input (testo)\")\n",
    "        p.add_argument(\"--output\", help=\"Cartella di output (CSV)\")\n",
    "        p.add_argument(\"--topN\", type=int, default=20)\n",
    "        if argv is None:\n",
    "            # In Jupyter ci sono argomenti extra (--f=...), li ignoriamo\n",
    "            args, _ = p.parse_known_args()\n",
    "            return args\n",
    "        return p.parse_args(argv)\n",
    "\n",
    "    def main(argv=None):\n",
    "        args = parse_args(argv)\n",
    "        spark = SparkSession.builder.appName(\"WordCountDF\").getOrCreate()\n",
    "        # ... resto identico (creazione df, split, groupBy, show/write) ...\n",
    "        spark.stop()\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3cc1dd",
   "metadata": {},
   "source": [
    "In sintesi, il codice √® un **Word Count con Spark**, pensato per essere lanciato con `spark-submit` (ma ‚Äútollerante‚Äù anche in Jupyter/VSC):\n",
    "- **Parsing argomenti**: legge `--input` (file di testo), `--output` (cartella CSV) e `--topN` (righe da mostrare). In notebook ignora gli argomenti estranei di Jupyter usando `parse_known_args`.\n",
    "- **Sessione Spark**: crea una `SparkSession` chiamata `WordCountDF`.\n",
    "- **Lettura dati**: se c‚Äô√® `--input`, legge il file di testo; altrimenti usa poche righe di esempio in memoria.\n",
    "- **Tokenizzazione**: porta a minuscolo, fa `split` con `regex \\W+`, `explode` delle parole e filtra i vuoti.\n",
    "- **Conteggio**: `groupBy(\"word\").count()` e ordinamento per `count` decrescente (poi parola crescente).\n",
    "- **Output**:\n",
    "    - se c‚Äô√® `--output`, scrive un CSV (in directory stile Spark, tipicamente con `coalesce(1)` e `header=true`);\n",
    "    - altrimenti stampa a video i primi topN.\n",
    "- **Chiusura**: `spark.stop()` a fine esecuzione.\n",
    "\n",
    "Esempi d‚Äôuso (da cmd):\n",
    "```text\n",
    "    spark-submit --master local[*] wordcount_df.py            # esempio demo, senza input e output\n",
    "    spark-submit --master local[*] wordcount_df.py --input input.txt --output out/wordcount\n",
    "    spark-submit --master local[*] wordcount_df.py            # usa i dati di esempio e stampa a video\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb70abe",
   "metadata": {},
   "source": [
    "# I dataframe di PySpark\n",
    "\n",
    "Il **DataFrame di PySpark**:\n",
    "* √® un **DataFrame** di PySpark, accessibile attraverso la potente API DataFrame di Spark (vedi la precedente figura sulle componenti di Spark), √® simile ma non uguale ad un dataframe di *pandas*; √® un'astrazione per dataset che hanno una struttura regolare nella quale ogni record √® una riga composta da una serie di colonne, ed ogni colonna ha un data type ben definito, come ad esempio una tabella relazionale. **A differenza dei dataframe di pandas**, i DataFrame di PySpark sono <u>dataset distributi su un cluster</u>, con differenti porzioni di righe su ogni nodo del cluster.\n",
    "<br><br>\n",
    "* il **DataFrame √® un'evoluzione degli RDD** ‚ùó (*resilient distributed datasets*), pi√π performante. Grazie ai DataFrame, il query plan risulta pi√π efficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1484b",
   "metadata": {},
   "source": [
    "Un post interessante su pyspark:<br>\n",
    "![](post_nardini.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3fd32",
   "metadata": {},
   "source": [
    "# L'architettura interna di Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951e7ab",
   "metadata": {},
   "source": [
    "![](Figure_2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff297e",
   "metadata": {},
   "source": [
    "La Figura precedente illustra l'architettura Spark nelle sue **componenti di alto livello**. Le applicazioni Spark sono eseguite come <u>set indipendenti di processi in un cluster o in locale</u>. Ad un livello alto, un'applicazione Spark √® composta da un **processo driver** (a sx in figura), un **gestore di cluster** (*Cluster Manager*) ed un insieme di **processi esecutori**. Il programma driver √® il componente centrale ed √® responsabile della distribuzione delle attivit√† (*task*) tra i processi esecutori. C'√® sempre <u>un solo processo driver</u>. Lo *scaling* aumenta il numero di esecutori. Il gestore di cluster gestisce semplicemente le risorse.<br><br>\n",
    "Spark √® **un motore di calcolo (*compute engine*) distribuito e *data-parallel***. In un modello \"data-parallel\", pi√π partizioni di dati implicano un maggiore parallelismo. Il partizionamento (*partitioning*) consente un parallelismo efficiente. Uno schema distribuito di <u>suddivisione dei dati in blocchi o partizioni</u> consente ai processi esecutori di Spark di elaborare solo i dati **a loro vicini**, riducendo al minimo il consumo di larghezza di banda di rete (*network bandwith*). In altre parole, al core di ogni processo esecutore √® assegnata la propria partizione di dati su cui lavorare. Il progettista Spark deve ricordarsi di ci√≤ ogni volta  che si presenta la necessit√† di una scelta progettuale relativa al partizionamento.<br><br>\n",
    "La programmazione Spark inizia con un **dataset**, che di solito risiede in una qualche forma di archiviazione distribuita e persistente come il file system distribuito Hadoop (HDFS) o una soluzione basata su cloud come AWS S3 e in un formato come [Parquet](https://parquet.apache.org/). La scrittura di un programma Spark in genere consiste di **pochi passaggi**:\n",
    "* definire un insieme di **trasformazioni** al dataset di input;\n",
    "<br><br>\n",
    "* richiamare le **azioni** che eseguono le trasformazioni e le archiviano in modo permanente o restituiscono i risultati alla memoria locale del driver. Idealmente, queste azioni sono eseguite dai nodi di lavoro, come illustrato a destra nella figura precedente;\n",
    "<br><br>\n",
    "* eseguire i calcoli locali che operano sui risultati, calcolati in modo distribuito. Questi possono aiutare il progettista  a decidere quali trasformazioni e azioni intraprendere successivamente.<br><br>\n",
    "\n",
    "√à importante ricordare che tutte le astrazioni di livello superiore di PySpark si basano sulla stessa filosofia presente in Spark sin dal suo inizio: **l'interazione tra archiviazione (*storage*) ed esecuzione**. La comprensione di questi principi  aiuta il progettista ad utilizzare Spark al meglio per l'analisi dei dati.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062bc07",
   "metadata": {},
   "source": [
    "La framework Spark supporta in modo ufficiale **quattro modalit√† di *cluster deployment***: \n",
    "* stand-alone\n",
    "* YARN\n",
    "* Kubernetes\n",
    "* Mesos<br>\n",
    "\n",
    "Per maggiori dettagli si veda [qui](https://oreil.ly/hG2a5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62bc353",
   "metadata": {},
   "source": [
    "# Spark da notebook python\n",
    "\n",
    "Differenza rispetto alla CLI (*pyspark.cmd*)\n",
    "\n",
    "1Ô∏è‚É£ **`pyspark.cmd`** ‚Üí √® uno script che avvia:\n",
    "- la JVM di Spark\n",
    "- e un interprete Python gi√† ‚Äúcablato‚Äù con pyspark.\n",
    "\n",
    "2Ô∏è‚É£ **modulo `pyspark` nel notebook** ‚Üí √® la stessa libreria, ma:\n",
    "- viene importata dentro il kernel del notebook,\n",
    "- e si crea a mano la sessione:\n",
    "```python\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Test da notebook\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "```\n",
    "\n",
    "In altre parole:\n",
    "\n",
    "> üîπ La CLI √® solo **un modo ‚Äúpronto all‚Äôuso‚Äù** per avviare Python + pyspark.<br>\n",
    "> La CLI non √® Spark in s√©, ma:\n",
    "> - un ambiente di avvio che collega Python (PySpark) al motore Spark vero e proprio, basato su Java.\n",
    "> - √® utile per **testare comandi, fare debug o lanciare job manualmente**.<br>\n",
    "> In produzione si usa quasi sempre **`spark-submit`**, che lancia script completi.\n",
    ">\n",
    "> üîπ Il modulo pyspark √® la **libreria vera e propria** che il notebook usa per comandare Spark.<br>\n",
    "> üìå Il modulo pyspark del notebook √® una libreria Python, cio√® un pacchetto installato dentro l‚Äôambiente Python (come *pandas*, *numpy*, ecc.), che fa da interfaccia client verso il motore Apache Spark (in Java/Scala)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "488485a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000002001754AFD0>>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test da notebook\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "spark\n",
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb202a4",
   "metadata": {},
   "source": [
    "## Impostazioni iniziali e lettura dei dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703fefcf",
   "metadata": {},
   "source": [
    "Impostazione del **timer di cella**, utile con i big data.<br>\n",
    "Si utilizza la Nbextension *Execute Time*. Vedi anche [questo post](https://stackoverflow.com/questions/32565829/simple-way-to-measure-cell-execution-time-in-ipython-notebook).<br>\n",
    "L'estensione √® configurabile (vedi i parametri nel tab *NBextensions*).<br>\n",
    "Un'alternativa (inferiore alla *Execute Time*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "054fe471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T15:38:15.600318Z",
     "iopub.status.busy": "2025-11-12T15:38:15.600318Z",
     "iopub.status.idle": "2025-11-12T15:38:15.605902Z",
     "shell.execute_reply": "2025-11-12T15:38:15.605902Z",
     "shell.execute_reply.started": "2025-11-12T15:38:15.600318Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import random\n",
    "\n",
    "def randword(delay=1,length=10):\n",
    "    time.sleep(delay)\n",
    "    return ''.join(\n",
    "        random.choice(string.lowercase)\n",
    "    for i in range(length)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b378b8b9",
   "metadata": {},
   "source": [
    "Da un anaconda prompt (disponibile nel men√π Windows digitando \"anaconda\"), che √® differente dal normale terminale DOS, oppure da un terminale DOS, **digitare il seguente comando** [*pip3*](https://www.activestate.com/resources/quick-reads/how-to-install-and-use-pip3/):<br> *pip3 install pyspark* (<u>nell'ambiente virtuale *py311*</u>).<br>\n",
    "Meglio prima **aggiornate la versione di pip** con: `python.exe -m pip install --upgrade pip`.<br>\n",
    "\n",
    "Per la corretta esecuzione del codice di questo corso, come detto precedentemente, √® necessario avere installato sul proprio PC/laptop Java 8 o successivi. La verifica della versione di Java installata √® fattibile dalla *installazione applicazioni* di Windows (vedi [qui](java_version.png)). L'ultima versione di Java √® scaricabile da [qui](https://www.java.com/it/download/ie_manual.jsp?locale=it).<br>\n",
    "\n",
    "Se si vuole installare anche SQL, ML e MLlib (necessario per questo corso) digitare questo comando (sempre <u>nell'ambiente virtuale *py311*</u>):<br>\n",
    "*pip3 install pyspark[sql,ml,mllib]*.<br>\n",
    "\n",
    "L'installazione di PySpark da PyPI (cio√® con il *pip*) evita l'installazione delle librerie per Scala, Java ed R. Per l'installazione completa si veda [qui](https://oreil.ly/pK2Wi). Ulteriori informazioni sull'installazione locale o su cluster sono disponibili [qui](https://oreil.ly/FLh4U).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f43ea90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T15:38:21.246248Z",
     "iopub.status.busy": "2025-11-12T15:38:21.246248Z",
     "iopub.status.idle": "2025-11-12T15:38:22.949714Z",
     "shell.execute_reply": "2025-11-12T15:38:22.949714Z",
     "shell.execute_reply.started": "2025-11-12T15:38:21.246248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\utente\\anaconda3\\envs\\py311\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\utente\\anaconda3\\envs\\py311\\lib\\site-packages (from pyspark) (0.10.9.9)\n"
     ]
    }
   ],
   "source": [
    "# !conda install pyspark -c conda-forge -y\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a0fa898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T15:38:26.636346Z",
     "iopub.status.busy": "2025-11-12T15:38:26.636346Z",
     "iopub.status.idle": "2025-11-12T15:38:26.752593Z",
     "shell.execute_reply": "2025-11-12T15:38:26.752593Z",
     "shell.execute_reply.started": "2025-11-12T15:38:26.636346Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession  # The entry point to programming Spark with the Dataset and DataFrame API --> vedi pi√π  avanti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e334606c",
   "metadata": {},
   "source": [
    "Facciamo un **piccolo test pratico** che tutto funzioni:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f26bb82d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T15:38:29.501802Z",
     "iopub.status.busy": "2025-11-12T15:38:29.501802Z",
     "iopub.status.idle": "2025-11-12T15:38:29.522146Z",
     "shell.execute_reply": "2025-11-12T15:38:29.521145Z",
     "shell.execute_reply.started": "2025-11-12T15:38:29.501802Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop() # lo stop √® tollerante, cio√® permette di chiudere molte volte. Ma √® buona norma chiudere appena finita l'elaborazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaa10910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T15:38:32.008312Z",
     "iopub.status.busy": "2025-11-12T15:38:32.008312Z",
     "iopub.status.idle": "2025-11-12T15:38:41.156766Z",
     "shell.execute_reply": "2025-11-12T15:38:41.156766Z",
     "shell.execute_reply.started": "2025-11-12T15:38:32.008312Z"
    }
   },
   "outputs": [],
   "source": [
    "# la prima volta ci pu√≤ mettere alcuni secondi (8-10):\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestDaNotebook\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d66d06",
   "metadata": {},
   "source": [
    "La cella precedente importa la classe principale (`SparkSessionBuilder`) con cui oggi si lavora in Spark (da quando c‚Äô√® l‚ÄôAPI DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1eee990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T15:38:43.516400Z",
     "iopub.status.busy": "2025-11-12T15:38:43.516400Z",
     "iopub.status.idle": "2025-11-12T15:38:44.347797Z",
     "shell.execute_reply": "2025-11-12T15:38:44.347361Z",
     "shell.execute_reply.started": "2025-11-12T15:38:43.516400Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(\"Mario\", 10), (\"Luigi\", 20)],\n",
    "    [\"nome\", \"valore\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a26902dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T15:38:46.057534Z",
     "iopub.status.busy": "2025-11-12T15:38:46.056537Z",
     "iopub.status.idle": "2025-11-12T15:38:56.108986Z",
     "shell.execute_reply": "2025-11-12T15:38:56.108986Z",
     "shell.execute_reply.started": "2025-11-12T15:38:46.057534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| nome|valore|\n",
      "+-----+------+\n",
      "|Mario|    10|\n",
      "|Luigi|    20|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()   # --> qualche secondo (circa 10) --> vedi nota sotto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06671154",
   "metadata": {},
   "source": [
    "La creazione del DataFrame sembra ‚Äúistantanea‚Äù, ma il primo (e spesso anche i successivi) `df.show()` sono lenti, anche se i dati sono 5 righe. Non √® il DataFrame ad essere lento, √® **Spark** che si muove lento per poco lavoro.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Spark √® ‚Äúlazy‚Äù**<br>\n",
    "Quando si fa:\n",
    "\n",
    "```python\n",
    "   df = spark.createDataFrame(...)\n",
    "```\n",
    "oppure si fa una select/join, Spark **non esegue davvero**. Si limita a costruire il piano (la lineage).\n",
    "\n",
    "Quando invece si fa:\n",
    "\n",
    "```python\n",
    "   df.show()\n",
    "```\n",
    "quella √® un‚Äô**action**: Spark deve davvero:\n",
    "\n",
    "1. creare il job,\n",
    "2. suddividerlo in stage,\n",
    "3. avviare i task,\n",
    "4. raccogliere i risultati,\n",
    "5. restituirli a Python e stamparli.\n",
    "\n",
    "Questa trafila c‚Äô√® anche per 3 righe nel dataframe.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Overhead fisso**\n",
    "\n",
    "Spark √® pensato per cluster e per gigabyte. Quindi ha **un overhead fisso** ogni volta che parte un job:\n",
    "\n",
    "* comunicazione driver ‚Üî executor (anche in locale),\n",
    "* passaggio dati tra JVM e Python,\n",
    "* logging,\n",
    "* eventuale inizializzazione di funzioni Python UDF,\n",
    "* stampa su console.\n",
    "\n",
    "Se l‚Äôoperazione √® piccola, quell‚Äôoverhead domina e sembra ‚Äú10 secondi per mostrare 5 righe‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Perch√© ‚Äúsempre‚Äù 10 secondi?**\n",
    "\n",
    "Se ogni `df.show()` ci fa aspettare circa lo stesso tempo, √® perch√© ogni action **rif√† il giro del job**. Spark non ‚Äúricorda‚Äù automaticamente il risultato della show precedente. Se il DataFrame deriva da una sorgente (anche piccola) o da una trasformazione, ogni volta ricalcola.\n",
    "\n",
    "In pi√π, su Windows spesso ci sono:\n",
    "\n",
    "* logging in console piuttosto verboso,\n",
    "* antivirus che controlla la JVM,\n",
    "* modalit√† `local[*]` che avvia pi√π task anche se non servirebbe.\n",
    "\n",
    "Tutto questo pu√≤ portare a quei 6-10 secondi di latenza per job.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Cosa pu√≤ fare per alleggerire**\n",
    "\n",
    "1. **Cache se si riusa lo stesso DF pi√π volte**\n",
    "\n",
    "   ```python\n",
    "      df.cache()   # \"prenota\" la cache\n",
    "      df.count()   # forza il calcolo e mette in cache\n",
    "      df.show()\n",
    "   ```\n",
    "   Le action successive dovrebbero essere pi√π rapide (non sempre fulminee, ma meno pesanti).\n",
    "\n",
    "   Cosa fanno i due comandi?\n",
    "   * **`df.cache()`**\n",
    "   Non esegue nulla subito. √à solo un **flag**: dice a Spark ‚Äúquando eseguirai la prossima action su `df`, **conserverai** le tue partizioni in cache‚Äù. Tecnicamente √® un alias di\n",
    "   `df.persist(StorageLevel.MEMORY_ONLY)`.\n",
    "\n",
    "   * **`df.count()`**\n",
    "   √à un‚Äô**action**. Eseguendo il job **materializza** il DataFrame e, dato il flag di prima, **salva** le partizioni calcolate nella cache (secondo lo storage level).\n",
    "\n",
    "   Quindi: il **primo** ‚Äúprenota‚Äù la cache, il **secondo** la **riempie**.\n",
    "\n",
    "   Suggerimenti rapidi:\n",
    "\n",
    "   * Se il dataset √® troppo grande per la RAM:\n",
    "\n",
    "   ```python\n",
    "         from pyspark import StorageLevel\n",
    "         df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "         df.count()  # warm-up\n",
    "   ```\n",
    "   * Per liberare: `df.unpersist()`.\n",
    "   * Per verificare: `df.is_cached` (in PySpark) e la tab *Storage* della Spark UI.\n",
    "<br><br>\n",
    "\n",
    "2. **Limitare prima di mostrare**\n",
    "\n",
    "   ```python\n",
    "   df.limit(20).show()\n",
    "   ```\n",
    "   Cos√¨ Spark sa che deve prendere poche righe.\n",
    "<br><br>\n",
    "3. **Usare `toPandas()` solo per dati mddio-piccoli**\n",
    "   Se sono proprio poche righe:\n",
    "\n",
    "   ```python\n",
    "      pdf = df.toPandas()\n",
    "      display(pdf)\n",
    "   ```\n",
    "\n",
    "   Una volta in pandas, le stampe sono immediate. (Va fatto solo se i dati ci stanno in memoria.)\n",
    "<br><br>\n",
    "4. **Ridurre i log** (a volte √® quello che fa perdere il tempo visibile) mettendo in `log4j2.properties` un livello pi√π basso (WARN invece di INFO).\n",
    "<br><br>\n",
    "5. **Valutare se per dati piccoli convenga proprio Spark**.\n",
    "   Se si sta provando, va benissimo; ma Spark **non brilla sui micro-dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Quindi: non √® il DataFrame, √® Spark**\n",
    "\n",
    "Quello prima visto √® dunque un **funzionamento sano**: Spark sta facendo tutto il giro ‚Äúda grande‚Äù, solo che i dati sono piccoli. Per questo la creazione sembra istantanea (√® lazy) e la `show()` sembra lento (√® l‚Äôazione che fa davvero il lavoro).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034b8f5",
   "metadata": {},
   "source": [
    "Trasformiamo il `df` di *spark* in un `DataFrame` *pandas*, come accennato prima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef78caa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# circa 8 secondi\n",
    "pdf = df.toPandas()\n",
    "type(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82104fc7",
   "metadata": {},
   "source": [
    "E' un **dataframe pandas**, con tutti i metodi e le funzioni di pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "92f8aae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>valore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mario</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Luigi</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nome  valore\n",
       "0  Mario      10\n",
       "1  Luigi      20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "display(pdf.head())\n",
    "print(pdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4a8c8",
   "metadata": {},
   "source": [
    "**Quando conviene convertire in pandas?**\n",
    "- Risultati piccoli/medi da ispezionare subito (campioni, aggregati, top-N):\n",
    "`df.limit(1000).toPandas()` per vedere/filtrare velocemente.\n",
    "\n",
    "- Grafici e report locali: molte librerie (matplotlib, seaborn, plotly, xlsxwriter) si aspettano un DataFrame pandas.\n",
    "\n",
    "- Modeling locale: scikit-learn, statsmodels, prophet, ecc. richiedono pandas/numpy.\n",
    "\n",
    "- Debug/EDA ‚Äúdi fino‚Äù: operazioni interattive cella-per-cella tipiche di pandas.\n",
    "\n",
    "üëâ Regola empirica: convertire solo se il risultato **‚Äúci sta comodo‚Äù nella RAM del driver** (ad es. < ~100‚Äì200 MB per lavoro agile; l‚Äôordine di grandezza dipende da colonne/tipi)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63909cbf",
   "metadata": {},
   "source": [
    "**Differenze chiave (Spark DF vs pandas DF)**\n",
    "\n",
    "- **Esecuzione**: Spark √® **lazy** e **distribuito** (cluster o local[*]); pandas √® **eager** e **single-node** (tutto in memoria sul driver).\n",
    "\n",
    "- **Scala**: Spark gestisce dati che non stanno in RAM; pandas necessita che **l‚Äôintero** dataset entri nella RAM locale.\n",
    "\n",
    "- **API**: Spark orientato a **DataFrame/SQL** (ottimizzatore, join/distribuzione/partitioning); pandas √® tabellare in-memory, molto ricco per manipolazioni di dettaglio.\n",
    "\n",
    "- **Ordine/semantica**: in Spark l‚Äôordine **non √® garantito** senza `orderBy`; pandas preserva l‚Äôordine delle righe.\n",
    "\n",
    "- **Tipi/NA**: differenze su `null/NaN`, interi con null, timezone dei timestamp, categorie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b4c5b5",
   "metadata": {},
   "source": [
    "**Accortezze pratiche**\n",
    "\n",
    "Usare `Arrow` per conversioni pi√π veloci:\n",
    "```python\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    pdf = df.toPandas()\n",
    "```\n",
    "(serve pyarrow installato).\n",
    "\n",
    "Limitare prima di convertire:\n",
    "```python\n",
    "    pdf = df.limit(10_000).toPandas()\n",
    "```\n",
    "\n",
    "Per lavorare ‚Äúin stile pandas‚Äù senza raccogliere dati: valuti la pandas API on Spark\n",
    "```python\n",
    "    import pyspark.pandas as ps\n",
    "    # ps.DataFrame si esegue su Spark, sintassi tipo pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00b3a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Nota su `Arrow`**\n",
    "\n",
    "Apache **Arrow** √® un **formato colonnare in-memoria** e un set di librerie per scambiare dati **ad alta velocit√†** tra processi/linguaggi (JVM ‚Üî Python, C++‚Ä¶ ) **senza copie inutili**.\n",
    "\n",
    "Nel contesto PySpark serve a due cose principali:\n",
    "\n",
    "**1) Conversione veloce Spark ‚Üî pandas**\n",
    "\n",
    "* **`toPandas()` / `createDataFrame(pandas_df)`** usano Arrow per trasferire i dati in blocchi colonnari invece che riga-per-riga.\n",
    "* Risultato: **molto pi√π rapido** e con meno CPU.\n",
    "* Attivazione (di solito √® gi√† on nelle versioni recenti, ma esplicitarlo non guasta):\n",
    "\n",
    "  ```python\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    import pandas as pd\n",
    "    pdf = df.toPandas()  # veloce con Arrow\n",
    "  ```\n",
    "* Requisito: avere **`pyarrow`** installato nel Python del kernel:\n",
    "\n",
    "  ```bash\n",
    "    \n",
    "  ```\n",
    "\n",
    "**2) Pandas UDF (vectorized UDF)**\n",
    "\n",
    "* Le **Pandas UDF** (a volte chiamate vectorized UDF) usano Arrow per passare/ritornare **colonne** alla funzione Python in blocco, anzich√© riga-per-riga ‚Üí enorme speed-up rispetto alle UDF classiche.\n",
    "\n",
    "  ```python\n",
    "    import pandas as pd\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql.types import DoubleType\n",
    "\n",
    "    @F.pandas_udf(DoubleType())\n",
    "    def zscore(col: pd.Series) -> pd.Series:\n",
    "        return (col - col.mean()) / col.std()\n",
    "\n",
    "    df = df.withColumn(\"z\", zscore(F.col(\"valore\")))\n",
    "  ```\n",
    "\n",
    "**Quando usarlo**:\n",
    "\n",
    "* Conversioni frequenti **Spark‚Üîpandas** (EDA, plotting, scikit-learn su subset).\n",
    "* UDF numeriche/vettoriali dove serve performance in Python.\n",
    "\n",
    "**Limiti e accortezze**:\n",
    "\n",
    "* **Non elimina i vincoli di memoria del driver**: `toPandas()` raccoglie comunque tutto **sul driver**. Usarlo solo su dati che ‚Äústanno‚Äù in RAM.\n",
    "* Tipi complessi/nidificati e certe timezone possono forzare **fallback** (Spark logga e va pi√π lento).\n",
    "* Opzioni utili:\n",
    "\n",
    "  ```python\n",
    "    # dimensione batch Arrow (aumentare pu√≤ aiutare)\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"20000\")\n",
    "\n",
    "    # tipo timestamp lato driver (se serve coerenza)\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")  # torna al percorso non-Arrow se necessario\n",
    "  ```\n",
    "* Verifica rapida:\n",
    "\n",
    "  ```python\n",
    "    spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")\n",
    "  ```\n",
    "\n",
    "**In breve:** Arrow √® il ‚Äúponte colonnare‚Äù che rende veloci le conversioni Spark‚Üîpandas e abilita le Pandas UDF vettoriali. √à il modo standard per parlare tra JVM e Python **senza costi di serializzazione riga-per-riga**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230eadf",
   "metadata": {},
   "source": [
    "**In sintesi**:\n",
    "- **Spark DF** per **calcolo pesante/distribuito** e pipeline su **grandi volumi**.\n",
    "- **pandas DF** quando il risultato √® **medio-piccolo** e serve **integrazione** con librerie Python locali, grafici, modellazione o analisi interattiva dettagliata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ea45e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214ca41b",
   "metadata": {},
   "source": [
    "Da quel momento non c‚Äô√® pi√π lo `SparkContext` che esegue i job, non c‚Äô√® pi√π il catalogo, non c‚Äô√® pi√π la parte che materializza i DataFrame.<br>\n",
    "Il nome df in Python c‚Äô√® ancora, cio√®:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f608d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[nome: string, valore: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f654cf3",
   "metadata": {},
   "source": [
    "l‚Äôoggetto esiste in memoria. Ma appena si prova a fare qualcosa che richiede Spark (es. `df.show()`, `df.count()`, scriverlo su disco), Spark deve partire‚Ä¶ e non pu√≤, perch√© √® stato fermato ‚Üí dunque errore, e sembra che ‚Äúnon lo veda pi√π‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "177668f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o77.showString.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"head\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\r\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\r\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.readMetrics$lzycompute(limit.scala:68)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.readMetrics(limit.scala:67)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.metrics$lzycompute(limit.scala:69)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.metrics(limit.scala:69)\r\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t... 31 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# --> errore, perch√© la sessione Spark √® stata fermata\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    298\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o77.showString.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"head\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\r\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\r\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.readMetrics$lzycompute(limit.scala:68)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.readMetrics(limit.scala:67)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.metrics$lzycompute(limit.scala:69)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.metrics(limit.scala:69)\r\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t... 31 more\r\n"
     ]
    }
   ],
   "source": [
    "df.show()   # --> errore, perch√© la sessione Spark √® stata fermata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448c55d",
   "metadata": {},
   "source": [
    "Rieseguiamo la **creazione della sessione Spark** con 2 differenze rispetto a prima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "086dfe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\utente\\anaconda3\\envs\\py311\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57049f3e-6e8f-4c4f-83e5-50cc10199a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prima differenza\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# seconda differenza\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061e118",
   "metadata": {},
   "source": [
    "**<u>Prima differenza</u>**:\n",
    "\n",
    "```python\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "```\n",
    "\n",
    "* `findspark` serve quando Spark non √® nel `PYTHONPATH` o non √® installato ‚Äúalla maniera Python‚Äù.\n",
    "* `findspark.init()` aggiunge la cartella di Spark al path di Python, cos√¨ l‚Äô`import pyspark` e tutto il resto funzionano anche se Spark √® installato ‚Äúa parte‚Äù (per esempio in `C:\\spark`).\n",
    "\n",
    "Spieghiamo meglio.<br>\n",
    "\n",
    "***1. Come Python trova i moduli*** (i package)<br>\n",
    "Quando in Python si scrive:<br>\n",
    "```python\n",
    "    import pyspark\n",
    "```\n",
    "\n",
    "Python va a cercare un pacchetto chiamato pyspark in una serie di cartelle. L‚Äôelenco di queste cartelle √® il famoso **PYTHONPATH** (pi√π le cartelle standard dei pacchetti).\n",
    "Se pyspark non √® in nessuna di queste cartelle, l‚Äôimport fallisce.<br>\n",
    "In generale, infatti, Python cerca il pacchetto da importare in una serie di posti, in quest‚Äôordine pi√π o meno:\n",
    "- le cartelle standard dell‚Äôinstallazione di Python,\n",
    "- le cartelle dei pacchetti installati con pip,\n",
    "- le cartelle elencate in PYTHONPATH (se √® stata definita).\n",
    "\n",
    "Se il modulo non √® in nessuna di queste, Python d√† `ModuleNotFoundError`.\n",
    "\n",
    "***2. ‚ÄúInstallato alla maniera Python‚Äù***<br>\n",
    "Se si fa:<br>\n",
    "```python\n",
    "    pip install pyspark\n",
    "```\n",
    "\n",
    "allora Python sa gi√† dove andare a prenderlo, perch√© pip lo mette in una delle cartelle che Python controlla. Questa √® la ‚Äúmaniera Python‚Äù: il pacchetto √® visibile a Python perch√© √® stato installato come libreria Python.\n",
    "\n",
    "***3. Quando Spark NON √® installato cos√¨***<br>\n",
    "Spesso per√≤ Spark si scarica come archivio:\n",
    "- su Windows: si mette in C:\\spark\n",
    "- su Linux/Mac: in /opt/spark o dove si preferisce\n",
    "\n",
    "In questo caso Spark esiste sul disco, ma Python non sa che l√¨ dentro c‚Äô√® il codice di pyspark. Quella cartella non √® nel PYTHONPATH, quindi *import pyspark* darebbe errore.\n",
    "\n",
    "‚ùó Passo saltato: rendere visibile a Python la parte pyspark (o installando con *pip install pyspark*, o aggiungendo la cartella al PYTHONPATH).\n",
    "\n",
    "***4. Che cosa fa findspark***<br>\n",
    "*findspark* serve proprio a colmare questo buco.<br>\n",
    "Quando si fa:<br>\n",
    "```python\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "```\n",
    "*findspark*:\n",
    "- prova a capire dove si trova Spark (a volte leggendo SPARK_HOME, a volte cercando in percorsi tipici);\n",
    "- aggiunge quella cartella alle vie di ricerca di Python;\n",
    "- dopo questo, import pyspark funziona anche se Spark non √® stato installato con pip.\n",
    "\n",
    "> Se si **attiva pyspark da CLI**, lo script di avvio di Spark **sa gi√† dov‚Äô√® `SPARK_HOME`**, sa dov‚Äô√® la cartella python di Spark e aggiunge lui quelle cartelle al PYTHONPATH prima di far partire Python. Quindi quando poi parte il REPL e si √® dentro a >>>, *import pyspark* (da fare!) funziona gi√†.\n",
    "---\n",
    "\n",
    "**<u>Seconda differenza</u>**:\n",
    "\n",
    "```python\n",
    "    sc = spark.sparkContext\n",
    "```\n",
    "\n",
    "* Recupera il vecchio `SparkContext` (l‚Äôoggetto ‚Äústorico‚Äù di Spark) dalla sessione e lo mette in `sc`. Serve se si vogliono **usare API RDD o certe funzioni di basso livello**.\n",
    "* Oggi si lavora soprattutto con `spark` (DataFrame, SQL), ma `sc` √® ancora accessibile.\n",
    "\n",
    "Come si chiude il contesto?<br>\n",
    "Per chiuderlo correttamente basta fermare Spark. Ci sono due strade (funzionano entrambe):\n",
    "- Chiudere la sessione\n",
    "```python\n",
    "    spark.stop()\n",
    "```\n",
    "Questo √® il modo ‚Äúmoderno‚Äù. Fermando la SparkSession, Spark ferma anche il SparkContext sotto.\n",
    "- Chiudere direttamente il context\n",
    "```python\n",
    "    sc.stop()\n",
    "```\n",
    "Funziona lo stesso, perch√© **sc √® il SparkContext**.<br>\n",
    "Di solito si preferisce il primo (`spark.stop()`), perch√© √® l‚Äôoggetto principale che abbiamo creato.\n",
    "\n",
    "**Una cosa sola da ricordare**: dopo che √® stato fatto `spark.stop()`, se si prova a usare di nuovo spark o sc nella stessa sessione (pu√≤ essere infatti che qualche oggetto resti in memoria e non esploda subit√≤), pu√≤ ed in genere d√† errore. In quel caso va ricreata la sessione, nel solito modo:\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "```\n",
    "\n",
    "come fatto dalla cella seguente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67b49663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:03:54.188353Z",
     "start_time": "2023-06-29T18:03:42.804794Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Test\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "938bd8d2-27bd-478e-abd3-9dd968aa4aaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:03:54.188353Z",
     "start_time": "2023-06-29T18:03:42.804794Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local') # la creazione del contesto Spark \n",
    "spark = SparkSession(sc)   # la creazione della sessione Spark (sul mio PC 4-5Ghz, 16GB RAM) \n",
    "                           # 8 secondi circa;\n",
    "                           # NON si possono creare 2 sessioni contemporaneamente - d√† errore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38c14183",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T15:57:47.941518Z",
     "start_time": "2023-06-29T15:57:47.429553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-5GVTEG7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test da notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x200174c9550>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark                      # √® l'entry point a Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81de2f",
   "metadata": {},
   "source": [
    "Da qui in avanti la sessione Spark sar√† indicata con l'oggetto `spark`, che agir√† come <u>unico</u> **punto di ingresso** (*entry point*) a tutte le operazioni e dati Spark.<br>\n",
    "Prima della versione Spark 2, infatti, esistevano molteplici entry-point: il contesto Spark, il contesto SQL, il contesto Hive, il contesto Streaming, ecc.<br>\n",
    "[Qui](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html) la documentazione ufficiale della `SparkSession`, che ha diversi *metodi* ed *attributi*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55858adb",
   "metadata": {},
   "source": [
    "Alcuni utili link:\n",
    "* Per info sulla *Spark UI*, vedi [questa documentazione](https://spark.apache.org/docs/latest/web-ui.html).<br>\n",
    "* Per info sugli RDD (Resilient Distributed Dataset (vs DataFrame)), vedi [qui](https://spark.apache.org/docs/latest/rdd-programming-guide.html) e [qui](https://sparkbyexamples.com/spark-rdd-tutorial/).<br>\n",
    "* Per info sui DAG, vedi [qui](https://sparkbyexamples.com/spark/what-is-dag-in-spark/?expand_article=1).\n",
    "* Per la differenza tra SparkContext e SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "688699af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:00.872763Z",
     "start_time": "2023-06-29T18:04:00.857836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version # per eseguire il codice di questo corso √® necessaria la versione Spark dalla 3.1.1 in avanti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168568e3",
   "metadata": {},
   "source": [
    "La sessione Spark pu√≤ essere creata in modo **personalizzato**, cos√¨:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adb18d30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:02.789388Z",
     "start_time": "2023-06-29T18:04:02.398459Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6125259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:05.484660Z",
     "start_time": "2023-06-29T18:04:05.396708Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"MyApp\") \\\n",
    ".master(\"local[6]\") \\\n",
    ".config(\"spark-driver.memory\", \"8g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74f81e",
   "metadata": {},
   "source": [
    "dove:\n",
    "* si d√† un nome (qui fittizio) alla app Spark\n",
    "* si esegue Spark localmente con tutti i miei 6 core\n",
    "* si assegnano 8GB di RAM al processo esecutore\n",
    "\n",
    "Pi√π dettagli disponibili [qui](https://oreil.ly/3bRjy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1302dcd",
   "metadata": {},
   "source": [
    "NB. Se si inseriscono commenti o caratteri testuali (anche se preceduti dal carattere '#') DOPO il backslash '\\' (il separatore di linea di Spark), l'esecuzione della cella d√† errore ed occorre RISCRIVERE la cella ex-novo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01645b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:08.926360Z",
     "start_time": "2023-06-29T18:04:08.417933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-5GVTEG7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20018f58150>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda1fec",
   "metadata": {},
   "source": [
    "NB. Un'applicazione Spark √® spesso chiamata in gergo *un cluster Spark*. E' un'astrazione **logica**, differente dal cluster fisico (macchine multiple)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb1c8b",
   "metadata": {},
   "source": [
    "## La Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f78417",
   "metadata": {},
   "source": [
    "Spark UI √® **l'interfaccia web** della applicazione Spark in esecuzione che **monitora** lo stato ed il consumo delle risorse del nostro cluster Spark. Per default √® disponibile all'indirizzo http://<driver>:4040.<br>\n",
    "\n",
    "La Spark UI offre i seguenti tab:\n",
    "* jobs\n",
    "* stages\n",
    "* storage with DataFrame size and memory use\n",
    "* environment\n",
    "* executors\n",
    "* SQL\n",
    "    \n",
    "Alcuni tab, come quelli relativi allo *streaming*, sono creati in modo *lazy*, cio√® quando sono necessari.<br>\n",
    "Vedi [qui](https://spark.apache.org/docs/latest/web-ui.html) per la documentazione Apache di questa interfaccia con il significato e lo scopo dei vari tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b537b07",
   "metadata": {},
   "source": [
    "La **Spark Web UI** (di default su `http://localhost:4040` mentre l‚Äôapp gira) √® utilissima per job lunghi ‚Äî ma non solo. Essa serve in pratica a:\n",
    "\n",
    "**Quando usarla**:<br>\n",
    "\n",
    "* **Durante lo sviluppo (anche su job brevi)**\n",
    "\n",
    "  * Verificare che il piano eseguito abbia senso (DAG in *Jobs/Stages*).\n",
    "  * Controllare **numero di partizioni**, shuffle inutili, broadcast realmente applicati.\n",
    "  * Capire perch√© una `show()` ‚Äúbanale‚Äù impiega secondi (overhead, shuffle, UDF, ecc.).\n",
    "* **Su job lunghi / produzione**\n",
    "\n",
    "  * Monitorare **progresso** e **colli di bottiglia** in tempo reale.\n",
    "  * Individuare **stragglers** (task molto pi√π lenti degli altri), **skew** di dati, **spills** su disco, **OOM/GC**.\n",
    "  * Confermare che la **cache/persist** sia usata davvero (tab *Storage*).\n",
    "  * Analizzare il comportamento di **Structured Streaming** (tab *Streaming*).\n",
    "\n",
    "**Cosa guardare (tabs chiave)**:\n",
    "\n",
    "* **Jobs / Stages**: tempi per stage, DAG, *Shuffle Read/Write*, *Task Time*, *Spill*.\n",
    "* **SQL**: piano fisico/metriche per DataFrame/SQL (join type, *BroadcastHashJoin*, *AQE* coalescing, ecc.).\n",
    "* **Executors**: memoria usata, CPU, task falliti, log stderr/stdout per diagnosi.\n",
    "* **Storage**: cosa √® in cache, dimensione, livello di persistenza.\n",
    "* **Environment**: configurazioni effettive (utile per scovare override o settaggi mancati).\n",
    "\n",
    "**Segnali da tenere d‚Äôocchio**\n",
    "\n",
    "* **Shuffle Read enorme** rispetto all‚Äôinput ‚Üí join mal posizionati, partizionamento inefficiente.\n",
    "* **Task outlier** molto pi√π lenti ‚Üí possibile **skew** (valori ‚Äúcalamita‚Äù).\n",
    "* **Spill frequenti** (memory ‚Üí disk) ‚Üí partizioni troppo grandi / poca memoria executor.\n",
    "* **Result serialization time/size** alti ‚Üí si stanno riportando troppi dati al driver.\n",
    "* **Troppe o troppo poche partizioni** ‚Üí parallelismo inefficace (vedere `spark.sql.shuffle.partitions`, `repartition/coalesce`).\n",
    "\n",
    "**Pratiche utili**:\n",
    "\n",
    "* **Aprire la UI**: mentre l‚Äôapp gira ‚Üí `http://localhost:4040` (se pi√π app: 4041, 4042‚Ä¶).\n",
    "* **Vedere la UI dopo la fine del job**: usare **History Server** salvando gli **event logs**:\n",
    "\n",
    "  ```text\n",
    "  spark.eventLog.enabled=true\n",
    "  spark.eventLog.dir=hdfs:///logs  (o file:///C:/spark-logs/)\n",
    "  ```\n",
    "\n",
    "  e poi avviare lo *History Server* per consultare job gi√† terminati.\n",
    "\n",
    "‚ùó In sintesi: per job lunghi √® fondamentale (monitoraggio e tuning), ma anche su job corti aiuta a capire subito se il piano √® sano, se la cache funziona e dove si perde tempo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc206d2",
   "metadata": {},
   "source": [
    "## Il dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883c4eb",
   "metadata": {},
   "source": [
    "Useremo un **dataset di dati di pazienti**, presi dal Machine Learning Repository della [UC Irvine](https://it.wikipedia.org/wiki/Universit%C3%A0_della_California_-_Irvine). Questo dataset √® stato prodotto da uno studio di tipo [record linkage](https://en.wikipedia.org/wiki/Record_linkage) (de-duplicazione) eseguito in un ospedale tedesco nel 2010. Esso contiene parecchi milioni di coppie di record paziente matchati tramite differenti criteri come il loro nome, l'indirizzo o la data di nascita. Ad ogni campo di matching √® stato assegnato uno score numerico da 0.0 a 1.0 in base a quanto simili le stringhe erano; i dati quindi sono stati etichettati a mano per identificare quale coppie rappresentavano la stessa persona e quali no. I valori sottostanti dei campi usati per creare il dataset sono stati rimossi per proteggere la privacy dei pazienti. Gli identificatori numerici, cio√® gli score di match dei campi, e le etichette per ogni coppia (match vs non-match) sono stati pubblicati per poter essere utilizzati in uno studio di record linkage.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3bbd0",
   "metadata": {},
   "source": [
    "*The general structure of a record linkage problem is something like this: we have a large collection of records from one or more source systems, and it is likely that multiple records refer to the same underlying entity, such as a customer, a patient, or the location of a business or an event. Each entity has a number of attributes, such as a name, an address, or a birthday, and we will need to use these attributes to find the records that refer to the same entity. Unfortunately, the values of these attributes aren‚Äôt perfect: values might have different formatting, typos, or missing information, which means that a simple equality test on the values of the attributes will cause us to miss a significant number of duplicate records. For example, let‚Äôs compare the business listings shown in the following table:* <br>\n",
    "\n",
    "![](Table_2_1.png)\n",
    "\n",
    "*The first two entries in this table refer to the same small coffee shop, even though a data entry error makes it look as if they are in two different cities (West Hollywood and Hollywood). The second two entries, on the other hand, are actually referring to different business locations of the same chain of coffee shops that happen to share a common address: one of the entries refers to an actual coffee shop, and the other one refers to a local corporate office location. Both of the entries give the official phone number of the corporate headquarters in Seattle. This example illustrates everything that makes record linkage so difficult: even though both pairs of entries look similar to each other, the criteria that we use to make the duplicate/not-duplicate decision is different for each pair. This is the kind of distinction that is easy for a human to understand and identify at a glance, but is difficult for a computer to learn. Record linkage goes by a lot of different names in the literature and in practice: entity resolution, record deduplication, merge-and-purge, and list washing. For our purposes, we refer to this problem as record linkage.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd51dca",
   "metadata": {},
   "source": [
    "Scaricamento dei dati da terminale anaconda (dalla directory corrente, nella sotto-directory *linkage*, da creare):<br>\n",
    "- mkdir linkage<br>\n",
    "- cd linkage/<br>\n",
    "- curl -L -o donation.zip https://bit.ly/1Aoywaq<br>\n",
    "- unzip donation.zip<br>\n",
    "- unzip 'block_*.zip'<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2a61ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Utente\\\\Desktop\\\\salvataggi\\\\SALVATAGGIO DATI\\\\Documents\\\\Seminari\\\\Data Science (corsi)\\\\Corso Big Data con Spark'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd    # la directory corrente (un magic command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6f0ab",
   "metadata": {},
   "source": [
    "Se la procedura da terminale prima descritta NON funziona, fare click sul link ed unzippare i file zip uno per volta (estranedoli nella sotto-directory *linkage*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "34c2cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".appName(\"MyApp\") \\\n",
    ".master(\"local[6]\") \\\n",
    ".config(\"spark-driver.memory\", \"8g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "294ec2db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:17.080787Z",
     "start_time": "2023-06-29T18:04:14.639798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lettura in spark dei 10 file CSV \n",
    "# --> per la lettura di molti file vedi il post stackoverflow 69350586;\n",
    "#     * (Tandon-Ryza-Laserson-Owen-Wills) non funziona;\n",
    "#     con la directory 'linkage/file/csv' non funziona;\n",
    "             \n",
    "prev = spark.read.csv([\n",
    "    \"linkage/file_csv/block_1.csv\",\"linkage/file_csv/block_2.csv\",\n",
    "    \"linkage/file_csv/block_3.csv\",\"linkage/file_csv/block_4.csv\",\n",
    "    \"linkage/file_csv/block_5.csv\",\"linkage/file_csv/block_6.csv\",\n",
    "    \"linkage/file_csv/block_7.csv\",\"linkage/file_csv/block_8.csv\",\n",
    "    \"linkage/file_csv/block_9.csv\",\"linkage/file_csv/block_10.csv\"\n",
    "\n",
    "])    \n",
    "prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5dedbe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.classic.dataframe.DataFrame"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3290416f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:26.592305Z",
     "start_time": "2023-06-29T18:04:25.828800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749142"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c646d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`df.size` alla pandas non esiste in spark.<br>\n",
    "Se vogliamo sapere quante righe e quante colonne (lo ‚Äúshape‚Äù):\n",
    "- **Righe**: `df.count()` ‚üµ action, pu√≤ costare tempo\n",
    "- **Colonne**: `len(df.columns)`\n",
    "\n",
    "Esempio ‚Äúalla pandas‚Äù:\n",
    "```python\n",
    "    n_rows = df.count()              # attenzione: esegue un job\n",
    "    n_cols = len(df.columns)\n",
    "    shape  = (n_rows, n_cols)\n",
    "```\n",
    "\n",
    "Suggerimento: se useremo pi√π volte il dataframe, si pu√≤:\n",
    "```python\n",
    "    df.cache()\n",
    "    df.count()   # materializza in cache\n",
    "```\n",
    "\n",
    "Le action successive saranno pi√π rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af87a597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prev.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7d64f61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:29.361998Z",
     "start_time": "2023-06-29T18:04:29.248415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|  _c0|  _c1|         _c2|         _c3|         _c4|         _c5|    _c6|   _c7|   _c8|   _c9|   _c10|    _c11|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "| 3148| 8326|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|14055|94934|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|33948|34740|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|  946|71870|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|64880|71676|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|25739|45991|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "|62415|93584|           1|           ?|           1|           ?|      1|     1|     1|     1|      0|    TRUE|\n",
      "|27995|31399|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "| 4909|12238|           1|           ?|           1|           ?|      1|     1|     1|     1|      1|    TRUE|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "prev.show(10)  # l'equivalente del comando 'head(n)', √® un metodo dell'oggetto.\n",
    "               # l'output del comando √® sensibile allo zoom! (si riformatta!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52271d92",
   "metadata": {},
   "source": [
    "Quale √® la **dimensione** del dataframe?\n",
    "\n",
    "spark mantiene una stima (`sizeInBytes`) nel piano ottimizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5ee5675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262522417\n"
     ]
    }
   ],
   "source": [
    "est = prev._jdf.queryExecution().optimizedPlan().stats().sizeInBytes()\n",
    "print(est)  # pu√≤ essere una stima grossolana\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc2b39",
   "metadata": {},
   "source": [
    "Memoria usata in cache:<br>\n",
    "Dopo `df.persist()` oppure `df.cache()` si guarda la Spark UI ‚Üí Storage per i MB/partizione. √à il modo pi√π chiaro per **l‚Äôuso reale in RAM degli executor**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca99e0",
   "metadata": {},
   "source": [
    "Bene, come ci aspettavamo, il file CSV √® stato correttamente suddiviso nelle sue singole colonne. Per default, ogni colonna di un file CSV √® trattata come tipo *stringa*, ed i nomi di default delle varie colonne sono *_c0*, *_c1*, *_c2*, ecc. La prima riga √® costituita dagli header di colonna del file csv. Le stringhe contenenti \"?\" indicano la presenza di valori mancanti (*missing values*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3b08546a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:30.076426Z",
     "start_time": "2023-06-29T18:04:30.060467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev.printSchema() # la visualizzazione della struttura (schema, cio√® metadati) dell'oggetto creato \n",
    "                   # (l'equivalente di 'str(dataframe)' in R)\n",
    "                   # nullable = missing values ammessi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b7aa0",
   "metadata": {},
   "source": [
    "E' possibile dire a Spark di **inferire** i nomi delle colonne dagli header del file csv, di sostituire i \"?\" con *null*, e di inferire il data type della colonna dal file csv, in questo modo (tramite le opzioni della API `DataFrameReader`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d68afb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:35.661889Z",
     "start_time": "2023-06-29T18:04:33.154887Z"
    }
   },
   "outputs": [],
   "source": [
    "parsed = spark.read.option(\"header\", \"true\").option(\"nullValue\", \"?\").\\\n",
    "          option(\"inferSchema\", \"true\").csv([\n",
    "            \"linkage/file_csv/block_1.csv\",\"linkage/file_csv/block_2.csv\",\n",
    "            \"linkage/file_csv/block_3.csv\",\"linkage/file_csv/block_4.csv\",\n",
    "            \"linkage/file_csv/block_5.csv\",\"linkage/file_csv/block_6.csv\",\n",
    "            \"linkage/file_csv/block_7.csv\",\"linkage/file_csv/block_8.csv\",\n",
    "            \"linkage/file_csv/block_9.csv\",\"linkage/file_csv/block_10.csv\"\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "170b4466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:38.163929Z",
     "start_time": "2023-06-29T18:04:37.793919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc31a8",
   "metadata": {},
   "source": [
    "NB.Si parla di \"parsificazione\" perch√® Spark deve inferire le informazioni dall'esame di tutto il dataset, estraendo le stringhe da valutare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "33c9cd32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:38.461055Z",
     "start_time": "2023-06-29T18:04:38.346841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "parsed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e737c",
   "metadata": {},
   "source": [
    "   **Significato colonne**:<br>\n",
    "   1. id_1: Internal identifier of first record.\n",
    "   2. id_2: Internal identifier of second record.\n",
    "   3. cmp_fname_c1: agreement of first name, first component\n",
    "   4. cmp_fname_c2: agreement of first name, second component\n",
    "   5. cmp_lname_c1: agreement of family name, first component\n",
    "   6. cmp_lname_c2: agreement of family name, second component\n",
    "   7. cmp_sex: agreement sex\n",
    "   8. cmp_bd: agreement of date of birth, day component\n",
    "   9. cmp_bm: agreement of date of birth, month component\n",
    "   10. cmp_by: agreement of date of birth, year component\n",
    "   11. cmp_plz: agreement of postal code\n",
    "   12. is_match: matching status (TRUE for matches, FALSE for non-matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae8c4d",
   "metadata": {},
   "source": [
    "Per vedere il data type inferito per ogni colonna, possiamo visualizzare lo schema del DataFrame *parsed* in questo modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0a0f4577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:43.628089Z",
     "start_time": "2023-06-29T18:04:43.622115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7144db4a",
   "metadata": {},
   "source": [
    "Per ogni colonna √® riportato il nome della colonna stessa, il data type pi√π appropriato (inferito appunto in base ai dati rilevati nel file CSV per quella colonna), ed un booleano per la presenza/assenza di null-value in quella colonna (default = True).<br>\n",
    "L'inferenza √® fatta in due passi da Spark:\n",
    "* capire/ipotizzare il data-type di ogni colonna (prima scansione del dataset), fattibile anche solo su un campione, se richiesto dall'utente\n",
    "* il parsing effettivo (seconda scansione del dataset)\n",
    "\n",
    "Se **si conosce in anticipo lo schema dei dati in input (la loro struttura)**, si pu√≤ ottenere un significativo beneficio prestazionale in questo modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d86abf8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:48.851861Z",
     "start_time": "2023-06-29T18:04:48.728498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id_1: int, id_2: string, cmp_fname_c1: double, cmp_fname_c2: double, cmp_lname_c1: double, cmp_lname_c2: double, cmp_sex: int, cmp_bd: int, cmp_bm: int, cmp_by: int, cmp_plz: int, is_match: boolean]\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| NULL| id_2|             NULL|        NULL|        NULL|        NULL|   NULL|  NULL|  NULL|  NULL|   NULL|    NULL|\n",
      "|37291|53113|0.833333333333333|        NULL|         1.0|        NULL|      1|     1|     1|     1|      0|    true|\n",
      "|39086|47614|              1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|70031|70237|              1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|84795|97439|              1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"id_1\", IntegerType(), False),\n",
    "    StructField(\"id_2\", StringType(), False),\n",
    "    StructField(\"cmp_fname_c1\", DoubleType(), False),\n",
    "    StructField(\"cmp_fname_c2\", DoubleType(), False),\n",
    "    StructField(\"cmp_lname_c1\", DoubleType(), False),\n",
    "    StructField(\"cmp_lname_c2\", DoubleType(), False),\n",
    "    StructField(\"cmp_sex\", IntegerType(), False),\n",
    "    StructField(\"cmp_bd\", IntegerType(), False),\n",
    "    StructField(\"cmp_bm\", IntegerType(), False),\n",
    "    StructField(\"cmp_by\", IntegerType(), False),\n",
    "    StructField(\"cmp_plz\", IntegerType(), False),\n",
    "    StructField(\"is_match\", BooleanType(), False)\n",
    "                        ])\n",
    "prev2 = spark.read.schema(schema).csv(\"linkage/file_csv/block_1.csv\")\n",
    "print(prev2)\n",
    "prev2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d27007",
   "metadata": {},
   "source": [
    "Ed ora un altro esempio di creazione di un DataFrame di **impiegati** (che qui per semplicit√† chiamiamo `df`) fornendo i dati nel metodo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8af6ed5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:51.909429Z",
     "start_time": "2023-06-29T18:04:51.661723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.classic.dataframe.DataFrame"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d596eaf",
   "metadata": {},
   "source": [
    "L'elenco di tutte le opzioni della API `DataFrameReader` disponibili √® consultabile nella [documentazione di PySpark](https://spark.apache.org/docs/2.0.2/api/java/org/apache/spark/sql/DataFrameReader.html), **nella sezione *CSV***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa406a4",
   "metadata": {},
   "source": [
    "## Formati dei dati e sorgenti dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9ae04",
   "metadata": {},
   "source": [
    "Spark √® fornito con il supporto integrato per la <u>lettura e la scrittura di dataframe</u> in una **variet√† di formati** tramite le API *DataFrameReader* e *DataFrameWriter*. **Oltre al formato CSV** qui discusso, si pu√≤ anche leggere e scrivere dati strutturati dalle seguenti fonti: \n",
    "* *parquet*: un formato di archiviazione dei dati orientato alle colonne (opzione predefinita in Spark), molto utilizzato\n",
    "* *orc*: un altro formato di archiviazione dei dati orientato alle colonne\n",
    "* *json*: supporta molti delle stesse funzionalit√† di inferenza dello schema fornita dal formato CSV\n",
    "* *jdbc*: si connette a un database relazionale tramite lo standard di connessione dati JDBC\n",
    "* *avro*: fornisce un'efficiente serializzazione e deserializzazione dei messaggi quando si utilizza un'origine di streaming come Apache Kafka\n",
    "* *text*: associa ogni riga di un file a un dataframe con una singola colonna di tipo *string*\n",
    "* *image*: carica i file immagine da una directory come un dataframe con una colonna, contenente i dati dell'immagine memorizzati come schema immagine \n",
    "* *libsvm*: un popolare formato di file di testo, usato per rappresentare osservazioni etichettate con caratteristiche sparse \n",
    "* *binary*: legge i file binari e converte ogni file in una singola riga di dataframe (novit√† di Spark 3.0)\n",
    "* *xml*: un semplice formato basato su testo per rappresentare informazioni strutturate come documenti, dati, configurazione o libri (disponibili tramite il pacchetto `spark-xml`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7aebd",
   "metadata": {},
   "source": [
    "# Analisi dei dati con la *dataframe API* di PySpark\n",
    "\n",
    "Utilizziamo il dataframe dei pazienti di prima (`parsed`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b4fcba62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:56.024913Z",
     "start_time": "2023-06-29T18:04:55.941385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: double (nullable = true)\n",
      " |-- cmp_fname_c2: double (nullable = true)\n",
      " |-- cmp_lname_c1: double (nullable = true)\n",
      " |-- cmp_lname_c2: double (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "parsed.printSchema() \n",
    "parsed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67f6c7",
   "metadata": {},
   "source": [
    "**Il significato delle colonne**:<br>\n",
    "* Le prime due colonne sono ID interi che rappresentano i pazienti che sono stati confrontati nel record. Ogni record infatti rappresenta un **confronto**, da qui il prefisso *cmp* (comparison). \n",
    "* Le successive nove colonne contengono i **match score** (magari mancanti), di tipo intero o double, tra i due pazienti, per ognuna delle colonne (nome, data di nascita, residenza, ecc). I match score sono interi quando i possibili risultati del match sono solo 0 (no-match) ed 1 (match), sono double quando sono possibili match parziali. \n",
    "* L'ultima colonna √® un booleano (vero o falso) che indica se i due pazienti confrontati sono gli stessi. Nel gergo del Machine Learning, questa colonna √® detta **risposta** (o target).<br>\n",
    "\n",
    "Lo scopo √® di costruire un **semplice classificatore** che ci permetta di <u>prevedere</u> se un nuovo record sia un match oppure no, in base ai valori dei vari match score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bcb2b4",
   "metadata": {},
   "source": [
    "La API DataFrame ha molti metodi - definiti sempre con () finali - che permettono di lavorare con i dataframe. Iniziamo con 2 metodi gi√† visti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0a7fee71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:59.612610Z",
     "start_time": "2023-06-29T18:04:59.546386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id_1=3148, id_2=8326, cmp_fname_c1=1.0, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=1, is_match=True)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.first() # uno dei metodi pi√π semplici: restituisce la prima riga del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1c6476fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:04:59.798958Z",
     "start_time": "2023-06-29T18:04:59.789950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.first of DataFrame[id_1: int, id_2: int, cmp_fname_c1: double, cmp_fname_c2: double, cmp_lname_c1: double, cmp_lname_c2: double, cmp_sex: int, cmp_bd: int, cmp_bm: int, cmp_by: int, cmp_plz: int, is_match: boolean]>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.first # senza parentesi, d√† la signature del metodo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eeb17221",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:05:03.473766Z",
     "start_time": "2023-06-29T18:05:03.124388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b25d4b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:05:03.506680Z",
     "start_time": "2023-06-29T18:05:03.506680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|cmp_fname_c1|cmp_fname_c2|\n",
      "+------------+------------+\n",
      "|         1.0|        NULL|\n",
      "|         1.0|        NULL|\n",
      "|         1.0|        NULL|\n",
      "|         1.0|        NULL|\n",
      "|         1.0|        NULL|\n",
      "|         1.0|        NULL|\n",
      "|         1.0|        NULL|\n",
      "|         1.0|        NULL|\n",
      "|         1.0|        NULL|\n",
      "|         1.0|        NULL|\n",
      "+------------+------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "parsed.select(\"cmp_fname_c1\", \"cmp_fname_c2\").show(10) # il metodo 'select' estrae il subset di colonne "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6db653",
   "metadata": {},
   "source": [
    "Se il dataframe NON √® troppo grande, pu√≤ essere <u>convertito in pandas</u> con il metodo `toPandas()` e <u>convertito in una array</u> di oggetti *Row* con il metodo `collect()`. Entrambi i metodi restituiscono i dati **alla app client**. Per dataframe estremamente grandi l'utilizzo di questi due metodi pu√≤ essere pericoloso e causare eccezioni *out-of-memory* (verificato!).<br>\n",
    "[Tuttavia, il caricamento dei 10 file CSV di linkage in *pandas* (avendo prima liberato le risorse allocate a Spark tramite la chiusura della connessione spark) sembra funzionare correttamente: vedi notebook *Caricamento di big data in pandas* in questa directory.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e5a3006a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T16:01:08.937138Z",
     "start_time": "2023-06-29T16:01:03.463069Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o448.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pd = \u001b[43mparsed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# per dataframe NON troppo grandi (ad esempio un solo file csv, altrimenti d√† messaggio \u001b[39;00m\n\u001b[32m      2\u001b[39m                         \u001b[38;5;66;03m# di errore \"out-of-memory\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:1792\u001b[39m, in \u001b[36mDataFrame.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtoPandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mPandasDataFrameLike\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1792\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPandasConversionMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:197\u001b[39m, in \u001b[36mPandasConversionMixin.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    194\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m rows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) > \u001b[32m0\u001b[39m:\n\u001b[32m    199\u001b[39m     pdf = pd.DataFrame.from_records(\n\u001b[32m    200\u001b[39m         rows, index=\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns=\u001b[38;5;28mself\u001b[39m.columns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    201\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:443\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Row]:\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m         sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o448.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n"
     ]
    }
   ],
   "source": [
    "pd = parsed.toPandas()  # per dataframe NON troppo grandi (ad esempio un solo file csv, altrimenti d√† messaggio \n",
    "                        # di errore \"out-of-memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7e6db",
   "metadata": {},
   "source": [
    "Infatti! `OutOfMemory`! Dataset troppo grande per il driver (RAM).<br>\n",
    "`toPandas()` porta tutto sul driver. Con 10 CSV pu√≤ saturare la memoria.<br>\n",
    "Fix sicuro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "50e5f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = parsed.limit(50_000).toPandas()          # subset\n",
    "# oppure\n",
    "pdf = parsed.sample(fraction=0.1, seed=42).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03084022",
   "metadata": {},
   "source": [
    "Vediamo ora il metodo `collect`.<br>\n",
    "La seguente cella d√† l'errore `OutOfMemory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d7f893f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o448.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mparsed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# per dataframe NON troppo grandi\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:443\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Row]:\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m         sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utente\\anaconda3\\envs\\py311\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o448.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\r\n"
     ]
    }
   ],
   "source": [
    "parsed.collect() # per dataframe NON troppo grandi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8e013",
   "metadata": {},
   "source": [
    "`df.collect()` √® un‚Äô**action** che:\n",
    "\n",
    "* **Esegue** tutto il piano del DataFrame.\n",
    "* **Trasferisce tutte le righe al driver** (il processo Python locale).\n",
    "* **Restituisce** una **lista di `Row`** Python (es.: `[Row(...), Row(...), ...]`).\n",
    "\n",
    "**Implicazioni pratiche**\n",
    "\n",
    "* **Rischio memoria**: porta *tutti* i dati sul driver ‚Üí su dataset grandi pu√≤ andare in OOM o rallentare molto.\n",
    "* **Ordine**: nessuna garanzia d‚Äôordine se non √® stato fatto un `orderBy(...)`.\n",
    "* **Costo**: computazione + serializzazione + rete (anche in `local[*]`).\n",
    "\n",
    "Quando (non) usarla**\n",
    "\n",
    "* ‚úîÔ∏è OK solo per risultati **piccoli** (campioni, aggregati compatti).\n",
    "* ‚ùå Evitarla su tabelle grandi; preferire:\n",
    "\n",
    "  * `df.limit(n).collect()` o `df.take(n)` per poche righe.\n",
    "  * `df.show(n)` per ispezione veloce.\n",
    "  * `df.toPandas()` (solo se il risultato entra nella RAM del driver; meglio con Arrow).\n",
    "  * `df.toLocalIterator()` per **streaming** al driver senza caricare tutto in una lista (meno memoria, pi√π lento).\n",
    "  * Scrivere su storage: `df.write.parquet(...)` anzich√© riportare i dati al driver.\n",
    "\n",
    "**Mini-esempi**\n",
    "\n",
    "```python\n",
    "    rows = df.collect()             # -> List[Row]\n",
    "    first5 = df.take(5)             # pi√π sicuro se basta un campione\n",
    "    for row in df.toLocalIterator():  # iterazione ‚Äúa flusso‚Äù, meno picchi di RAM\n",
    "        process(row)\n",
    "```\n",
    "\n",
    "> Nota: da **non confondere** con le funzioni SQL `collect_list` / `collect_set` (aggregazioni che producono array per gruppo), che sono un‚Äôaltra cosa rispetto a `DataFrame.collect()`.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869bc5b",
   "metadata": {},
   "source": [
    "Finora, per ogni elaborazione dei dati, Spark ha riaperto il file, ri-parsificato le righe e poi eseguito l'azione richiesta, come ad esempio mostrare le prime righe o contare le righe. E' meglio salvare sul cluster il DataFrame parsificato, subito dopo la prima parsificazione, per non doverla ripetere all'infinito ad ogni azione richiesta, <u>tramite il metodo `cache()`</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "46f71235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:05:23.278065Z",
     "start_time": "2023-06-29T18:05:23.244735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_1: int, id_2: int, cmp_fname_c1: double, cmp_fname_c2: double, cmp_lname_c1: double, cmp_lname_c2: double, cmp_sex: int, cmp_bd: int, cmp_bm: int, cmp_by: int, cmp_plz: int, is_match: boolean]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.cache()   # il DataFrame √® messo in cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab740e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Trasformazioni vs azioni**<br>\n",
    "In Spark occorre distinguere tra *trasformazione* ed *azione*.<br>\n",
    "La creazione di un DataFrame non provoca alcun calcolo distribuito nel cluster. Piuttosto, i DataFrame definiscono dataset <u>logici</u> che sono **passaggi intermedi di un calcolo**. Le operazioni Spark sui dati distribuiti possono essere classificate in **due tipi**: trasformazioni e azioni.<br>\n",
    "Tutte le trasformazioni sono valutate in modo \"pigro\" (*lazy*); cio√®, i loro risultati non sono calcolati immediatamente, ma vengono registrati (*data lineage* = ciclo di vita / storia del dato). Ci√≤ permette a Spark di ottimizzare il piano di query al momento dell'esecuzione dell'azione. Il calcolo distribuito, infatti, √® eseguito solo allla richiesta di un'azione sul DataFrame. Ad esempio, l'azione *count()* restituisce il numero di oggetti in un DataFrame, come visto prima. La azione *collect()* restituisce un'array (formata da oggetti di tipo *Row*, come detto) che **risiede nella memoria locale, non nel cluster**.<br>\n",
    "Lo scopo delle azioni non √® solo quello di retituire i dati al processo locale. L'azione `save()` salva i contenuti del DataFrame in modo persistente, cio√® su disco.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5763ba",
   "metadata": {},
   "source": [
    "**Salvataggio su disco del DataFrame**\n",
    "\n",
    "Varie possibilit√†:\n",
    "\n",
    "```python\n",
    "  # 1) Scorciatoia specifica Parquet\n",
    "  parsed.write.parquet(\"proto.parquet\")\n",
    "\n",
    "  # 2) Con overwrite (se la cartella esiste gi√†)\n",
    "  parsed.write.mode(\"overwrite\").parquet(\"proto.parquet\")\n",
    "\n",
    "  # 3) Via API generica .format(...).save(...)\n",
    "  parsed.write.format(\"parquet\").save(\"proto.parquet\")\n",
    "  # oppure\n",
    "  parsed.write.format(\"parquet\").mode(\"overwrite\").save(\"proto.parquet\")\n",
    "```\n",
    "\n",
    "Note utili:\n",
    "\n",
    "* Spark scrive **una directory** chiamata `proto.parquet` con dentro uno o pi√π file `part-*.parquet` + `_SUCCESS`.\n",
    "  Se vuole un **solo file** (comodo per demo), pu√≤:\n",
    "\n",
    "  ```python\n",
    "    parsed.coalesce(1).write.mode(\"overwrite\").parquet(\"proto.parquet\")\n",
    "  ```\n",
    "* Per rileggerlo:\n",
    "\n",
    "  ```python\n",
    "    df2 = spark.read.parquet(\"proto.parquet\")\n",
    "  ```\n",
    "* Percorsi Windows: usare stringhe raw o lo schema file:\n",
    "\n",
    "  ```python\n",
    "    parsed.write.parquet(r\"C:\\percorso\\proto.parquet\")\n",
    "    # oppure\n",
    "    parsed.write.parquet(\"file:///C:/percorso/proto.parquet\")\n",
    "  ```\n",
    "\n",
    "  Ottima osservazione: √® importante capire bene questo comportamento perch√© **√® diverso** da quello a cui si √® abituati con file ‚Äúnormali‚Äù (come in pandas o Python puro).\n",
    "\n",
    "---\n",
    "\n",
    "üß© In Spark, *‚Äúscrivere un file‚Äù* significa **scrivere una directory**\n",
    "\n",
    "Quando si fa :\n",
    "\n",
    "```python\n",
    "  parsed.write.parquet(\"proto.parquet\")\n",
    "```\n",
    "\n",
    "Spark **non crea un singolo file** `proto.parquet`,\n",
    "bens√¨ una **cartella** (directory) chiamata `proto.parquet/` che contiene:\n",
    "\n",
    "```\n",
    "proto.parquet/\n",
    " ‚îú‚îÄ‚îÄ part-00000-...snappy.parquet\n",
    " ‚îú‚îÄ‚îÄ part-00001-...snappy.parquet\n",
    " ‚îú‚îÄ‚îÄ _SUCCESS\n",
    " ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "perch√©:\n",
    "\n",
    "* ogni *partizione* del DataFrame Spark scrive il suo **pezzo** (`part-xxxxx`),\n",
    "* Spark √® progettato per il calcolo distribuito ‚Üí pi√π nodi = pi√π file.\n",
    "\n",
    "---\n",
    "\n",
    "üí° Se si lavora in locale e si vuole un solo file\n",
    "\n",
    "Si possono **unire le partizioni** prima della scrittura:\n",
    "\n",
    "```python\n",
    "  parsed.coalesce(1).write.mode(\"overwrite\").parquet(\"proto.parquet\")\n",
    "```\n",
    "\n",
    "Cos√¨ si avr√† un‚Äôunica directory con **un solo file `part-00000...parquet`** al suo interno.\n",
    "\n",
    "Se poi ci serve proprio un **singolo file .parquet fisico** (per esempio per aprirlo da Windows Explorer o pandas):\n",
    "\n",
    "```python\n",
    "  import shutil\n",
    "  import glob\n",
    "\n",
    "  # Spark crea la directory proto.parquet/\n",
    "  file = glob.glob(\"proto.parquet/part-*.parquet\")[0]\n",
    "  shutil.copy(file, \"proto_unico.parquet\")   # copia fuori il singolo file\n",
    "```\n",
    "\n",
    "üìÇ **In sintesi**:\n",
    "\n",
    "* ‚Äú**Cartella**‚Äù = directory che contiene i veri file parquet.\n",
    "* Spark usa questo schema sempre, anche per CSV, JSON, ecc.\n",
    "* √à perfettamente normale e previsto dal design ‚Äúdistribuito‚Äù.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a34668",
   "metadata": {},
   "source": [
    "Dopo questa breve rassegna di metodi, calcoliamo le **frequenze di classe**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bfa329e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:05:32.923818Z",
     "start_time": "2023-06-29T18:05:27.690752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|  count|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show() # si noti l'applicazione a cascata dei metodi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77b29b",
   "metadata": {},
   "source": [
    "L'applicazione a cascata dei metodi, utilizzata dal precedente comando, √® un esempio del **modo pulito, veloce ed espressivo di fare analisi dei dati in Spark**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d924dd",
   "metadata": {},
   "source": [
    "Vediamo ora alcune **funzioni statistiche**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ed982625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:05:37.108886Z",
     "start_time": "2023-06-29T18:05:36.848849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+-------------------+\n",
      "|     avg(cmp_sex)|min(cmp_sex)|    stddev(cmp_sex)|\n",
      "+-----------------+------------+-------------------+\n",
      "|0.955001381078048|           0|0.20730111116897781|\n",
      "+-----------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, min, stddev                    # oppure max, stddev_pop, ecc\n",
    "parsed.agg(avg(\"cmp_sex\"), min(\"cmp_sex\"),stddev(\"cmp_sex\")).show()   # 'agg' sta per 'aggregate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4145a0",
   "metadata": {},
   "source": [
    "I nomi di queste funzioni ed anche di altre funzioni della API DataFrame (ad esempio, count, group_by, order_by, ecc) sono volutamente **simili a quelli dell'SQL**. Infatti, abbiamo la possibilit√† di trattare qualsiasi DataFrame <u>come se fosse una tabella relazionale</u> ed accedervi direttamente <u>tramite il linguaggio SQL</u> (ANSI 2003-compliant).<br>\n",
    "A questo scopo la prima cosa da fare √® dire al motore SQL di Spark il nome da associare al DataFrame `parsed`, che infatti non √® disponibile oltre la vita della sessione. L'oggetto *parsed* cio√® √® **temporaneo**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "28521c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:05:40.936493Z",
     "start_time": "2023-06-29T18:05:40.895649Z"
    }
   },
   "outputs": [],
   "source": [
    "parsed.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06445d9",
   "metadata": {},
   "source": [
    "Ora che la tabella temporanea *parsed* √® registrata nel motore SQL di Spark come *linkage*, possiamo farci sopra delle query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "064508ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:05:43.295867Z",
     "start_time": "2023-06-29T18:05:42.994802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_match|    cnt|\n",
      "+--------+-------+\n",
      "|   false|5728201|\n",
      "|    true|  20931|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SELECT is_match, COUNT(*) cnt\n",
    "  FROM linkage\n",
    "  GROUP BY is_match\n",
    "  ORDER BY cnt DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7401a",
   "metadata": {},
   "source": [
    "**E' meglio accedere al dataset con le API DataFrame o con SQL?** Ci sono pro e contro:<br>\n",
    "* SQL √® semplice, potente e conosciuto; permette inoltre di usare le tradizionali connessioni [JDBC](https://it.wikipedia.org/wiki/Java_DataBase_Connectivity) / [ODBC](https://it.wikipedia.org/wiki/Open_Database_Connectivity) per accedere a database come Oracle, PostgreSQL od a tool come Tableau;\n",
    "* per contro, le API DataFrame sono certamente superiori all'SQL per esprimere complesse analisi multi-step in modo dinamico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccc496",
   "metadata": {},
   "source": [
    "[SQL vs PySpark](https://www.linkedin.com/feed/update/urn:li:activity:7120427171269001219?utm_source=share&utm_medium=member_desktop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d5e9a",
   "metadata": {},
   "source": [
    "# API DataFrame vs API RDD (in Spark 4)\n",
    "\n",
    "La differenza non √® tanto ‚ÄúSpark 4‚Äù in s√© (il modello √® lo stesso da Spark 2.x), quanto il **livello di astrazione**:\n",
    "\n",
    "* **RDD** = API storica, a basso livello, collezione distribuita di oggetti.\n",
    "* **DataFrame** = API moderna e principale, tabellare (righe/colonne, con schema), pesantemente ottimizzata e integrata con SQL.\n",
    "\n",
    "In Spark 4 l‚Äôevoluzione delle feature va quasi tutta in direzione **DataFrame/SQL**, mentre l‚ÄôAPI RDD rimane di supporto e per casi speciali. ([Apache Spark][1])\n",
    "\n",
    "---\n",
    "1Ô∏è‚É£ **Modello concettuale**\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)**\n",
    "\n",
    "* Collezione distribuita di elementi generici (object, Row, string, ecc.). ([Baeldung on Kotlin][2])\n",
    "* Non ha uno schema tabellare di default.\n",
    "* Si lavora con funzioni tipo `map`, `flatMap`, `filter`, `reduceByKey`, ecc. ‚Üí stile ‚Äúfunctional‚Äù, molto libero.\n",
    "\n",
    "**DataFrame**\n",
    "\n",
    "* Collezione distribuita **con colonne nominate** e **schema** (tipo tabella SQL). ([Databricks][3])\n",
    "* √à in realt√† un `Dataset[Row]` (in Scala) e in PySpark √® implementato sopra gli RDD. ([Apache Spark][4])\n",
    "* Si ragiona in termini di colonne e operatori SQL-like: `select`, `where`, `groupBy`, `join`, funzioni di aggregazione, ecc.\n",
    "\n",
    "---\n",
    "\n",
    "2Ô∏è‚É£ **Ottimizzazione ed esecuzione**\n",
    "\n",
    "**RDD**\n",
    "\n",
    "* Le trasformazioni su RDD vengono eseguite con un piano relativamente ‚Äúdiretto‚Äù; Spark pu√≤ fare alcune ottimizzazioni, ma **non ha informazione sullo schema** n√© sulle espressioni SQL.\n",
    "* Non entra in gioco il **Catalyst optimizer** in modo profondo.\n",
    "\n",
    "**DataFrame**\n",
    "\n",
    "* Ogni query DataFrame viene tradotta in un **piano logico** e poi ottimizzata da **Catalyst** (riordino di join, pushdown di filtri, rimozione di colonne inutili, ecc.), poi in un **piano fisico** con **whole-stage codegen**. ([Databricks][3])\n",
    "* Risultato pratico: per la maggior parte dei workload di tipo ETL/analytics, i DataFrame sono **molto pi√π veloci e scalabili** rispetto all‚Äôequivalente logica scritta a mano con RDD.\n",
    "\n",
    "In Spark 4 le nuove feature ‚Äúserie‚Äù sono tutte agganciate al mondo SQL/DataFrame, ad esempio:\n",
    "\n",
    "* tipo **VARIANT** per JSON e semi-strutturato, integrato con SQL e DataFrame; ([Decube][5])\n",
    "* nuove **Data Source API (DSV2, Python Data Source)** per scrivere sorgenti/target personalizzati, sempre viste come DataFrame; ([Medium][6])\n",
    "* plotting nativo `.plot()` sulle DataFrame PySpark. ([Databricks][7])\n",
    "\n",
    "Nulla di paragonabile √® stato aggiunto sul lato RDD.\n",
    "\n",
    "---\n",
    "\n",
    "3Ô∏è‚É£ **Schema, tipi e integrazione**\n",
    "\n",
    "**RDD**\n",
    "\n",
    "* Pu√≤ contenere qualsiasi tipo di oggetto (anche strutture annidate arbitrarie).\n",
    "* Nessun concetto di schema a livello di motore SQL ‚Üí niente metadati su tipi e colonne.\n",
    "* Per usare SQL su un RDD serve **promuoverlo a DataFrame** (es. `spark.createDataFrame(rdd, schema)`).\n",
    "\n",
    "**DataFrame**\n",
    "\n",
    "* Ha sempre uno **schema** (colonne + tipi), che pu√≤ essere inferito (es. da CSV/JSON) o definito manualmente. ([algoscale][8])\n",
    "* Si integra naturalmente con:\n",
    "\n",
    "  * **Spark SQL** (`spark.sql(...)`);\n",
    "  * **pandas API on Spark**;\n",
    "  * **MLlib moderno** (pipeline basate su DataFrame);\n",
    "  * **Structured Streaming**. ([Apache Spark][1])\n",
    "\n",
    "---\n",
    "\n",
    "4Ô∏è‚É£ **Type safety e linguaggi**\n",
    "\n",
    "Qui √® pi√π rilevante per Scala/Java che per Python, ma per completezza:\n",
    "\n",
    "* **RDD**: type safety a compile-time (in Scala/Java): √® noto il tipo degli elementi, il compilatore controlla le funzioni applicate. ([algoscale][8])\n",
    "* **DataFrame**: in Scala √® `Dataset[Row]` ‚Üí perde type safety forte; in Python/R comunque non c‚Äô√® compile-time type checking.\n",
    "* In Scala esiste anche **Dataset[T]**, che combina type safety con l‚Äôottimizzazione Catalyst, ma in Spark 4 Python resta centrato su DataFrame. ([Databricks][3])\n",
    "\n",
    "---\n",
    "\n",
    "5Ô∏è‚É£ **Performance pratica (Spark 4 compreso)**\n",
    "\n",
    "In generale, anche su Spark 4:\n",
    "\n",
    "* Per operazioni **relazionali/analitiche** (join, groupBy, aggregazioni, proiezioni) i DataFrame:\n",
    "\n",
    "  * sono **pi√π brevi da scrivere**,\n",
    "  * sfruttano automaticamente gli **optimizer**,\n",
    "  * si integrano con tutte le nuove feature (VARIANT, plotting, datasource V2, ecc.). ([Medium][9])\n",
    "* Gli RDD danno **pi√π controllo** sulla struttura dei dati e sul flusso, ma si paga in verbosit√† e, spesso, in performance.\n",
    "\n",
    "---\n",
    "\n",
    "6Ô∏è‚É£ **Quando usare cosa (oggi, con Spark 4)**\n",
    "\n",
    "**DataFrame API (consigliata come default)**\n",
    "\n",
    "Meglio usare DataFrame quando:\n",
    "\n",
    "* i dati sono **strutturati o semi-strutturati** (CSV, JSON, Parquet, tabelle SQL, Delta, ecc.); ([algoscale][8])\n",
    "* si fanno classiche pipeline di **ETL**, reportistiche, BI, data science;\n",
    "* si vogliono usare **Spark SQL**, **pandas API on Spark**, **MLlib** moderno, **Structured Streaming**;\n",
    "* si vogliono sfruttare le novit√† di Spark 4 (VARIANT, `.plot()`, nuovi data source, ecc.).\n",
    "\n",
    "In pratica, nel mondo Spark 3.x/4.x, tutto ci√≤ che √® ‚Äúdata engineering / analytics‚Äù sta molto meglio in DataFrame.\n",
    "\n",
    "**RDD API (per casi speciali)**\n",
    "\n",
    "Ha ancora senso usare RDD quando:\n",
    "\n",
    "* si lavora con **dati non strutturati** difficilmente mappabili subito in colonne (log grezzi, testi liberi, byte array, ecc.);\n",
    "* serve una logica **molto custom** o algoritmi iterativi complessi (alcuni algoritmi di grafi, di basso livello, ecc.);\n",
    "* si ha bisogno di **controllo fine** su partizionamento e struttura in memoria e il modello tabellare √® troppo stretto;\n",
    "* si devono usare librerie legacy o componenti che **espongono ancora solo RDD** (es. **GraphX**, API MLlib storiche). ([sparkbyexamples.com][10])\n",
    "\n",
    "Spesso, la strategia ‚Äúpulita‚Äù √®:\n",
    "\n",
    "1. leggere e preparare i dati soprattutto con DataFrame/SQL;\n",
    "2. se necessario, scendere a RDD solo per lo step particolarmente custom;\n",
    "3. eventualmente risalire a DataFrame per il resto della pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "**Riassunto**\n",
    "\n",
    "* In Spark 4 **l‚ÄôAPI consigliata √® DataFrame** (pi√π concisa, pi√π veloce, pi√π integrata con tutte le novit√†).\n",
    "* L‚Äô**API RDD** rimane la base del motore e uno strumento di nicchia per casi avanzati o molto custom, ma non √® il punto focale dell‚Äôevoluzione della piattaforma.\n",
    "\n",
    "\n",
    "[1]: https://spark.apache.org/docs/latest/?utm_source=chatgpt.com \"Overview - Spark 4.0.1 Documentation\"\n",
    "[2]: https://www.baeldung.com/java-spark-dataframe-dataset-rdd?utm_source=chatgpt.com \"Apache Spark: Differences between Dataframes, Datasets ...\"\n",
    "[3]: https://www.databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html?utm_source=chatgpt.com \"A Tale of Three Apache Spark APIs: RDDs vs DataFrames ...\"\n",
    "[4]: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html?utm_source=chatgpt.com \"Quickstart: DataFrame ‚Äî PySpark 4.0.1 documentation\"\n",
    "[5]: https://www.decube.io/post/apache-spark-4-release?utm_source=chatgpt.com \"Apache Spark 4 | Comparison with previous version\"\n",
    "[6]: https://medium.com/%40anil.jain.baba/apache-spark-4-0-new-features-with-sample-code-402ebf8d01e1?utm_source=chatgpt.com \"Apache Spark 4.0: New Features with Sample Code\"\n",
    "[7]: https://www.databricks.com/blog/introducing-apache-spark-40?utm_source=chatgpt.com \"Introducing Apache Spark 4.0\"\n",
    "[8]: https://algoscale.com/blog/apache-spark-rdd-vs-dataframe/?utm_source=chatgpt.com \"Apache Spark RDD vs Dataframe - What Is The Difference?\"\n",
    "[9]: https://medium.com/%40kiranvutukuri/rdds-vs-dataframes-vs-datasets-choosing-the-right-part-11-b12d6969885d?utm_source=chatgpt.com \"RDDs vs. DataFrames vs. Datasets Choosing the Right ...\"\n",
    "[10]: https://sparkbyexamples.com/spark/spark-rdd-vs-dataframe-vs-dataset/?utm_source=chatgpt.com \"Spark RDD vs DataFrame vs Dataset\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cc834",
   "metadata": {},
   "source": [
    "Prendiamo un **caso semplicissimo**: calcolare lo stipendio medio per reparto a partire da una lista in memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e8323",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ Dati di esempio (uguali per RDD e DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065ae105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"RDD_vs_DataFrame\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Piccolo dataset in memoria: (reparto, stipendio)\n",
    "data = [\n",
    "    (\"IT\",  3000),\n",
    "    (\"IT\",  3500),\n",
    "    (\"HR\",  2000),\n",
    "    (\"HR\",  2200),\n",
    "    (\"FIN\", 4000),\n",
    "    (\"FIN\", 4200),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa1118",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Versione con RDD\n",
    "\n",
    "Qui si ragiona in termini di **collezioni di tuple** e **operazioni tipo `map`, `reduceByKey`**.<br>\n",
    "Circa **17 secondi** la prima volta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5d88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR 2100.0\n",
      "IT 3250.0\n",
      "FIN 4100.0\n"
     ]
    }
   ],
   "source": [
    "# 1) Creare l'RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "# RDD di tuple: (\"IT\", 3000), ...\n",
    "\n",
    "# 2) (facoltativo) trasformare i valori per calcolare media: (somma, conteggio)\n",
    "#    mappiamo (dept, sal) -> (dept, (sal, 1))\n",
    "paired = rdd.map(lambda row: (row[0], (row[1], 1)))\n",
    "\n",
    "# 3) Riduzione per chiave: somma stipendi e somma conteggi\n",
    "reduced = paired.reduceByKey(\n",
    "    lambda a, b: (a[0] + b[0], a[1] + b[1])   # (somma_sal, somma_n)\n",
    ")\n",
    "\n",
    "# 4) Calcolo della media: somma_sal / somma_n\n",
    "avg_by_dept_rdd = reduced.mapValues(lambda x: x[0] / x[1])\n",
    "\n",
    "for dept, avg_sal in avg_by_dept_rdd.collect():\n",
    "    print(dept, avg_sal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddf5a2",
   "metadata": {},
   "source": [
    "**Osservazioni**:\n",
    "- Nessuno schema, solo tuple.\n",
    "- La logica per la media deve essere implementata ‚Äúa mano‚Äù (somma, conteggi, divisione).\n",
    "- Il codice √® pi√π verboso e l‚Äôottimizzazione √® limitata alle trasformazioni RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9b055",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ Versione con DataFrame\n",
    "\n",
    "Stessa logica, ma vista come tabella: colonne `department`, `salary`.<br>\n",
    "Circa 10 secondi la prima volta.\n",
    "\n",
    "Output tipico:<br>\n",
    "+----------+----------+<br>\n",
    "|department|avg_salary|<br>\n",
    "+----------+----------+<br>\n",
    "|        IT|    3250.0|<br>\n",
    "|        HR|    2100.0|<br>\n",
    "|       FIN|    4100.0|<br>\n",
    "+----------+----------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74fc61d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|department|avg_salary|\n",
      "+----------+----------+\n",
      "|        IT|    3250.0|\n",
      "|        HR|    2100.0|\n",
      "|       FIN|    4100.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Creare il DataFrame con schema\n",
    "df = spark.createDataFrame(data, [\"department\", \"salary\"])\n",
    "\n",
    "# 2) Calcolare lo stipendio medio per reparto\n",
    "avg_by_dept_df = (df\n",
    "    .groupBy(\"department\")\n",
    "    .agg(F.avg(\"salary\").alias(\"avg_salary\"))\n",
    ")\n",
    "\n",
    "avg_by_dept_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e514a96",
   "metadata": {},
   "source": [
    "**Osservazioni**:\n",
    "- Esiste uno schema esplicito (`department: string, salary: bigint/double`).\n",
    "- L‚Äôoperatore `groupBy().agg(F.avg(...))` √® dichiarativo: si dice cosa calcolare, non come implementarlo.\n",
    "- Spark pu√≤ ottimizzare il piano con Catalyst, il motore di ottimizzazione delle query SQL/DataFrame di Spark (per riordinare operazioni, pushdown, codegen, ecc.). Vedi [qui](https://www.databricks.com/it/glossary/catalyst-optimizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173330a",
   "metadata": {},
   "source": [
    "4Ô∏è‚É£ **Confronto ‚Äúdi stile‚Äù in una riga**\n",
    "\n",
    "Con **RDD**: si costruisce manualmente la logica per gruppi, somme, conteggi, medie usando funzioni funzionali.\n",
    "\n",
    "Con **DataFrame**: si scrive una query quasi SQL (`groupBy`, `avg`, `alias`) e si lascia al motore il lavoro ‚Äúsporco‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce99bb",
   "metadata": {},
   "source": [
    "# Statistiche di sintesi veloci per Dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1540cb",
   "metadata": {},
   "source": [
    "Oltre che con SQL o le API DataFrame, le statistiche di base (min, max, mean, count, stddev, ecc) possono essere calcolate in modo semplice <u>anche in questo modo</u>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2d580847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:05:52.623715Z",
     "start_time": "2023-06-29T18:05:51.742374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, id_1: string, id_2: string, cmp_fname_c1: string, cmp_fname_c2: string, cmp_lname_c1: string, cmp_lname_c2: string, cmp_sex: string, cmp_bd: string, cmp_bm: string, cmp_by: string, cmp_plz: string]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = parsed.describe()   # questo metodo ha lo stesso nome che in 'pandas'\n",
    "summary                       # --> il dataframe 'summary' ha una colonna per ogni colonna di 'parsed', la colonna 'summary'\n",
    "                              #     indica la statistica applicata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1c5e5e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:05:52.656626Z",
     "start_time": "2023-06-29T18:05:52.656626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+\n",
      "|summary|       cmp_fname_c1|      cmp_fname_c2|\n",
      "+-------+-------------------+------------------+\n",
      "|  count|            5748125|            103698|\n",
      "|   mean| 0.7129024704437266|0.9000176718903189|\n",
      "| stddev|0.38875835961628014|0.2713176105782334|\n",
      "|    min|                0.0|               0.0|\n",
      "|    max|                1.0|               1.0|\n",
      "+-------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show() # il metodo 'select' sceglie il subset di colonne alle\n",
    "                                                                 # quali applicare le statistiche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7e8b2",
   "metadata": {},
   "source": [
    "Si noti la differenza nei valori di 'count' tra le due colonne scelte. Mentre quasi tutti i record hanno un valore not-null per la prima colonna, meno del 2% dei record ha un valore not-null nella seconda colonna. Per costruire un classificatore utile, dobbiamo utilizzare colonne (predittori o feature) che sono quasi sempre presenti nel DataFrame (a meno che la mancanza sia significativa per il match-nomatch). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286bda2b",
   "metadata": {},
   "source": [
    "Come le varie colonne del DataFrame *parsed* sono **correlate** con la risposta? A questo scopo possiamo calcolare le medesime statistiche di prima <u>in modo separato per i due sotto-campioni match e miss</u>, esprimendo i filtri in uno dei due modi equivalenti (SQL way oppure API DataFrame way, dove la where √® un alias Spark della filter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f17409e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:05:56.790659Z",
     "start_time": "2023-06-29T18:05:55.600845Z"
    }
   },
   "outputs": [],
   "source": [
    "matches = parsed.where(\"is_match = true\")           # filtro SQL-way (oppure API DataFrame way); pu√≤ includere statement validi\n",
    "                                                    # per le clausole where dell'SQL\n",
    "match_summary = matches.describe()\n",
    "\n",
    "misses = parsed.filter(col(\"is_match\") == False)    # filtro API DataFrame way (oppure SQL way); l'operatore '==' √® Python\n",
    "miss_summary = misses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c21666",
   "metadata": {},
   "source": [
    "La seguente analisi cerca di capire se la distribuzione dei vari match score cambi a seconda che il record sia un match od un miss, e non √® facile da fare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "61f5a0ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:01.015076Z",
     "start_time": "2023-06-29T18:06:00.955822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "| 3148| 8326|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|14055|94934|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|33948|34740|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|  946|71870|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|64880|71676|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|25739|45991|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|62415|93584|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      0|    true|\n",
      "|27995|31399|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "| 4909|12238|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|15161|16743|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|31703|37310|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|30213|36558|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|56596|56630|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|16481|21174|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|32649|37094|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|34268|37260|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|66117|69253|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      0|    true|\n",
      "| 2771|31982|         1.0|        NULL|         1.0|        NULL|      0|     1|     1|     1|      1|    true|\n",
      "|23557|29673|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "|37156|39557|         1.0|        NULL|         1.0|        NULL|      1|     1|     1|     1|      1|    true|\n",
      "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "matches.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "770895b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:04.104141Z",
     "start_time": "2023-06-29T18:06:03.955689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20931"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d2298e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:04.833750Z",
     "start_time": "2023-06-29T18:06:04.800868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "|summary|             id_1|             id_2|       cmp_fname_c1|       cmp_fname_c2|        cmp_lname_c1|       cmp_lname_c2|            cmp_sex|             cmp_bd|              cmp_bm|             cmp_by|            cmp_plz|\n",
      "+-------+-----------------+-----------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "|  count|            20931|            20931|              20922|               1333|               20931|                475|              20931|              20925|               20925|              20925|              20902|\n",
      "|   mean|34575.72117911232|51259.95939037791| 0.9973163859635038| 0.9898900320318176|  0.9970152595958817|  0.969370167843852|  0.987291577086618| 0.9970848267622461|  0.9979450418160095| 0.9961290322580645| 0.9584250310975027|\n",
      "| stddev|21950.31285196913|24345.73345377519|0.03650667584833679|0.08251973727615237|0.043118807533945126|0.15345280740388917|0.11201570591216435|0.05391487659807981|0.045286127452170664|0.06209804856731055|0.19962063345931919|\n",
      "|    min|                5|                6|                0.0|                0.0|                 0.0|                0.0|                  0|                  0|                   0|                  0|                  0|\n",
      "|    max|            99946|            99996|                1.0|                1.0|                 1.0|                1.0|                  1|                  1|                   1|                  1|                  1|\n",
      "+-------+-----------------+-----------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_summary.show()  # poco leggibile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b08bfd92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:07.622628Z",
     "start_time": "2023-06-29T18:06:07.586729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|       cmp_fname_c1|       cmp_fname_c2|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|              20922|               1333|\n",
      "|   mean| 0.9973163859635038| 0.9898900320318176|\n",
      "| stddev|0.03650667584833679|0.08251973727615237|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_summary.select(\"summary\",\"cmp_fname_c1\", \"cmp_fname_c2\").show() # il summary per solo 2 colonne (pi√π leggibile!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1e2875e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:08.261596Z",
     "start_time": "2023-06-29T18:06:08.204749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+-----------------+------------------+------------+-------+------+------+------+-------+--------+\n",
      "| id_1| id_2|     cmp_fname_c1|     cmp_fname_c2|      cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----+-----+-----------------+-----------------+------------------+------------+-------+------+------+------+-------+--------+\n",
      "|18206|58536|              1.0|             NULL|               0.0|        NULL|      1|     0|     1|     0|      0|   false|\n",
      "| 6373|47769|            0.875|             NULL| 0.166666666666667|        NULL|      1|     1|     0|     0|      0|   false|\n",
      "|14095|79961|              1.0|             NULL|               0.0|        NULL|      1|     0|     1|     0|      0|   false|\n",
      "|16247|63203|              1.0|             NULL| 0.142857142857143|        NULL|      1|     0|     0|     1|      0|   false|\n",
      "|15525|97448|              1.0|             NULL| 0.222222222222222|        NULL|      1|     0|     1|     1|      0|   false|\n",
      "| 3004|68135|              0.0|             NULL|               1.0|        NULL|      1|     1|     0|     0|      0|   false|\n",
      "| 4331| 7693|            0.875|             NULL| 0.166666666666667|        NULL|      1|     1|     0|     0|      0|   false|\n",
      "|  263|14076|              1.0|             NULL|0.0909090909090909|        NULL|      1|     0|     0|     1|      0|   false|\n",
      "|61638|82485|              1.0|             NULL|               0.0|        NULL|      1|     1|     0|     0|      0|   false|\n",
      "| 4295|89513|              1.0|             NULL| 0.111111111111111|        NULL|      1|     0|     1|     0|      0|   false|\n",
      "| 8810|77731|              1.0|             NULL| 0.222222222222222|        NULL|      1|     0|     1|     0|      0|   false|\n",
      "|55597|87589|              0.0|0.285714285714286|               0.0|        NULL|      1|     1|     1|     1|      0|   false|\n",
      "| 9296|27119|              1.0|             NULL| 0.166666666666667|        NULL|      1|     0|     1|     0|      0|   false|\n",
      "|48870|78963|              1.0|             NULL|              0.25|        NULL|      1|     0|     1|     0|      0|   false|\n",
      "|40561|53170|              1.0|             NULL|             0.125|        NULL|      1|     0|     1|     0|      0|   false|\n",
      "|11741|88816|              1.0|             NULL|               0.0|        NULL|      1|     0|     1|     0|      0|   false|\n",
      "|36252|48314|              1.0|             NULL|               0.0|        NULL|      1|     0|     1|     1|      0|   false|\n",
      "|67307|98994|0.142857142857143|             NULL|             0.125|        NULL|      1|     1|     1|     1|      0|   false|\n",
      "| 7230|15748|0.166666666666667|             NULL|               0.6|        NULL|      1|     0|     0|     0|      0|   false|\n",
      "|51327|84236|              0.4|             NULL|               0.0|        NULL|      1|     0|     1|     0|      0|   false|\n",
      "+-----+-----+-----------------+-----------------+------------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "misses.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2846507f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:11.030804Z",
     "start_time": "2023-06-29T18:06:10.997893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|       cmp_fname_c1|       cmp_fname_c2|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|            5727203|             102365|\n",
      "|   mean| 0.7118634802175091| 0.8988473514090158|\n",
      "| stddev|0.38908060096985553|0.27272090294010215|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "miss_summary.select(\"summary\",\"cmp_fname_c1\", \"cmp_fname_c2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42edc352",
   "metadata": {},
   "source": [
    "Per semplificare l'analisi, dobbiamo [trasporre](https://it.wikipedia.org/wiki/Matrice_trasposta) le righe / colonne dei due DataFrame 'match_summary' e 'miss_summary'. L'inversione di righe / colonne ci permette di mettere in [**join**](SQL_join.png) i due DataFrame ed analizzare le statitiche di sintesi; √® un'operazione spesso detta *pivoting* o *reshaping*:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed529ab0",
   "metadata": {},
   "source": [
    "# Pivot e reshape dei dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b6a78",
   "metadata": {},
   "source": [
    "La trasposizione righe-colonne pu√≤ essere fatta tramite le funzioni di PySpark, oppure, in modo qui pi√π istruttivo, **in pandas**, previa conversione e successiva ri-conversione. Si pu√≤ usare pandas senza timori date le piccole dimensioni dei DataFrame da trasporre (*summary*, creato precedentemente e che riporta le statistiche di base per TUTTO il dataframe *parsed*). Si noti che la conversione-riconversione Spark-pandas √® possibile grazie al progetto *Apache Arrow* che ha implementato un data transfer efficiente tra JVM e Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "260a4897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:14.015293Z",
     "start_time": "2023-06-29T18:06:13.975404Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_p = summary.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b1da8147",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:14.217462Z",
     "start_time": "2023-06-29T18:06:14.208526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(summary_p)   # verifica che il nuovo oggetto 'summary_p' sia effettivamente un dataframe di pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5a7f2401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:14.775300Z",
     "start_time": "2023-06-29T18:06:14.757348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_fname_c2</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_lname_c2</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5748125</td>\n",
       "      <td>103698</td>\n",
       "      <td>5749132</td>\n",
       "      <td>2464</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5736289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>0.7129024704437266</td>\n",
       "      <td>0.9000176718903189</td>\n",
       "      <td>0.3156278193080383</td>\n",
       "      <td>0.3184128315317443</td>\n",
       "      <td>0.955001381078048</td>\n",
       "      <td>0.22446526708507172</td>\n",
       "      <td>0.48885529849763504</td>\n",
       "      <td>0.2227485966810923</td>\n",
       "      <td>0.00552866147434343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>23659.859374488064</td>\n",
       "      <td>23620.487613269695</td>\n",
       "      <td>0.38875835961628014</td>\n",
       "      <td>0.2713176105782334</td>\n",
       "      <td>0.3342336339615828</td>\n",
       "      <td>0.36856706620066537</td>\n",
       "      <td>0.20730111116897781</td>\n",
       "      <td>0.41722972238462636</td>\n",
       "      <td>0.4998758236779031</td>\n",
       "      <td>0.4160909629831756</td>\n",
       "      <td>0.07414914925420046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>99980</td>\n",
       "      <td>100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                id_1                id_2         cmp_fname_c1  \\\n",
       "0   count             5749132             5749132              5748125   \n",
       "1    mean   33324.48559643438   66587.43558331935   0.7129024704437266   \n",
       "2  stddev  23659.859374488064  23620.487613269695  0.38875835961628014   \n",
       "3     min                   1                   6                  0.0   \n",
       "4     max               99980              100000                  1.0   \n",
       "\n",
       "         cmp_fname_c2        cmp_lname_c1         cmp_lname_c2  \\\n",
       "0              103698             5749132                 2464   \n",
       "1  0.9000176718903189  0.3156278193080383   0.3184128315317443   \n",
       "2  0.2713176105782334  0.3342336339615828  0.36856706620066537   \n",
       "3                 0.0                 0.0                  0.0   \n",
       "4                 1.0                 1.0                  1.0   \n",
       "\n",
       "               cmp_sex               cmp_bd               cmp_bm  \\\n",
       "0              5749132              5748337              5748337   \n",
       "1    0.955001381078048  0.22446526708507172  0.48885529849763504   \n",
       "2  0.20730111116897781  0.41722972238462636   0.4998758236779031   \n",
       "3                    0                    0                    0   \n",
       "4                    1                    1                    1   \n",
       "\n",
       "               cmp_by              cmp_plz  \n",
       "0             5748337              5736289  \n",
       "1  0.2227485966810923  0.00552866147434343  \n",
       "2  0.4160909629831756  0.07414914925420046  \n",
       "3                   0                    0  \n",
       "4                   1                    1  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1992a450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:17.754889Z",
     "start_time": "2023-06-29T18:06:17.735940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>id_1</th>\n",
       "      <th>id_2</th>\n",
       "      <th>cmp_fname_c1</th>\n",
       "      <th>cmp_fname_c2</th>\n",
       "      <th>cmp_lname_c1</th>\n",
       "      <th>cmp_lname_c2</th>\n",
       "      <th>cmp_sex</th>\n",
       "      <th>cmp_bd</th>\n",
       "      <th>cmp_bm</th>\n",
       "      <th>cmp_by</th>\n",
       "      <th>cmp_plz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5748125</td>\n",
       "      <td>103698</td>\n",
       "      <td>5749132</td>\n",
       "      <td>2464</td>\n",
       "      <td>5749132</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5748337</td>\n",
       "      <td>5736289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>0.7129024704437266</td>\n",
       "      <td>0.9000176718903189</td>\n",
       "      <td>0.3156278193080383</td>\n",
       "      <td>0.3184128315317443</td>\n",
       "      <td>0.955001381078048</td>\n",
       "      <td>0.22446526708507172</td>\n",
       "      <td>0.48885529849763504</td>\n",
       "      <td>0.2227485966810923</td>\n",
       "      <td>0.00552866147434343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>23659.859374488064</td>\n",
       "      <td>23620.487613269695</td>\n",
       "      <td>0.38875835961628014</td>\n",
       "      <td>0.2713176105782334</td>\n",
       "      <td>0.3342336339615828</td>\n",
       "      <td>0.36856706620066537</td>\n",
       "      <td>0.20730111116897781</td>\n",
       "      <td>0.41722972238462636</td>\n",
       "      <td>0.4998758236779031</td>\n",
       "      <td>0.4160909629831756</td>\n",
       "      <td>0.07414914925420046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>99980</td>\n",
       "      <td>100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                id_1                id_2         cmp_fname_c1  \\\n",
       "0   count             5749132             5749132              5748125   \n",
       "1    mean   33324.48559643438   66587.43558331935   0.7129024704437266   \n",
       "2  stddev  23659.859374488064  23620.487613269695  0.38875835961628014   \n",
       "3     min                   1                   6                  0.0   \n",
       "4     max               99980              100000                  1.0   \n",
       "\n",
       "         cmp_fname_c2        cmp_lname_c1         cmp_lname_c2  \\\n",
       "0              103698             5749132                 2464   \n",
       "1  0.9000176718903189  0.3156278193080383   0.3184128315317443   \n",
       "2  0.2713176105782334  0.3342336339615828  0.36856706620066537   \n",
       "3                 0.0                 0.0                  0.0   \n",
       "4                 1.0                 1.0                  1.0   \n",
       "\n",
       "               cmp_sex               cmp_bd               cmp_bm  \\\n",
       "0              5749132              5748337              5748337   \n",
       "1    0.955001381078048  0.22446526708507172  0.48885529849763504   \n",
       "2  0.20730111116897781  0.41722972238462636   0.4998758236779031   \n",
       "3                    0                    0                    0   \n",
       "4                    1                    1                    1   \n",
       "\n",
       "               cmp_by              cmp_plz  \n",
       "0             5748337              5736289  \n",
       "1  0.2227485966810923  0.00552866147434343  \n",
       "2  0.4160909629831756  0.07414914925420046  \n",
       "3                   0                    0  \n",
       "4                   1                    1  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p.head() # classica funzione di pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6f78a467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:17.992005Z",
     "start_time": "2023-06-29T18:06:17.978034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p.shape  # classica funzione di pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "659a2c9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:18.271782Z",
     "start_time": "2023-06-29T18:06:18.262806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>23659.859374488064</td>\n",
       "      <td>1</td>\n",
       "      <td>99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_2</td>\n",
       "      <td>5749132</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>23620.487613269695</td>\n",
       "      <td>6</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmp_fname_c1</td>\n",
       "      <td>5748125</td>\n",
       "      <td>0.7129024704437266</td>\n",
       "      <td>0.38875835961628014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmp_fname_c2</td>\n",
       "      <td>103698</td>\n",
       "      <td>0.9000176718903189</td>\n",
       "      <td>0.2713176105782334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmp_lname_c1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.3156278193080383</td>\n",
       "      <td>0.3342336339615828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cmp_lname_c2</td>\n",
       "      <td>2464</td>\n",
       "      <td>0.3184128315317443</td>\n",
       "      <td>0.36856706620066537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cmp_sex</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.955001381078048</td>\n",
       "      <td>0.20730111116897781</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cmp_bd</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.22446526708507172</td>\n",
       "      <td>0.41722972238462636</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cmp_bm</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.48885529849763504</td>\n",
       "      <td>0.4998758236779031</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cmp_by</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.2227485966810923</td>\n",
       "      <td>0.4160909629831756</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cmp_plz</td>\n",
       "      <td>5736289</td>\n",
       "      <td>0.00552866147434343</td>\n",
       "      <td>0.07414914925420046</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary         index    count                 mean               stddev  min  \\\n",
       "0                id_1  5749132    33324.48559643438   23659.859374488064    1   \n",
       "1                id_2  5749132    66587.43558331935   23620.487613269695    6   \n",
       "2        cmp_fname_c1  5748125   0.7129024704437266  0.38875835961628014  0.0   \n",
       "3        cmp_fname_c2   103698   0.9000176718903189   0.2713176105782334  0.0   \n",
       "4        cmp_lname_c1  5749132   0.3156278193080383   0.3342336339615828  0.0   \n",
       "5        cmp_lname_c2     2464   0.3184128315317443  0.36856706620066537  0.0   \n",
       "6             cmp_sex  5749132    0.955001381078048  0.20730111116897781    0   \n",
       "7              cmp_bd  5748337  0.22446526708507172  0.41722972238462636    0   \n",
       "8              cmp_bm  5748337  0.48885529849763504   0.4998758236779031    0   \n",
       "9              cmp_by  5748337   0.2227485966810923   0.4160909629831756    0   \n",
       "10            cmp_plz  5736289  0.00552866147434343  0.07414914925420046    0   \n",
       "\n",
       "summary     max  \n",
       "0         99980  \n",
       "1        100000  \n",
       "2           1.0  \n",
       "3           1.0  \n",
       "4           1.0  \n",
       "5           1.0  \n",
       "6             1  \n",
       "7             1  \n",
       "8             1  \n",
       "9             1  \n",
       "10            1  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p = summary_p.set_index('summary').transpose().reset_index()  # trasposizione in pandas\n",
    "summary_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9c8ef682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:20.565843Z",
     "start_time": "2023-06-29T18:06:20.551889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>summary</th>\n",
       "      <th>field</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>23659.859374488064</td>\n",
       "      <td>1</td>\n",
       "      <td>99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_2</td>\n",
       "      <td>5749132</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>23620.487613269695</td>\n",
       "      <td>6</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmp_fname_c1</td>\n",
       "      <td>5748125</td>\n",
       "      <td>0.7129024704437266</td>\n",
       "      <td>0.38875835961628014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmp_fname_c2</td>\n",
       "      <td>103698</td>\n",
       "      <td>0.9000176718903189</td>\n",
       "      <td>0.2713176105782334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmp_lname_c1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.3156278193080383</td>\n",
       "      <td>0.3342336339615828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cmp_lname_c2</td>\n",
       "      <td>2464</td>\n",
       "      <td>0.3184128315317443</td>\n",
       "      <td>0.36856706620066537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cmp_sex</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.955001381078048</td>\n",
       "      <td>0.20730111116897781</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cmp_bd</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.22446526708507172</td>\n",
       "      <td>0.41722972238462636</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cmp_bm</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.48885529849763504</td>\n",
       "      <td>0.4998758236779031</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cmp_by</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.2227485966810923</td>\n",
       "      <td>0.4160909629831756</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cmp_plz</td>\n",
       "      <td>5736289</td>\n",
       "      <td>0.00552866147434343</td>\n",
       "      <td>0.07414914925420046</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "summary         field    count                 mean               stddev  min  \\\n",
       "0                id_1  5749132    33324.48559643438   23659.859374488064    1   \n",
       "1                id_2  5749132    66587.43558331935   23620.487613269695    6   \n",
       "2        cmp_fname_c1  5748125   0.7129024704437266  0.38875835961628014  0.0   \n",
       "3        cmp_fname_c2   103698   0.9000176718903189   0.2713176105782334  0.0   \n",
       "4        cmp_lname_c1  5749132   0.3156278193080383   0.3342336339615828  0.0   \n",
       "5        cmp_lname_c2     2464   0.3184128315317443  0.36856706620066537  0.0   \n",
       "6             cmp_sex  5749132    0.955001381078048  0.20730111116897781    0   \n",
       "7              cmp_bd  5748337  0.22446526708507172  0.41722972238462636    0   \n",
       "8              cmp_bm  5748337  0.48885529849763504   0.4998758236779031    0   \n",
       "9              cmp_by  5748337   0.2227485966810923   0.4160909629831756    0   \n",
       "10            cmp_plz  5736289  0.00552866147434343  0.07414914925420046    0   \n",
       "\n",
       "summary     max  \n",
       "0         99980  \n",
       "1        100000  \n",
       "2           1.0  \n",
       "3           1.0  \n",
       "4           1.0  \n",
       "5           1.0  \n",
       "6             1  \n",
       "7             1  \n",
       "8             1  \n",
       "9             1  \n",
       "10            1  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p = summary_p.rename(columns={'index':'field'})\n",
    "summary_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "54ec940e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:21.710296Z",
     "start_time": "2023-06-29T18:06:21.696335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>33324.48559643438</td>\n",
       "      <td>23659.859374488064</td>\n",
       "      <td>1</td>\n",
       "      <td>99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_2</td>\n",
       "      <td>5749132</td>\n",
       "      <td>66587.43558331935</td>\n",
       "      <td>23620.487613269695</td>\n",
       "      <td>6</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmp_fname_c1</td>\n",
       "      <td>5748125</td>\n",
       "      <td>0.7129024704437266</td>\n",
       "      <td>0.38875835961628014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmp_fname_c2</td>\n",
       "      <td>103698</td>\n",
       "      <td>0.9000176718903189</td>\n",
       "      <td>0.2713176105782334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmp_lname_c1</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.3156278193080383</td>\n",
       "      <td>0.3342336339615828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cmp_lname_c2</td>\n",
       "      <td>2464</td>\n",
       "      <td>0.3184128315317443</td>\n",
       "      <td>0.36856706620066537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cmp_sex</td>\n",
       "      <td>5749132</td>\n",
       "      <td>0.955001381078048</td>\n",
       "      <td>0.20730111116897781</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cmp_bd</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.22446526708507172</td>\n",
       "      <td>0.41722972238462636</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cmp_bm</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.48885529849763504</td>\n",
       "      <td>0.4998758236779031</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cmp_by</td>\n",
       "      <td>5748337</td>\n",
       "      <td>0.2227485966810923</td>\n",
       "      <td>0.4160909629831756</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cmp_plz</td>\n",
       "      <td>5736289</td>\n",
       "      <td>0.00552866147434343</td>\n",
       "      <td>0.07414914925420046</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           field    count                 mean               stddev  min  \\\n",
       "0           id_1  5749132    33324.48559643438   23659.859374488064    1   \n",
       "1           id_2  5749132    66587.43558331935   23620.487613269695    6   \n",
       "2   cmp_fname_c1  5748125   0.7129024704437266  0.38875835961628014  0.0   \n",
       "3   cmp_fname_c2   103698   0.9000176718903189   0.2713176105782334  0.0   \n",
       "4   cmp_lname_c1  5749132   0.3156278193080383   0.3342336339615828  0.0   \n",
       "5   cmp_lname_c2     2464   0.3184128315317443  0.36856706620066537  0.0   \n",
       "6        cmp_sex  5749132    0.955001381078048  0.20730111116897781    0   \n",
       "7         cmp_bd  5748337  0.22446526708507172  0.41722972238462636    0   \n",
       "8         cmp_bm  5748337  0.48885529849763504   0.4998758236779031    0   \n",
       "9         cmp_by  5748337   0.2227485966810923   0.4160909629831756    0   \n",
       "10       cmp_plz  5736289  0.00552866147434343  0.07414914925420046    0   \n",
       "\n",
       "       max  \n",
       "0    99980  \n",
       "1   100000  \n",
       "2      1.0  \n",
       "3      1.0  \n",
       "4      1.0  \n",
       "5      1.0  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p = summary_p.rename_axis(None, axis=1)\n",
    "summary_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "31ee19b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:24.228279Z",
     "start_time": "2023-06-29T18:06:24.212322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 6)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa85d004",
   "metadata": {},
   "source": [
    "Abbiamo trasposto con successo il dataframe 'summary_p' in pandas. Ora dobbiamo ri-convertirlo in Spark con il metodo `createDataFrame`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cb0b0c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:06:26.796977Z",
     "start_time": "2023-06-29T18:06:26.761072Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[field: string, count: string, mean: string, stddev: string, min: string, max: string]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaryT = spark.createDataFrame(summary_p)\n",
    "summaryT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624d4c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T16:12:23.860973Z",
     "start_time": "2023-06-29T16:12:13.712754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------------+-------------------+---+------+\n",
      "|       field|  count|               mean|             stddev|min|   max|\n",
      "+------------+-------+-------------------+-------------------+---+------+\n",
      "|        id_1|5749132|  33324.48559643438| 23659.859374488064|  1| 99980|\n",
      "|        id_2|5749132|  66587.43558331935| 23620.487613269695|  6|100000|\n",
      "|cmp_fname_c1|5748125| 0.7129024704437266|0.38875835961628014|0.0|   1.0|\n",
      "|cmp_fname_c2| 103698| 0.9000176718903189| 0.2713176105782334|0.0|   1.0|\n",
      "|cmp_lname_c1|5749132| 0.3156278193080383| 0.3342336339615828|0.0|   1.0|\n",
      "|cmp_lname_c2|   2464| 0.3184128315317443|0.36856706620066537|0.0|   1.0|\n",
      "|     cmp_sex|5749132|  0.955001381078048|0.20730111116897781|  0|     1|\n",
      "|      cmp_bd|5748337|0.22446526708507172|0.41722972238462636|  0|     1|\n",
      "|      cmp_bm|5748337|0.48885529849763504| 0.4998758236779031|  0|     1|\n",
      "|      cmp_by|5748337| 0.2227485966810923| 0.4160909629831756|  0|     1|\n",
      "|     cmp_plz|5736289|0.00552866147434343|0.07414914925420046|  0|     1|\n",
      "+------------+-------+-------------------+-------------------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaryT.show() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "102003c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T16:12:50.435047Z",
     "start_time": "2023-06-29T16:12:50.429064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- field: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- mean: string (nullable = true)\n",
      " |-- stddev: string (nullable = true)\n",
      " |-- min: string (nullable = true)\n",
      " |-- max: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaryT.printSchema()  # --> ogni campo √® qui trattato come stringa; per fare le statistiche ci serve il formato NUMERICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d3bb9b9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:07:35.031739Z",
     "start_time": "2023-06-29T18:07:34.988822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- field: string (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- mean: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- min: double (nullable = true)\n",
      " |-- max: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conversione delle stringhe in double (tranne che per la colonna 'field'):\n",
    "from pyspark.sql.types import DoubleType\n",
    "for c in summaryT.columns:\n",
    "    if c == 'field':\n",
    "        continue\n",
    "    summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))\n",
    "...\n",
    "summaryT.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191df70c",
   "metadata": {},
   "source": [
    "Definiamo infine una **funzione** che implementi tutta la logica ora vista, e che possa poi essere applicata ai DataFrame 'match_summary' e 'miss_summary': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "49d6d238",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:07:37.932979Z",
     "start_time": "2023-06-29T18:07:37.923006Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def pivot_summary(desc):\n",
    "    # conversione ad un dataframe pandas\n",
    "    desc_p = desc.toPandas()\n",
    "    # trasposizione in pandas\n",
    "    desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
    "    desc_p = desc_p.rename(columns={'index':'field'})\n",
    "    desc_p = desc_p.rename_axis(None, axis=1)\n",
    "    # conversione ad un dataframe Spark\n",
    "    descT = spark.createDataFrame(desc_p)\n",
    "    # conversione delle metriche da stringa a double\n",
    "    for c in descT.columns:\n",
    "        if c == 'field':\n",
    "            continue\n",
    "        else:\n",
    "            descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "        return descT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9f6173c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:07:38.522566Z",
     "start_time": "2023-06-29T18:07:38.448728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[field: string, count: double, mean: string, stddev: string, min: string, max: string]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applicazione della funzione ai due DataFrame:\n",
    "match_summaryT = pivot_summary(match_summary)\n",
    "match_summaryT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7c5af309",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T18:07:39.176092Z",
     "start_time": "2023-06-29T18:07:39.122232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[field: string, count: double, mean: string, stddev: string, min: string, max: string]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_summaryT = pivot_summary(miss_summary)\n",
    "miss_summaryT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97c271",
   "metadata": {},
   "source": [
    "Ora che abbiamo trasposto con successo i due DataFrame possiamo fare il join e confrontarli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dcb1cf",
   "metadata": {},
   "source": [
    "# Join dei dataframe e selezione delle feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376921fa",
   "metadata": {},
   "source": [
    "Finora abbiamo utilizzato Spark SQL e l'API DataFrame solo per filtrare e aggregare i record del dataset; li possiamo anche usare per eseguire **join** (inner, left outer, right outer o full outer) su DataFrame. L'API DataFrame include una funzione di join; tuttavia spesso √® pi√π semplice esprimere questi join utilizzando Spark SQL, soprattutto quando le tabelle che stiamo unendo hanno un numero elevato di colonne in comune e quindi, per chiarezza, vogliamo poter indicare chiaramente a quale colonna ci riferiamo nelle nostre `select`.<br>\n",
    "\n",
    "Creiamo delle viste temporanee per i dataframe `match_summaryT` e `miss_summaryT`, mettiamole in join sulla colonna `field` e calcoliamo alcune semplici statistiche di riepilogo sulle righe risultanti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a5e18c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summaryT.createOrReplaceTempView(\"match_desc\")               # vista SQL temporanea dei match\n",
    "miss_summaryT.createOrReplaceTempView(\"miss_desc\")                 # vista SQL dei miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5b0445a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+\n",
      "|field       |total    |delta               |\n",
      "+------------+---------+--------------------+\n",
      "|cmp_plz     |5736289.0|0.9563812499852176  |\n",
      "|cmp_lname_c2|2464.0   |0.8064147192926266  |\n",
      "|cmp_by      |5748337.0|0.7762059675300512  |\n",
      "|cmp_bd      |5748337.0|0.775442311783404   |\n",
      "|cmp_lname_c1|5749132.0|0.6838772482594513  |\n",
      "|cmp_bm      |5748337.0|0.5109496938298685  |\n",
      "|cmp_fname_c1|5748125.0|0.2854529057459947  |\n",
      "|cmp_fname_c2|103698.0 |0.09104268062280174 |\n",
      "|cmp_sex     |5749132.0|0.032408185250332844|\n",
      "+------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Seleziono e rinomino le colonne \"sensibili\" PRIMA del join\n",
    "a = (match_summaryT\n",
    "     .select(\n",
    "         \"field\",\n",
    "         F.col(\"count\").cast(\"double\").alias(\"a_count\"),\n",
    "         F.col(\"mean\").cast(\"double\").alias(\"a_mean\"))\n",
    "    )\n",
    "\n",
    "b = (miss_summaryT\n",
    "     .select(\n",
    "         \"field\",\n",
    "         F.col(\"count\").cast(\"double\").alias(\"b_count\"),\n",
    "         F.col(\"mean\").cast(\"double\").alias(\"b_mean\"))\n",
    "    )\n",
    "\n",
    "# 2) Join + filtro + calcoli\n",
    "out = (a.join(b, on=\"field\", how=\"inner\")\n",
    "         .where(~F.col(\"field\").isin(\"id_1\", \"id_2\"))\n",
    "         .select(\n",
    "             \"field\",\n",
    "             (F.col(\"a_count\") + F.col(\"b_count\")).alias(\"total\"),\n",
    "             (F.col(\"a_mean\")  - F.col(\"b_mean\")).alias(\"delta\"),\n",
    "         )\n",
    "         .orderBy(F.desc(\"delta\"), F.desc(\"total\"))\n",
    "      )\n",
    "\n",
    "out.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5cd06c",
   "metadata": {},
   "source": [
    "La colonna *Total* conta il numero di valori not null. La colonna *delta* calcola la differenza tra le due medie delle due popolazioni (match e miss). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c49b710",
   "metadata": {},
   "source": [
    "Un buon predittore ha due propriet√† (applicate al datasaet in oggetto):\n",
    "* tende ad avere valori significativamente diversi per match oppure miss (e quindi la differenza tra le due medie sar√† grande),\n",
    "* ed abbastanza spesso esso √® disponibile per ogni record, in questo caso per ogni confronto e dunque per ogni coppia di pazienti.<br>\n",
    "\n",
    "Con questo criterio, il predittore *cmp_fname_c2* non √® molto utile perch√© √® spesso missing, e la differenza tra valori medi di match e miss √® relativamente piccola (0.09) per uno score compreso tra 0 e 1.<br>\n",
    "Anche il predittore *cmp_sex* non √® particolarmente utile perch√©, anche se √® disponibile per qualsiasi coppia di record, la differenza tra le due medie √® solo di 0.03.<br>\n",
    "\n",
    "I predittori *cmp_plz* e *cmp_by*, invece, sono ottimi. Sono quasi sempre valorizzati per entrambi i pazienti confrontati e la loro differenza di media (tra match e miss) √® molto grande (pi√π di 0.77 per entramb i predittori). Anche i predittori *cmp_bd*, *cmp_lname_c1* e *cmp_bm* sembrano utili: sono in genere disponibili nel dataframe e la differenza nei valori medi per match e miss √® notevole.<br>\n",
    "\n",
    "I predittori *cmp_fname_c1* e *cmp_lname_c2* sono ambivalenti: *cmp_fname_c1* non discrimina molto bene (la differenza tra le medie √® solo 0.28) anche se di solito √® disponibile per entrambi i pazienti, mentre *cmp_lname_c2* ha una grande differenza tra medie, ma manca quasi sempre. Non √® del tutto chiaro in quali circostanze dovremmo includere questi predittori nel nostro modello.<br>\n",
    "\n",
    "Per ora, costruiamo un semplice **scoring model** (cio√® un classificatore \"deterministico\", cio√® basati su regole) che classifica la somiglianza delle coppie di pazienti in base alle somme dei valori dei predittori che sono chiaramente utili, e cio√®: *cmp_plz*, *cmp_by*, *cmp_bd*, *cmp_lname_c1* e *cmp_bm*. Per i pochi record per i quali mancano i valori di questi predittori usiamo 0 al posto del valore nullo nella somma.<br>\n",
    "\n",
    "Possiamo avere un'idea approssimativa delle prestazioni di questo semplice modello creando un dataframe degli score calcolati e delle risposte e valutando quanto bene lo score calcolato discrimini o meno tra match e miss per vari valori di **threshold (soglia)**. Ovviamente il valore threshold dev'essere nel **range [0,5]**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d305d",
   "metadata": {},
   "source": [
    "# Scoring e valutazione del modello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aee140",
   "metadata": {},
   "source": [
    "La funzione di scoring somma lo score dei 5 predittori, tramite la funzione `expr` (da `pyspark.sql.functions`) che parsifica la stringa dell'espressione in input nella colonna che rappresenta. La stringa pu√≤ coinvolgere molte colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "58d6077e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T16:30:47.760511Z",
     "start_time": "2023-06-29T16:30:47.746556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cmp_lname_c1 + cmp_plz + cmp_by + cmp_bd + cmp_bm'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# costruiamo la stringa dell'espressione:\n",
    "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "...\n",
    "sum_expression = \" + \".join(good_features) # questa stringa calcola lo score\n",
    "...\n",
    "sum_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dac8b7cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T16:30:50.235332Z",
     "start_time": "2023-06-29T16:30:50.019431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|score|is_match|\n",
      "+-----+--------+\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "scored = parsed.fillna(0, subset=good_features). \\\n",
    "                withColumn('score', expr(sum_expression)). \\\n",
    "                select('score','is_match')\n",
    "scored.show()\n",
    "scored.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8342c5f7",
   "metadata": {},
   "source": [
    "L'ultimo passo nella creazione della funzione di scoring √® la scelta del valore di threshold. Cio√®: quale valore di threshold lo score deve superare perch√® il confronto sia considerato un match?<br>\n",
    "\n",
    "Se il threshold √® troppo alto (ad es. 5), il classificatore (lo scoring model) tender√† a classificare il confronto come miss (e quindi con molti [falsi negativi](https://it.wikipedia.org/wiki/Falso_positivo_e_falso_negativo)); se il threshold √® troppo basso (ad esempio), il classificatore tender√† a classificare il confronto come match (e quindi con molti falsi positivi).Infatti, se cmp_by e cmp_bm sono = 1 il classificatore classificher√† il confronto come match anche se cmp_bd √® diverso (e quindi il confronto √® chiaramente un miss)<br>\n",
    "\n",
    "La scelta del valore ottimale di threshold √® frutto di attente considerazioni di business (**non √® un aspetto tecnico da delegare al Data Scientist!**) ed √® in genere un compromesso (*trade-off*) tra differenti esigenze. In ultima analisi, esso dipende dai costi comparati dei due errori."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04958540",
   "metadata": {},
   "source": [
    "Per facilitare la scelta del valore ottimale di threshold dobbiamo costruire la cosiddetta [**matrice di confusione**](https://it.wikipedia.org/wiki/Matrice_di_confusione), tramite la seguente funzione che\n",
    "* conta il numero di confronti il cui score cade sopra o sotto il valore di threshold \n",
    "* incrociato con il numero di confronti in ognuna delle due classi che erano effettivamente match oppure no.<br>\n",
    "Poich√® non conosciamo ancora il valore ottimale di threshold, la funzione ha come argomenti di input il DataFrame `scored` ed anche il valore di threshold. La funzione utilizza la DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b6ab6e5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T16:30:58.444518Z",
     "start_time": "2023-06-29T16:30:58.440517Z"
    }
   },
   "outputs": [],
   "source": [
    "def Confusion_Matrix(scored: DataFrame, t: DoubleType) -> DataFrame:\n",
    "    return  scored.selectExpr(f\"score >= {t} as above\", \"is_match\").\\\n",
    "          groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).\\\n",
    "          count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6729f9",
   "metadata": {},
   "source": [
    "**Note alla funzione precedente**:<br>\n",
    "* *return*: come si vede, la funzione utilizza il metodo 'selectExpr' della DataFrame API per definire in modo dinamico il valore del campo (field) chiamato 'above' in base al valore dell'argomento 't' (utilizzando la sintassi di formattazione di Python \"f-string\", che permette di sostituire le variabili per nome se la stringa inizia con la lettera 'f'). In questo modo la funzione ha definito il campo 'above'.<br>\n",
    "* *groupBy*: la classica combinazione di groupBy, pivot e count (3 metodi cha abbiamo gi√† visto) che crea la matrice di confusione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613484a",
   "metadata": {},
   "source": [
    "Se applichiamo un valore di threshold alto come 0.4, che significa che la media dei 5 score √® 0.8, riusciamo a filtrare quasi tutti i miss ed, allo stesso tempo, a mantenere oltre il 90% dei match (un buon compromesso!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7d57a8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T16:31:02.365620Z",
     "start_time": "2023-06-29T16:31:01.846663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| true|20871|    637|\n",
      "|false|   60|5727564|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Confusion_Matrix(scored, 4.0).show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa63cd3",
   "metadata": {},
   "source": [
    "Se invece applichiamo un valore di threshold basso (0.2) riusciamo a catturare tutti i match, ma al costo di un alto numero di falsi positivi (la cella in alto a dx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "05181b65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T16:31:06.781642Z",
     "start_time": "2023-06-29T16:31:06.429658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| true|20871|    637|\n",
      "|false|   60|5727564|\n",
      "+-----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Confusion_Matrix(scored, 4.0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccdeef0",
   "metadata": {},
   "source": [
    "Anche se il numero di falsi positivi √® pi√π alto del desiderabile, questo filtro 0.2 (pi√π generoso) rimuove comunque il 90% dei record miss , includendo ogni match positivo. E' possibile fare ancora meglio; come esercizio si pu√≤ provare a trovare un modo per utilizzare alcuni degli altri valori di `MatchData` (sia mancanti che non) per trovare una funzione di scoring che identifichi con successo ogni vero match al costo di meno di 100 falsi positivi.<br>\n",
    "Da poche prove sembrerebbe che il threshold ottimale **a parit√† di costi dei falsi positivi e dei falsi negativi** sia 0.4, perch√® √® quello che fornisce l'accuratezza (*prediction accuracy*) migliore (cio√® pi√π alta). L'accuratezza √® data dalla seguente formula: $$(TP+TN) / (TP+TN+FP+FN)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3a06f",
   "metadata": {},
   "source": [
    "L'accuratezza in questo esempio con threshold = 4 √® **0.9998**.<br>\n",
    "Se invece i costi dei FP e FN sono (significativamente) differenti, l'accuratezza NON √® una metrica suffiente, ed occorre allora differenziare i casi. Supponiamo che il costo del FN sia (molto) maggiore di quello del FP (come spesso √® nella realt√†). In teoria, potrebbe esserci un valore di threshold <> 4.0 che produce una matrice di confusione con un'accuratezza inferiore a 0.9998 ma con un numero di FN < 6 (al costo, necessariamente, di un numero di FP maggiore)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95eba0",
   "metadata": {},
   "source": [
    "E' sempre bene, pr finire, chiudere la sessione Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "58d2b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd55dd",
   "metadata": {},
   "source": [
    "## Confronto di sintassi\n",
    "![](confronto_sintassi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cd4084",
   "metadata": {},
   "source": [
    "# Installazioni fisiche di Spark\n",
    "\n",
    "Apache Spark √® molto flessibile ‚Äî pu√≤ essere **installato fisicamente in diversi ambienti**, a seconda delle esigenze di sviluppo, test o produzione.<br>\n",
    "Vediamo i **principali scenari di installazione fisica**, <u>dal pi√π semplice al pi√π complesso</u>:\n",
    "\n",
    "üñ•Ô∏è 1Ô∏è‚É£ **Installazione locale (Standalone)**\n",
    "\n",
    "Dove: sul PC o notebook personale.<br>\n",
    "Tipico uso: sviluppo, test, esercitazioni, piccole analisi.<br>\n",
    "Descrizione:\n",
    "- Spark viene installato come normale applicazione (ad esempio in C:\\spark su Windows o /opt/spark su Linux).\n",
    "- Funziona in modalit√† ‚Äúlocal‚Äù o ‚Äúlocal[*]‚Äù, cio√® tutto gira in un unico processo Java con pi√π thread.\n",
    "- Non richiede un cluster n√© un HDFS: i dati possono essere in file CSV, Parquet, JSON, ecc.\n",
    "- √à la configurazione tipica per chi lavora con PySpark, VS Code, Jupyter, o Colab.\n",
    "\n",
    "‚ùó E' l'installazione per questo corso. Overnet ed il docente non dispongono di cluster, il cloud √® pi√π complesso e costos per un corso.\n",
    "\n",
    "üß© 2Ô∏è‚É£ **Cluster on-premise (fisico o virtuale)**\n",
    "\n",
    "Dove: su server aziendali o macchine virtuali interne.<br>\n",
    "Tipico uso: produzione interna, elaborazioni batch pianificate.<br>\n",
    "Descrizione:\n",
    "- Spark viene installato su pi√π nodi (master + worker) collegati in rete.\n",
    "- I dati risiedono spesso in HDFS, NAS, o DB aziendali.\n",
    "- Il cluster pu√≤ essere gestito con un resource manager come:\n",
    "    - Standalone (Spark gestisce da solo le risorse),\n",
    "    -YARN (Hadoop),\n",
    "    - Mesos (meno usato oggi),\n",
    "    - Kubernetes (in ambienti containerizzati).\n",
    "\n",
    "‚òÅÔ∏è 3Ô∏è‚É£ **Cloud provider**\n",
    "\n",
    "Dove: su cluster gestiti nel cloud.<br>\n",
    "Tipico uso: produzione scalabile, big data, machine learning distribuito.<br>\n",
    "Descrizione:\n",
    "- Si usa una distribuzione gestita:\n",
    "    - Databricks (basato su Spark, SaaS),\n",
    "    - Amazon EMR,\n",
    "    - Google Dataproc,\n",
    "    - Azure Synapse / HDInsight,\n",
    "    - Spark su Kubernetes (AKS, GKE, EKS, ecc.).\n",
    "- Non serve installare Spark manualmente: il servizio lo fornisce e gestisce automaticamente.\n",
    "\n",
    "üß± 4Ô∏è‚É£ **Container / Docker**\n",
    "\n",
    "Dove: in container isolati, su singola macchina o orchestrati da Kubernetes.<br>\n",
    "Tipico uso: sviluppo riproducibile, ambienti DevOps e CI/CD.<br>\n",
    "Descrizione:\n",
    "- Spark viene ‚Äúimpacchettato‚Äù in una Docker image con Java, Python, librerie, ecc.\n",
    "- √à molto usato per ambienti cloud-native e microservizi.\n",
    "\n",
    "üßÆ 5Ô∏è‚É£ **Integrato in altri ambienti**\n",
    "\n",
    "Dove: all‚Äôinterno di altri sistemi che lo **incorporano**.<br>\n",
    "Esempi:\n",
    "- Databricks, H2O.ai, Cloudera, Snowflake (tramite connettori Spark),\n",
    "- **Google Colab** o Kaggle (**installazione temporanea in RAM**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54d0f0",
   "metadata": {},
   "source": [
    "Vedi l'elenco precedente descritto in [questo grafico](https://www.mermaidchart.com/app/projects/e8daeede-6cd8-4bf7-bb72-ea5a9c2215e1/diagrams/d978205e-bc85-43a9-8214-7c71bae0c6da/version/v0.1/edit) *Mermaid*.\n",
    "\n",
    "Questa √® invece la installazione fisica di *pyspark*:\n",
    "![](pyspark_physical_installation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dab090",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.45px",
    "left": "1152.4px",
    "right": "20px",
    "top": "121px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
