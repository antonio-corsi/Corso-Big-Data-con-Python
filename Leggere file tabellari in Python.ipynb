{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d9d5a227-e219-418b-8d04-f62224e656c5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T08:35:39.679553Z",
          "iopub.status.busy": "2025-10-21T08:35:39.679228Z",
          "iopub.status.idle": "2025-10-21T08:35:39.683523Z",
          "shell.execute_reply": "2025-10-21T08:35:39.683015Z",
          "shell.execute_reply.started": "2025-10-21T08:35:39.679537Z"
        },
        "id": "d9d5a227-e219-418b-8d04-f62224e656c5"
      },
      "source": [
        "<font size=\"6\">**Leggere file tabellari in Python**</font><br>\n",
        "\n",
        "> (c) 2025 Antonio Piemontese"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b0b6f42-42b8-4cc7-94ea-92ec013eb6a5",
      "metadata": {
        "id": "4b0b6f42-42b8-4cc7-94ea-92ec013eb6a5"
      },
      "source": [
        "# I dati in Python (nella Data Science)\n",
        "Nella Data Science spesso siamo interessati:\n",
        "- all'analisi dei **dati passati**, non in tempo reale\n",
        "- all'analisi di un **singolo file**, non del DB\n",
        "\n",
        "Questi dati sono in genere **file tabellari**, cos√¨ chiamati perch√® <u>composti da righe e colonne</u> (2 dimensioni). Sono in genere **creati dagli **utenti**, ricevuti **da altre aziende** o **semplicemente estratti da un DB** (come export).\n",
        "\n",
        "Uno dei formati tabellari pi√π diffusi per l'import di dati in Python √® il formato [**CSV**](https://it.wikipedia.org/wiki/Comma-separated_values). E' un formato praticamente **ubiquo a tutti gli ambienti e tool di *data management* (tabellari)**: excel, google sheet, tutti i DB relazionali, ecc.\n",
        "\n",
        "Ancorch√® in python si possa caricare in memoria **qualsiasi tipo di file** (xml, json, PDF, txt, ecc) ed accedere a **qualsiasi tipo di database** (Oracle, SQLServer, MySQL, ecc), il formato pi√π semplice ed efficiente da caricare in memoria (in un `dataframe` *pandas*) √® il CSV."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae679e73-95ba-4316-836a-6186867428c8",
      "metadata": {
        "id": "ae679e73-95ba-4316-836a-6186867428c8"
      },
      "source": [
        "> ‚ÄúUn file CSV o Excel (**file *tabellare***) pu√≤ sembrare una tabella come nel database, ma √® solo un contenitore di dati grezzi.\n",
        "Una tabella SQL invece √® una struttura controllata, con regole, tipi e relazioni, che il database gestisce in modo coerente, sicuro e transazionale.‚Äù"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f7e0dab-91fb-4180-90ef-6a9ad3acbba1",
      "metadata": {
        "id": "0f7e0dab-91fb-4180-90ef-6a9ad3acbba1"
      },
      "source": [
        "# Leggere file tabellari tramite librerie specifiche"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1fabb53-a6c8-4127-902f-e4e40e57bdaf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T14:10:19.887968Z",
          "iopub.status.busy": "2025-10-21T14:10:19.887715Z",
          "iopub.status.idle": "2025-10-21T14:10:19.891475Z",
          "shell.execute_reply": "2025-10-21T14:10:19.890966Z",
          "shell.execute_reply.started": "2025-10-21T14:10:19.887952Z"
        },
        "id": "a1fabb53-a6c8-4127-902f-e4e40e57bdaf"
      },
      "source": [
        "Per leggere file tabellari <u>csv</u> una **prima possibilit√†** √® **la libreria `csv`**, cio√® il **CSV nativo di Python**.<br>\n",
        "La ragione di questa scelta, che tuttavia presenta molti limiti, √® di evitare di caricare in memoria tutto il package *pandas* (pesante) ed evitare il problema delle dipendenze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09fef0bb-3656-4cab-b542-baeef33d366c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:15:17.495587Z",
          "iopub.status.busy": "2025-10-24T18:15:17.495287Z",
          "iopub.status.idle": "2025-10-24T18:15:17.500288Z",
          "shell.execute_reply": "2025-10-24T18:15:17.499950Z",
          "shell.execute_reply.started": "2025-10-24T18:15:17.495572Z"
        },
        "id": "09fef0bb-3656-4cab-b542-baeef33d366c",
        "outputId": "705fc084-5d20-4861-e33b-6633a9eb95e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['', 'ID', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married', 'Ethnicity', 'Balance']\n",
            "['1', '1', '14.891', '3606', '283', '2', '34', '11', ' Male', 'No', 'Yes', 'Caucasian', '333']\n",
            "['2', '2', '106.025', '6645', '483', '3', '82', '15', 'Female', 'Yes', 'Yes', 'Asian', '903']\n",
            "['3', '3', '104.593', '7075', '514', '4', '71', '11', ' Male', 'No', 'No', 'Asian', '580']\n",
            "['4', '4', '148.924', '9504', '681', '3', '36', '11', 'Female', 'No', 'No', 'Asian', '964']\n",
            "['5', '5', '55.882', '4897', '357', '2', '68', '16', ' Male', 'No', 'Yes', 'Caucasian', '331']\n",
            "['6', '6', '80.18', '8047', '569', '4', '77', '10', ' Male', 'No', 'No', 'Caucasian', '1151']\n",
            "['7', '7', '20.996', '3388', '259', '2', '37', '12', 'Female', 'No', 'No', 'African American', '203']\n",
            "['8', '8', '71.408', '7114', '512', '2', '87', '9', ' Male', 'No', 'No', 'Asian', '872']\n",
            "['9', '9', '15.125', '3300', '266', '5', '66', '13', 'Female', 'No', 'No', 'Caucasian', '279']\n",
            "['10', '10', '71.061', '6819', '491', '3', '41', '19', 'Female', 'Yes', 'Yes', 'African American', '1350']\n",
            "['11', '11', '63.095', '8117', '589', '4', '30', '14', ' Male', 'No', 'Yes', 'Caucasian', '1407']\n",
            "['12', '12', '15.045', '1311', '138', '3', '64', '16', ' Male', 'No', 'No', 'Caucasian', '0']\n",
            "['13', '13', '80.616', '5308', '394', '1', '57', '7', 'Female', 'No', 'Yes', 'Asian', '204']\n",
            "['14', '14', '43.682', '6922', '511', '1', '49', '9', ' Male', 'No', 'Yes', 'Caucasian', '1081']\n",
            "['15', '15', '19.144', '3291', '269', '2', '75', '13', 'Female', 'No', 'No', 'African American', '148']\n",
            "['16', '16', '20.089', '2525', '200', '3', '57', '15', 'Female', 'No', 'Yes', 'African American', '0']\n",
            "['17', '17', '53.598', '3714', '286', '3', '73', '17', 'Female', 'No', 'Yes', 'African American', '0']\n",
            "['18', '18', '36.496', '4378', '339', '3', '69', '15', 'Female', 'No', 'Yes', 'Asian', '368']\n",
            "['19', '19', '49.57', '6384', '448', '1', '28', '9', 'Female', 'No', 'Yes', 'Asian', '891']\n",
            "['20', '20', '42.079', '6626', '479', '2', '44', '9', ' Male', 'No', 'No', 'Asian', '1048']\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "with open(\"Credit_ISLR.csv\", newline=\"\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    for i, row in enumerate(reader):\n",
        "        print(row)\n",
        "        if i >= 20:   # stampa solo le prime 20 righe (0‚Äì19)\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf0b1aa-8afa-40e2-8549-a4a916c5d04d",
      "metadata": {
        "id": "1cf0b1aa-8afa-40e2-8549-a4a916c5d04d"
      },
      "source": [
        "E' **leggerissimo**, ma si deve gestire **tutto ‚Äúa mano‚Äù** (tipi, header, encoding...):\n",
        "- gestione dei tipi (tutto √® stringa) - il modulo `csv` non converte automaticamente i tipi: tutto ci√≤ che legge √® una stringa.\n",
        "- header da gestire manualmente - il modulo `csv` non sa da solo se la prima riga √® intestazione o dati.\n",
        "- problemi di encoding-se il file non √® in UTF-8 (es. √® in `latin-1` o `windows-1252`), `open()` dar√† errore o caratteri strani.\n",
        "- separatori diversi (`,` oppure `;` oppure `\\t`) -i file CSV non sono sempre separati da virgole ‚Äî in Italia spesso da `;` o tabulazioni.\n",
        "- gestione di virgolette e caratteri speciali - se un campo contiene una virgola o un ritorno a capo, il parsing pu√≤ rompersi se non si usano i parametri giusti.\n",
        "- dati mancanti (celle vuote) - non esiste un concetto di `NaN`\n",
        "- con milioni di righe, `csv.reader` √® pi√π veloce di pandas in lettura pura, ma non si pu√≤ filtrare, unire, o fare operazioni sui dati facilmente.\n",
        "\n",
        "**In sintesi**:<br>\n",
        "Usare `csv` puro √® come leggere il file ‚Äúa mano‚Äù: abbiamo il pieno controllo ma anche tutto il lavoro √® a carico nostro. *pandas* (o *Polars*) invece capiscono header, tipi, separatori, encoding, missing, ecc. automaticamente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c553916-4ccd-46c2-b099-4dd448006f6a",
      "metadata": {
        "id": "3c553916-4ccd-46c2-b099-4dd448006f6a"
      },
      "source": [
        "Una **seconda possibilit√†** √® il modulo **`openpyxl`** (per file Excel **.xlsx**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447e03bf-b3ed-4816-bfd1-830e2031f9b3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:15:26.579532Z",
          "iopub.status.busy": "2025-10-24T18:15:26.579333Z",
          "iopub.status.idle": "2025-10-24T18:15:27.063881Z",
          "shell.execute_reply": "2025-10-24T18:15:27.063340Z",
          "shell.execute_reply.started": "2025-10-24T18:15:26.579518Z"
        },
        "id": "447e03bf-b3ed-4816-bfd1-830e2031f9b3",
        "outputId": "2e7ecdf9-e515-413e-9b0f-1cf2df2ceb30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Column1', 'ID', 'Income', 'Limit', 'Rating', 'Cards', 'Age', 'Education', 'Gender', 'Student', 'Married', 'Ethnicity', 'Balance')\n",
            "(1, 1, 14891, 3606, 283, 2, 34, 11, ' Male', 'No', 'Yes', 'Caucasian', 333)\n",
            "(2, 2, 106025, 6645, 483, 3, 82, 15, 'Female', 'Yes', 'Yes', 'Asian', 903)\n",
            "(3, 3, 104593, 7075, 514, 4, 71, 11, ' Male', 'No', 'No', 'Asian', 580)\n",
            "(4, 4, 148924, 9504, 681, 3, 36, 11, 'Female', 'No', 'No', 'Asian', 964)\n",
            "(5, 5, 55882, 4897, 357, 2, 68, 16, ' Male', 'No', 'Yes', 'Caucasian', 331)\n",
            "(6, 6, 8018, 8047, 569, 4, 77, 10, ' Male', 'No', 'No', 'Caucasian', 1151)\n",
            "(7, 7, 20996, 3388, 259, 2, 37, 12, 'Female', 'No', 'No', 'African American', 203)\n",
            "(8, 8, 71408, 7114, 512, 2, 87, 9, ' Male', 'No', 'No', 'Asian', 872)\n",
            "(9, 9, 15125, 3300, 266, 5, 66, 13, 'Female', 'No', 'No', 'Caucasian', 279)\n",
            "(10, 10, 71061, 6819, 491, 3, 41, 19, 'Female', 'Yes', 'Yes', 'African American', 1350)\n",
            "(11, 11, 63095, 8117, 589, 4, 30, 14, ' Male', 'No', 'Yes', 'Caucasian', 1407)\n",
            "(12, 12, 15045, 1311, 138, 3, 64, 16, ' Male', 'No', 'No', 'Caucasian', 0)\n",
            "(13, 13, 80616, 5308, 394, 1, 57, 7, 'Female', 'No', 'Yes', 'Asian', 204)\n",
            "(14, 14, 43682, 6922, 511, 1, 49, 9, ' Male', 'No', 'Yes', 'Caucasian', 1081)\n",
            "(15, 15, 19144, 3291, 269, 2, 75, 13, 'Female', 'No', 'No', 'African American', 148)\n",
            "(16, 16, 20089, 2525, 200, 3, 57, 15, 'Female', 'No', 'Yes', 'African American', 0)\n",
            "(17, 17, 53598, 3714, 286, 3, 73, 17, 'Female', 'No', 'Yes', 'African American', 0)\n",
            "(18, 18, 36496, 4378, 339, 3, 69, 15, 'Female', 'No', 'Yes', 'Asian', 368)\n",
            "(19, 19, 4957, 6384, 448, 1, 28, 9, 'Female', 'No', 'Yes', 'Asian', 891)\n"
          ]
        }
      ],
      "source": [
        "from openpyxl import load_workbook\n",
        "\n",
        "wb = load_workbook(\"Credit_ISLR.xlsx\")\n",
        "ws = wb.active\n",
        "\n",
        "for i, row in enumerate(ws.iter_rows(values_only=True)):\n",
        "    print(row)\n",
        "    if i >= 19:    # indice parte da 0 ‚Üí 0‚Äì19 = 20 righe\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e637d41-8fbc-4d8e-a75d-5525141c1167",
      "metadata": {
        "id": "0e637d41-8fbc-4d8e-a75d-5525141c1167"
      },
      "source": [
        "Una **terza** possibilit√† √® il modulo `xlrd` o `xlwt` - sempre per file excel\n",
        "- `xlrd` ‚Üí lettura di vecchi .xls (Excel 97-2003)\n",
        "- `xlwt` ‚Üí scrittura di .xls<br>\n",
        "\n",
        "‚ö†Ô∏è Deprecati per `.xlsx`, quindi oggi meno consigliati."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83e830e7-b719-4de9-a97a-833f78a85cd2",
      "metadata": {
        "id": "83e830e7-b719-4de9-a97a-833f78a85cd2"
      },
      "source": [
        "![](sintesi_formati_tabellari.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e4687b0-f8d9-414a-a71e-05b9fc4226a6",
      "metadata": {
        "id": "2e4687b0-f8d9-414a-a71e-05b9fc4226a6"
      },
      "source": [
        "# Il caricamento in pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d825a892-c123-438e-83f0-ef3b519f894c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T14:46:39.234929Z",
          "iopub.status.busy": "2025-10-21T14:46:39.234603Z",
          "iopub.status.idle": "2025-10-21T14:46:39.238351Z",
          "shell.execute_reply": "2025-10-21T14:46:39.237877Z",
          "shell.execute_reply.started": "2025-10-21T14:46:39.234914Z"
        },
        "id": "d825a892-c123-438e-83f0-ef3b519f894c"
      },
      "source": [
        "Se si vuole solo leggere/scrivere file (tabellari) senza installare grandi librerie, si pu√≤ usare `csv` o `openpyxl`, come visto prima.\n",
        "\n",
        "Altrimenti, si usano i [**dataframe**](https://en.wikipedia.org/wiki/Pandas_(software)#DataFrames) che sono la **struttura dati pi√π utilizzata nella Data Science**:\n",
        "- risiedono in memoria\n",
        "- possono essere caricati da disco o scaricati su disco con le funzioni `pd.read_***` o con i metodi `df.to_XXX` per i seguenti formati:<br>\n",
        "*clipboard, csv, excel, html, json, parquet, pickle, sas, sql, spss, stata, xml*.\n",
        "\n",
        "Per caricare un file tabellare *csv* oppure *xslx* in un dataframe si usano queste due funzioni *pandas*:\n",
        "```python\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv('dati.csv)\n",
        "    df = pd.read_excel('dati.xlsx')\n",
        "```\n",
        "\n",
        "Se invece si vogliono alternative **pi√π performanti**, oggi si usa molto `cuDF` (con GPU) oppure `Polars`.\n",
        "\n",
        "\n",
        "<p style=\"color:red; font-size:18px; font-weight:bold;\">\n",
        "üö® I file tabellari (ed anche le tabelle SQL che vedremo pi√π avanti) sono in genere caricati in un dataframe pandas üö®\n",
        "</p>\n",
        "Pandas √® comodo ma non √® l‚Äôunico modo per importare file tabellari in python.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aad194c6-cfc5-4629-acff-81f2ed49d549",
      "metadata": {
        "id": "aad194c6-cfc5-4629-acff-81f2ed49d549"
      },
      "source": [
        "# Excel o csv?\n",
        "Qual √® il formato migliore per importare file tabellari? Excel o csv?<br>\n",
        "Dipende dallo scopo e dal contesto, ma **nella maggior parte dei casi CSV √® pi√π efficiente, trasparente e robusto, mentre Excel √® pi√π comodo per l‚Äôutente umano**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d64c30a-84f9-4f1d-a18f-842a6a4b31d9",
      "metadata": {
        "id": "9d64c30a-84f9-4f1d-a18f-842a6a4b31d9"
      },
      "source": [
        "---\n",
        "\n",
        "Facciamo un confronto **CSV vs Excel** dal punto di vista <u>tecnico</u> e da quello <u>umano</u>.\n",
        "\n",
        "---\n",
        "\n",
        "**1. Dal punto di vista tecnico (*pandas*)**\n",
        "\n",
        "üü© **CSV: √® il formato ‚Äúnativo‚Äù per pandas**\n",
        "- √® pi√π leggero da leggere e scrivere:\n",
        "```python\n",
        "    df = pd.read_csv(\"dati.csv\")\n",
        "```\n",
        "- pandas lo apre molto pi√π velocemente (soprattutto file grandi).\n",
        "- nessuna dipendenza esterna (solo Python standard).\n",
        "- perfetto per scambi tra sistemi o integrazione con altri linguaggi.\n",
        "- √® testuale e trasparente ‚Üí lo si pu√≤ aprire anche con un editor o versionare su Git.\n",
        "\n",
        "‚ö†Ô∏è Svantaggi:\n",
        "- perde formattazioni, formule, fogli multipli (ad esempio se convertito da excel)\n",
        "- non ha metadati (tipi, date, ecc.), quindi pandas deve ‚Äúindovinarli‚Äù (**inferirli**).\n",
        "\n",
        "üü® **Excel (XLS / XLSX): comodo ma pi√π complesso**\n",
        "- supporta fogli multipli, celle formattate, tipi pi√π espliciti.\n",
        "- ottimo per file provenienti da utenti umani o report aziendali.\n",
        "\n",
        "```python\n",
        "    df = pd.read_excel(\"dati.xlsx\")\n",
        "```\n",
        "\n",
        "‚ö†Ô∏è Svantaggi:\n",
        "- meno stabile su grandi volumi (>100 000 righe).\n",
        "- usa librerie esterne (`openpyxl`, `xlrd`, `pyxlsb` ecc.) - come visto prima\n",
        "- pi√π lento da leggere e scrivere.\n",
        "- gli errori di formattazione (celle unite, righe vuote, formule, ecc.) spesso creano problemi.\n",
        "- meno adatto all‚Äôautomazione massiva (batch ETL, pipeline, ecc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b5d500-d1ee-4b7d-bb33-87f8d95e15d5",
      "metadata": {
        "id": "95b5d500-d1ee-4b7d-bb33-87f8d95e15d5"
      },
      "source": [
        "---\n",
        "**\"Librerie esterne\"**: cosa si intende?<br>\n",
        "La funzione `pd.read_excel()` √® integrata in pandas, ma non fa tutto da sola: per funzionare **ha bisogno di librerie esterne** che gestiscono concretamente il formato Excel.<br>\n",
        "Cio√®, come funziona davvero `pd.read_excel()`?<br>\n",
        "Quando si chiama:\n",
        "```python\n",
        "import pandas as pd\n",
        "df = pd.read_excel(\"dati.xlsx\")\n",
        "```\n",
        "pandas:\n",
        "- riconosce il formato del file (es. `.xls`, `.xlsx`, `.xlsb`)\n",
        "- usa un ‚Äúmotore‚Äù esterno (engine) per leggere i dati\n",
        "- trasforma ci√≤ che legge in un `DataFrame`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7693a008-8fd0-47b2-ba08-c65f68a81034",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T15:06:22.273393Z",
          "iopub.status.busy": "2025-10-21T15:06:22.273077Z",
          "iopub.status.idle": "2025-10-21T15:06:22.291626Z",
          "shell.execute_reply": "2025-10-21T15:06:22.291290Z",
          "shell.execute_reply.started": "2025-10-21T15:06:22.273373Z"
        },
        "id": "7693a008-8fd0-47b2-ba08-c65f68a81034"
      },
      "source": [
        "**2. Confronto dal punto di vista del ‚Äúdata pipeline‚Äù**:\n",
        "\n",
        "![](cfr_pipeline.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e55ed59-708f-4da5-913c-3c84dd371dde",
      "metadata": {
        "id": "6e55ed59-708f-4da5-913c-3c84dd371dde"
      },
      "source": [
        "**3. Performance: confronto indicativo**:<br>\n",
        "![](performance_csv_excel.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33cfda21-e530-405e-8f22-4d3672241961",
      "metadata": {
        "id": "33cfda21-e530-405e-8f22-4d3672241961"
      },
      "source": [
        "**4. In pratica**:<br>\n",
        "üëâ Se il file **arriva da un sistema o un processo automatico** --> scegliere CSV.<br>\n",
        "üëâ Se il file **arriva da un collega o un cliente che lavora in Excel** --> usa Excel, pulirlo e poi convertirlo in CSV o Parquet.<br>\n",
        "\n",
        "üí° Molti flussi aziendali fanno proprio cos√¨:\n",
        "1. `read_excel()`<br>\n",
        "\n",
        "2.  **pulizia dati in *pandas***<br>\n",
        "\n",
        "3. `to_csv()` o `to_parquet()` per uso interno / storage efficiente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e00a387",
      "metadata": {
        "id": "9e00a387"
      },
      "source": [
        "# L'importanza e la diffusione del formato *csv*\n",
        "Vediamo pi√π in dettaglio **perch√® il formato CSV √® cos√¨ ubiquo nel mondo dei dati**. Non c'√® praticamente ambiente di *data management* (excel, google sheet, DB relazionali, ecc) che non permetta import ed export di file csv.\n",
        "\n",
        "![](importanza_csv.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4810542f",
      "metadata": {
        "id": "4810542f"
      },
      "source": [
        "# Ragioni tecniche della diffusione di csv\n",
        "Ci sono diversi **motivi tecnici molto concreti** che spiegano perch√© il formato CSV √® cos√¨ onnipresente nel mondo dei dati.<br>\n",
        "Ecco una tabella riassuntiva chiara e tecnica:\n",
        "\n",
        "![](diffusione_csv.png)\n",
        "\n",
        "üí° In breve:<br>\n",
        "\n",
        "Il CSV √® il **‚Äúminimo comune denominatore‚Äù dei dati tabellari**: semplice, interoperabile, senza dipendenze e compatibile con tutto ‚Äî da Excel a Spark.<br>\n",
        "Non √® perfetto (niente tipi, schema, compressione o metadati), ma proprio **la sua povert√† strutturale √® la sua forza**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99d45255-be27-4a5d-9932-790901b179e2",
      "metadata": {
        "id": "99d45255-be27-4a5d-9932-790901b179e2"
      },
      "source": [
        "# Leggere file CSV in pandas\n",
        "\n",
        "Come detto, nella Data Science, spesso, NON siamo interessati alla fotografia dei dati in tempo reale. In genere sia le analisi (EDA) che i modelli (predittivi) - i due obiettivi tipici del ML/AI - sono fatti su dati **passati (congelati)**, sia perch√® manca l'interesse sui dati recentissimi sia perch√® i dati in input ai modelli devono essere preprocessati (controllo qualit√†, gestione outlier, gestione MV, gestione duplicati, standardizzazione, ecc). Quindi, sebbene Python/pandas siano in gradi di accedere **direttamente** a tabelle SQL remote (tramite i metodi `pd.read_sql_query` per le query e `pd.read_sql_table` per il download dell'intera tabella), tuttavia √® molto pi√π veloce caricare i dati da un file esterno locale, che pu√≤ essere di vari formati (csv, json, parquest, ecc).\n",
        "\n",
        "## 3 note tecniche sul formato CSV\n",
        "\n",
        "* i due argomenti principali del metodo pandas `read_csv` sono  `sep`, che indica il carattere usato nel file per \"separare\" le colonne (in genere √® \",\" oppure il \";\") e `header`, che indica la presenza (e l'eventuale numero) di righe di heading (intestazione).\n",
        "* ci sono diversi formati csv disponibili da excel; occorre scegliere quello indicato in figura sottostante con la freccia rossa; in proposito vedi anche il video [*Come convertire Excel in CSV (veloce e corretto)*](https://www.youtube.com/watch?v=S7SpFIg5iVM) della *Excel Tutorials by EasyClick Academy*.\n",
        "  \n",
        "  ![](tipi_csv.png)\n",
        "* [pro e contro](https://towardsdatascience.com/why-i-stopped-dumping-dataframes-to-a-csv-and-why-you-should-too-c0954c410f8f) del formato csv\n",
        "\n",
        "Un file csv √® testuale e dunque pu√≤ essere letto anche in Notepad."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cae4ec6c-ed02-4ecc-b78c-b8feb1efda58",
      "metadata": {
        "id": "cae4ec6c-ed02-4ecc-b78c-b8feb1efda58"
      },
      "source": [
        "Carichiamo in *pandas* il famoso file bancario `Credit_ISLR`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f8f076-dc8b-488e-8659-e63b4e9f43d8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T08:43:33.738874Z",
          "iopub.status.busy": "2025-10-21T08:43:33.738657Z",
          "iopub.status.idle": "2025-10-21T08:43:33.761516Z",
          "shell.execute_reply": "2025-10-21T08:43:33.761072Z",
          "shell.execute_reply.started": "2025-10-21T08:43:33.738860Z"
        },
        "id": "64f8f076-dc8b-488e-8659-e63b4e9f43d8",
        "outputId": "b5f5374a-7219-4142-9354-234ed26c8527"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ID</th>\n",
              "      <th>Income</th>\n",
              "      <th>Limit</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Cards</th>\n",
              "      <th>Age</th>\n",
              "      <th>Education</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Student</th>\n",
              "      <th>Married</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>Balance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>14.891</td>\n",
              "      <td>3606</td>\n",
              "      <td>283</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>11</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>106.025</td>\n",
              "      <td>6645</td>\n",
              "      <td>483</td>\n",
              "      <td>3</td>\n",
              "      <td>82</td>\n",
              "      <td>15</td>\n",
              "      <td>Female</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Asian</td>\n",
              "      <td>903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>104.593</td>\n",
              "      <td>7075</td>\n",
              "      <td>514</td>\n",
              "      <td>4</td>\n",
              "      <td>71</td>\n",
              "      <td>11</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Asian</td>\n",
              "      <td>580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>148.924</td>\n",
              "      <td>9504</td>\n",
              "      <td>681</td>\n",
              "      <td>3</td>\n",
              "      <td>36</td>\n",
              "      <td>11</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Asian</td>\n",
              "      <td>964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>55.882</td>\n",
              "      <td>4897</td>\n",
              "      <td>357</td>\n",
              "      <td>2</td>\n",
              "      <td>68</td>\n",
              "      <td>16</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>396</td>\n",
              "      <td>396</td>\n",
              "      <td>12.096</td>\n",
              "      <td>4100</td>\n",
              "      <td>307</td>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>13</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>397</td>\n",
              "      <td>397</td>\n",
              "      <td>13.364</td>\n",
              "      <td>3838</td>\n",
              "      <td>296</td>\n",
              "      <td>5</td>\n",
              "      <td>65</td>\n",
              "      <td>17</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>African American</td>\n",
              "      <td>480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>398</td>\n",
              "      <td>398</td>\n",
              "      <td>57.872</td>\n",
              "      <td>4171</td>\n",
              "      <td>321</td>\n",
              "      <td>5</td>\n",
              "      <td>67</td>\n",
              "      <td>12</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>399</td>\n",
              "      <td>399</td>\n",
              "      <td>37.728</td>\n",
              "      <td>2525</td>\n",
              "      <td>192</td>\n",
              "      <td>1</td>\n",
              "      <td>44</td>\n",
              "      <td>13</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>400</td>\n",
              "      <td>400</td>\n",
              "      <td>18.701</td>\n",
              "      <td>5524</td>\n",
              "      <td>415</td>\n",
              "      <td>5</td>\n",
              "      <td>64</td>\n",
              "      <td>7</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Asian</td>\n",
              "      <td>966</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows √ó 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0   ID   Income  Limit  Rating  Cards  Age  Education  Gender  \\\n",
              "0             1    1   14.891   3606     283      2   34         11    Male   \n",
              "1             2    2  106.025   6645     483      3   82         15  Female   \n",
              "2             3    3  104.593   7075     514      4   71         11    Male   \n",
              "3             4    4  148.924   9504     681      3   36         11  Female   \n",
              "4             5    5   55.882   4897     357      2   68         16    Male   \n",
              "..          ...  ...      ...    ...     ...    ...  ...        ...     ...   \n",
              "395         396  396   12.096   4100     307      3   32         13    Male   \n",
              "396         397  397   13.364   3838     296      5   65         17    Male   \n",
              "397         398  398   57.872   4171     321      5   67         12  Female   \n",
              "398         399  399   37.728   2525     192      1   44         13    Male   \n",
              "399         400  400   18.701   5524     415      5   64          7  Female   \n",
              "\n",
              "    Student Married         Ethnicity  Balance  \n",
              "0        No     Yes         Caucasian      333  \n",
              "1       Yes     Yes             Asian      903  \n",
              "2        No      No             Asian      580  \n",
              "3        No      No             Asian      964  \n",
              "4        No     Yes         Caucasian      331  \n",
              "..      ...     ...               ...      ...  \n",
              "395      No     Yes         Caucasian      560  \n",
              "396      No      No  African American      480  \n",
              "397      No     Yes         Caucasian      138  \n",
              "398      No     Yes         Caucasian        0  \n",
              "399      No      No             Asian      966  \n",
              "\n",
              "[400 rows x 13 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df_credit = pd.read_csv(\"Credit_ISLR.csv\",header=0)\n",
        "df_credit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "973f89a5-9591-40a8-8bfb-75d1643f41ae",
      "metadata": {
        "id": "973f89a5-9591-40a8-8bfb-75d1643f41ae"
      },
      "source": [
        "Come si vede, la funzione *pandas* `read_csv` ha creato automaticamente un **indice** numerico, sicch√® l'indice originario `ID` √® ora <u>ridondante</u>, ed ha aggiunto una colonna `Unnamed: 0` (vedremo pi√π avanti perch√®). E' bene cancellarle entrambe perch√® inutili."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adedb20e-72fa-4c93-95ed-4a48b2853d83",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T08:51:08.184431Z",
          "iopub.status.busy": "2025-10-21T08:51:08.183921Z",
          "iopub.status.idle": "2025-10-21T08:51:08.191669Z",
          "shell.execute_reply": "2025-10-21T08:51:08.191323Z",
          "shell.execute_reply.started": "2025-10-21T08:51:08.184414Z"
        },
        "id": "adedb20e-72fa-4c93-95ed-4a48b2853d83"
      },
      "outputs": [],
      "source": [
        "df_credit.drop(columns=['Unnamed: 0', 'ID'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d44fbb-2b10-4079-9d36-05411f3ce003",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T08:51:24.310829Z",
          "iopub.status.busy": "2025-10-21T08:51:24.310609Z",
          "iopub.status.idle": "2025-10-21T08:51:24.318242Z",
          "shell.execute_reply": "2025-10-21T08:51:24.317874Z",
          "shell.execute_reply.started": "2025-10-21T08:51:24.310813Z"
        },
        "id": "64d44fbb-2b10-4079-9d36-05411f3ce003",
        "outputId": "d4613669-fb74-4ea0-940d-e6dcd22ae105"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Income</th>\n",
              "      <th>Limit</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Cards</th>\n",
              "      <th>Age</th>\n",
              "      <th>Education</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Student</th>\n",
              "      <th>Married</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>Balance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.891</td>\n",
              "      <td>3606</td>\n",
              "      <td>283</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>11</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>106.025</td>\n",
              "      <td>6645</td>\n",
              "      <td>483</td>\n",
              "      <td>3</td>\n",
              "      <td>82</td>\n",
              "      <td>15</td>\n",
              "      <td>Female</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Asian</td>\n",
              "      <td>903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>104.593</td>\n",
              "      <td>7075</td>\n",
              "      <td>514</td>\n",
              "      <td>4</td>\n",
              "      <td>71</td>\n",
              "      <td>11</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Asian</td>\n",
              "      <td>580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>148.924</td>\n",
              "      <td>9504</td>\n",
              "      <td>681</td>\n",
              "      <td>3</td>\n",
              "      <td>36</td>\n",
              "      <td>11</td>\n",
              "      <td>Female</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Asian</td>\n",
              "      <td>964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>55.882</td>\n",
              "      <td>4897</td>\n",
              "      <td>357</td>\n",
              "      <td>2</td>\n",
              "      <td>68</td>\n",
              "      <td>16</td>\n",
              "      <td>Male</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Caucasian</td>\n",
              "      <td>331</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Income  Limit  Rating  Cards  Age  Education  Gender Student Married  \\\n",
              "0   14.891   3606     283      2   34         11    Male      No     Yes   \n",
              "1  106.025   6645     483      3   82         15  Female     Yes     Yes   \n",
              "2  104.593   7075     514      4   71         11    Male      No      No   \n",
              "3  148.924   9504     681      3   36         11  Female      No      No   \n",
              "4   55.882   4897     357      2   68         16    Male      No     Yes   \n",
              "\n",
              "   Ethnicity  Balance  \n",
              "0  Caucasian      333  \n",
              "1      Asian      903  \n",
              "2      Asian      580  \n",
              "3      Asian      964  \n",
              "4  Caucasian      331  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_credit.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "952e32b2-e3fd-48df-a65f-1fd81333037d",
      "metadata": {
        "id": "952e32b2-e3fd-48df-a65f-1fd81333037d"
      },
      "source": [
        "## pandas √® pesante?\n",
        "\n",
        "‚öôÔ∏è Cosa significa che ‚Äúpandas √® pesante‚Äù? Lo √® in vari sensi:\n",
        "\n",
        "1Ô∏è‚É£ **Dimensione e complessit√†**\n",
        "- Pandas non √® una piccola libreria:<br>\n",
        "    - l‚Äôinstallazione porta con s√© molte dipendenze:\n",
        "    - `numpy`, `dateutil`, `pytz`, `tzdata`, `matplotlib`, `openpyxl`, `xlrd`, ecc.\n",
        "- La dimensione del pacchetto √® di **decine di MB**.\n",
        "- Il caricamento in memoria all‚Äôavvio √® **pi√π lento** rispetto a un semplice `import csv`.\n",
        "\n",
        "üëâ Quindi per uno script che deve solo leggere un file CSV e stampare 10 righe, importare tutto pandas √® come usare un camion per consegnare una lettera.\n",
        "\n",
        "2Ô∏è‚É£ **Dipendenze esterne**<br>\n",
        "\n",
        "Pandas, per funzionare bene con molti formati, usa librerie esterne (come detto prima):\n",
        "- `openpyxl` per i file *.xlsx*\n",
        "- `xlrd` per i *.xls*\n",
        "- `pyarrow` per i file *.parquet*\n",
        "- `numexpr` per operazioni numeriche\n",
        "- `matplotlib per .plot()`<br>\n",
        "\n",
        "üëâ Queste dipendenze sono comode in un ambiente data science,\n",
        "ma eccessive in uno script di sistema o un microservizio.\n",
        "\n",
        "3Ô∏è‚É£ **Impatto su ambienti piccoli**<br>\n",
        "\n",
        "In contesti come:\n",
        "- microservizi Docker\n",
        "- script CLI leggeri\n",
        "- funzioni serverless (AWS Lambda, GCP Functions)\n",
        "- sistemi con vincoli di memoria\n",
        "\n",
        "importare pandas pu√≤:\n",
        "- rallentare l‚Äôavvio dello script,\n",
        "- aumentare l‚Äôimmagine Docker di decine o centinaia di MB,\n",
        "- portare a incompatibilit√† o tempi di cold start lunghi.\n",
        "\n",
        "\n",
        "**Ecco quindi perch√© a volte si vuole ‚Äúevitare pandas‚Äù**:\n",
        "- Non ci servono le sue funzioni di analisi avanzata.\n",
        "- Si vuole solo leggere un file e scorrerne le righe.\n",
        "- Si vuole ridurre dipendenze e tempo di startup.\n",
        "\n",
        "In quel caso ha pi√π senso usare:\n",
        "```python\n",
        "import csv      # per file CSV\n",
        "import openpyxl # per file Excel moderni\n",
        "```\n",
        "\n",
        "che sono moduli molto pi√π leggeri.\n",
        "\n",
        "üß† Metafora<br>\n",
        "Pandas √® come Excel o un gestionale completo.<br>\n",
        "Se ti serve solo aprire un file di testo e stampare due colonne, ti basta il Blocco Note."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2bf41df-1eaf-473d-849a-d7eddca92f06",
      "metadata": {
        "id": "f2bf41df-1eaf-473d-849a-d7eddca92f06"
      },
      "source": [
        "## Convertire file Excel in formato CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e06d281f-0e94-42e6-956f-122e767c9aaa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T08:43:41.545329Z",
          "iopub.status.busy": "2025-10-21T08:43:41.545074Z",
          "iopub.status.idle": "2025-10-21T08:43:41.549360Z",
          "shell.execute_reply": "2025-10-21T08:43:41.548809Z",
          "shell.execute_reply.started": "2025-10-21T08:43:41.545312Z"
        },
        "id": "e06d281f-0e94-42e6-956f-122e767c9aaa"
      },
      "source": [
        "Al contrario, per visualizzare un file CSV **nel formato Excel standard** si pu√≤ fare cos√¨ (ci sono anche altri modi):\n",
        "* aprire un **nuovo file**\n",
        "* scheda `Dati`\n",
        "* bottone in alto a sx `Recupera dati`\n",
        "* `Da file` --> `Da testo/CSV`\n",
        "* nella preview fare le eventuali modifiche (al caricamento) - per quanto riguarda il separatore ma anche l'origine e l'inferenza del data type (l'opzione 200 righe √® in genere sufficiente) -  e poi premere il bottone \"Carica\" in basso a dx.\n",
        "\n",
        "Questo processo √® ben descritto nel video [How to Convert CSV to Excel (Simple and Quick)](https://www.youtube.com/watch?v=jw1DSuqr3ew) della *Excel Tutorials by EasyClick Academy*. Disponibili i sottotitoli."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b439110",
      "metadata": {
        "id": "5b439110"
      },
      "source": [
        "Se un file nasce in excel e poi viene convertito in csv, salvato e in un secondo momento riaperto in excel, excel lo visualizza come excel (cio√® con la griglia). Come mai?<br>\n",
        "\n",
        "Excel non apre il CSV come file Excel vero e proprio ‚Äî lo interpreta come una tabella testuale, e lo visualizza nella stessa interfaccia grafica (celle, righe, colonne).<br>\n",
        "Questo fa s√¨ che ‚Äúsembri Excel‚Äù, ma in realt√† il file non contiene nessuna delle informazioni tipiche del formato `.xlsx`:\n",
        "- niente formattazione,\n",
        "- niente formule,\n",
        "- niente fogli multipli,\n",
        "- niente tipi di dato complessi.\n",
        "\n",
        "Excel semplicemente mostra **una griglia sopra un file di testo**.<br>\n",
        "√à un po‚Äô come aprire un file `.txt` in Word: il contenuto √® testo, ma l‚Äôambiente √® Word, con tutto il suo aspetto ‚Äúricco‚Äù.\n",
        "\n",
        "üí° Riassunto in una frase:<br>\n",
        "> Excel riconosce l‚Äôestensione `.csv`, ne interpreta i dati in forma tabellare e li visualizza nella sua interfaccia ‚Äî ma il file rimane puro testo strutturato, non un vero foglio Excel.\n",
        "\n",
        "**Obiezione**: se apro in excel un file csv generato indipendentemente da excel, la griglia non √® applicata.<br>\n",
        "Vero, Excel non legge il CSV ‚Äúintelligentemente‚Äù in base al contenuto, ma usa **impostazioni locali** del sistema operativo (la ‚Äúimpostazione paese‚Äù o ‚Äúseparatore di elenco‚Äù di Windows).<br>\n",
        "Per esempio:\n",
        "- in Italia, il separatore di elenco predefinito √® `;` (punto e virgola);\n",
        "- in USA/UK, √® `,` (virgola).\n",
        "\n",
        "Quindi:\n",
        "- se il CSV viene da Excel, esso usa lo stesso separatore della tua configurazione regionale ‚Üí Excel lo ‚Äúriconosce‚Äù e mostra la tabella.\n",
        "- Se invece il CSV viene da un altro programma (es. Python to_csv(), MySQL, o sistemi internazionali) che usa la virgola, Excel non sa che √® un separatore e ti mette tutto in un‚Äôunica cella.\n",
        "\n",
        "L‚Äôutente pu√≤ correggere manualmente:\n",
        "\n",
        "    `Dati ‚Üí Da testo/CSV ‚Üí scegli delimitatore corretto (virgola, punto e virgola, tab)`\n",
        "\n",
        "oppure cambiare l‚Äôimpostazione regionale del sistema.\n",
        "\n",
        "üí° In sintesi:\n",
        "\n",
        "> Excel ‚Äúinterpreta‚Äù come tabella solo i CSV che rispettano le sue convenzioni regionali (separatore e encoding).<br>\n",
        "> Se il file √® generato altrove con altri standard, Excel lo mostra come testo in una colonna.\n",
        "\n",
        "![](apertura_csv.png)\n",
        "\n",
        "üí¨ **Spiegazione breve**\n",
        "\n",
        "Quando Excel mostra tutto in una sola colonna, √® perch√©:\n",
        "- il delimitatore del file (es. `,`)\n",
        "‚â†\n",
        "- dal separatore di elenco di sistema (es. `;` in Italia).\n",
        "\n",
        "üëâ Soluzione: usare l‚Äôimportazione guidata, che ti fa scegliere manualmente il delimitatore e l‚Äôencoding (UTF-8 consigliato).<br>\n",
        "Dopo questa scelta, Excel mostra subito la griglia corretta e puoi salvare come .xlsx se vuoi mantenerla stabile.\n",
        "\n",
        "üí° Consiglio pratico per chi lavora spesso con Python o CSV esterni:\n",
        "- usa `df.to_csv('file.csv', sep=';', encoding='utf-8-sig')`\n",
        "‚Üí cos√¨ Excel (versione italiana) lo apre gi√† ‚Äúa griglia‚Äù senza interventi manuali.\n",
        "- `utf-8-sig` √® una variante dell‚Äôencoding UTF-8 molto usata proprio per far s√¨ che Excel riconosca correttamente i CSV."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26c5549a-b79e-44d5-b83a-39cad9e4ea90",
      "metadata": {
        "id": "26c5549a-b79e-44d5-b83a-39cad9e4ea90"
      },
      "source": [
        "## Gli argomenti di input della funzione `pd.read_csv`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0853b04-c48a-41cb-a7b4-4ffa761e149a",
      "metadata": {
        "id": "d0853b04-c48a-41cb-a7b4-4ffa761e149a"
      },
      "source": [
        "Abbiamo gi√† citato i due **fondamentali argomenti in input** della funzione pandas `read_csv`: `sep` e `header`. Sono **critici:\n",
        "- `header=1` dice a pandas di saltare la prima riga del file e usare la seconda riga come intestazione.<br>\n",
        "Se i nostri CSV non hanno due righe di intestazione, oppure se il primo file ha un formato leggermente diverso dagli altri (spazi, separatore, BOM, ecc.), pandas **interpreter√† in modo sbagliato le colonne**\n",
        "- `sep=';'` scassa tutti i dati (se il file √® effettivamente CSV!<br>\n",
        "Attenzione: molti \"file CSV\" hanno sep=';'!\n",
        "\n",
        "In realt√† la funzione `read_csv` ha **molti altri argomenti in input**, come si evince dall'help della cella successiva - pi√π avanti approfondiremo i **principali**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ea1ed4-0b0b-42df-a17a-6561d85f48b5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T08:43:37.520188Z",
          "iopub.status.busy": "2025-10-21T08:43:37.519710Z",
          "iopub.status.idle": "2025-10-21T08:43:37.524355Z",
          "shell.execute_reply": "2025-10-21T08:43:37.524052Z",
          "shell.execute_reply.started": "2025-10-21T08:43:37.520167Z"
        },
        "id": "e1ea1ed4-0b0b-42df-a17a-6561d85f48b5",
        "outputId": "3e678984-5553-4adf-ab48-fdfc1f584583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on function read_csv in module pandas.io.parsers.readers:\n",
            "\n",
            "read_csv(\n",
            "    filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]',\n",
            "    *,\n",
            "    sep: 'str | None | lib.NoDefault' = <no_default>,\n",
            "    delimiter: 'str | None | lib.NoDefault' = None,\n",
            "    header: \"int | Sequence[int] | None | Literal['infer']\" = 'infer',\n",
            "    names: 'Sequence[Hashable] | None | lib.NoDefault' = <no_default>,\n",
            "    index_col: 'IndexLabel | Literal[False] | None' = None,\n",
            "    usecols: 'UsecolsArgType' = None,\n",
            "    dtype: 'DtypeArg | None' = None,\n",
            "    engine: 'CSVEngine | None' = None,\n",
            "    converters: 'Mapping[Hashable, Callable] | None' = None,\n",
            "    true_values: 'list | None' = None,\n",
            "    false_values: 'list | None' = None,\n",
            "    skipinitialspace: 'bool' = False,\n",
            "    skiprows: 'list[int] | int | Callable[[Hashable], bool] | None' = None,\n",
            "    skipfooter: 'int' = 0,\n",
            "    nrows: 'int | None' = None,\n",
            "    na_values: 'Hashable | Iterable[Hashable] | Mapping[Hashable, Iterable[Hashable]] | None' = None,\n",
            "    keep_default_na: 'bool' = True,\n",
            "    na_filter: 'bool' = True,\n",
            "    verbose: 'bool | lib.NoDefault' = <no_default>,\n",
            "    skip_blank_lines: 'bool' = True,\n",
            "    parse_dates: 'bool | Sequence[Hashable] | None' = None,\n",
            "    infer_datetime_format: 'bool | lib.NoDefault' = <no_default>,\n",
            "    keep_date_col: 'bool | lib.NoDefault' = <no_default>,\n",
            "    date_parser: 'Callable | lib.NoDefault' = <no_default>,\n",
            "    date_format: 'str | dict[Hashable, str] | None' = None,\n",
            "    dayfirst: 'bool' = False,\n",
            "    cache_dates: 'bool' = True,\n",
            "    iterator: 'bool' = False,\n",
            "    chunksize: 'int | None' = None,\n",
            "    compression: 'CompressionOptions' = 'infer',\n",
            "    thousands: 'str | None' = None,\n",
            "    decimal: 'str' = '.',\n",
            "    lineterminator: 'str | None' = None,\n",
            "    quotechar: 'str' = '\"',\n",
            "    quoting: 'int' = 0,\n",
            "    doublequote: 'bool' = True,\n",
            "    escapechar: 'str | None' = None,\n",
            "    comment: 'str | None' = None,\n",
            "    encoding: 'str | None' = None,\n",
            "    encoding_errors: 'str | None' = 'strict',\n",
            "    dialect: 'str | csv.Dialect | None' = None,\n",
            "    on_bad_lines: 'str' = 'error',\n",
            "    delim_whitespace: 'bool | lib.NoDefault' = <no_default>,\n",
            "    low_memory: 'bool' = True,\n",
            "    memory_map: 'bool' = False,\n",
            "    float_precision: \"Literal['high', 'legacy'] | None\" = None,\n",
            "    storage_options: 'StorageOptions | None' = None,\n",
            "    dtype_backend: 'DtypeBackend | lib.NoDefault' = <no_default>\n",
            ") -> 'DataFrame | TextFileReader'\n",
            "    Read a comma-separated values (csv) file into DataFrame.\n",
            "\n",
            "    Also supports optionally iterating or breaking of the file\n",
            "    into chunks.\n",
            "\n",
            "    Additional help can be found in the online docs for\n",
            "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    filepath_or_buffer : str, path object or file-like object\n",
            "        Any valid string path is acceptable. The string could be a URL. Valid\n",
            "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
            "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
            "\n",
            "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
            "\n",
            "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
            "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
            "    sep : str, default ','\n",
            "        Character or regex pattern to treat as the delimiter. If ``sep=None``, the\n",
            "        C engine cannot automatically detect\n",
            "        the separator, but the Python parsing engine can, meaning the latter will\n",
            "        be used and automatically detect the separator from only the first valid\n",
            "        row of the file by Python's builtin sniffer tool, ``csv.Sniffer``.\n",
            "        In addition, separators longer than 1 character and different from\n",
            "        ``'\\s+'`` will be interpreted as regular expressions and will also force\n",
            "        the use of the Python parsing engine. Note that regex delimiters are prone\n",
            "        to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
            "    delimiter : str, optional\n",
            "        Alias for ``sep``.\n",
            "    header : int, Sequence of int, 'infer' or None, default 'infer'\n",
            "        Row number(s) containing column labels and marking the start of the\n",
            "        data (zero-indexed). Default behavior is to infer the column names: if no ``names``\n",
            "        are passed the behavior is identical to ``header=0`` and column\n",
            "        names are inferred from the first line of the file, if column\n",
            "        names are passed explicitly to ``names`` then the behavior is identical to\n",
            "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
            "        replace existing names. The header can be a list of integers that\n",
            "        specify row locations for a :class:`~pandas.MultiIndex` on the columns\n",
            "        e.g. ``[0, 1, 3]``. Intervening rows that are not specified will be\n",
            "        skipped (e.g. 2 in this example is skipped). Note that this\n",
            "        parameter ignores commented lines and empty lines if\n",
            "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
            "        data rather than the first line of the file.\n",
            "    names : Sequence of Hashable, optional\n",
            "        Sequence of column labels to apply. If the file contains a header row,\n",
            "        then you should explicitly pass ``header=0`` to override the column names.\n",
            "        Duplicates in this list are not allowed.\n",
            "    index_col : Hashable, Sequence of Hashable or False, optional\n",
            "      Column(s) to use as row label(s), denoted either by column labels or column\n",
            "      indices.  If a sequence of labels or indices is given, :class:`~pandas.MultiIndex`\n",
            "      will be formed for the row labels.\n",
            "\n",
            "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
            "      column as the index, e.g., when you have a malformed file with delimiters at\n",
            "      the end of each line.\n",
            "    usecols : Sequence of Hashable or Callable, optional\n",
            "        Subset of columns to select, denoted either by column labels or column indices.\n",
            "        If list-like, all elements must either\n",
            "        be positional (i.e. integer indices into the document columns) or strings\n",
            "        that correspond to column names provided either by the user in ``names`` or\n",
            "        inferred from the document header row(s). If ``names`` are given, the document\n",
            "        header row(s) are not taken into account. For example, a valid list-like\n",
            "        ``usecols`` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
            "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
            "        To instantiate a :class:`~pandas.DataFrame` from ``data`` with element order\n",
            "        preserved use ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]``\n",
            "        for columns in ``['foo', 'bar']`` order or\n",
            "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
            "        for ``['bar', 'foo']`` order.\n",
            "\n",
            "        If callable, the callable function will be evaluated against the column\n",
            "        names, returning names where the callable function evaluates to ``True``. An\n",
            "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
            "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
            "        parsing time and lower memory usage.\n",
            "    dtype : dtype or dict of {Hashable : dtype}, optional\n",
            "        Data type(s) to apply to either the whole dataset or individual columns.\n",
            "        E.g., ``{'a': np.float64, 'b': np.int32, 'c': 'Int64'}``\n",
            "        Use ``str`` or ``object`` together with suitable ``na_values`` settings\n",
            "        to preserve and not interpret ``dtype``.\n",
            "        If ``converters`` are specified, they will be applied INSTEAD\n",
            "        of ``dtype`` conversion.\n",
            "\n",
            "        .. versionadded:: 1.5.0\n",
            "\n",
            "            Support for ``defaultdict`` was added. Specify a ``defaultdict`` as input where\n",
            "            the default determines the ``dtype`` of the columns which are not explicitly\n",
            "            listed.\n",
            "    engine : {'c', 'python', 'pyarrow'}, optional\n",
            "        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
            "        is currently more feature-complete. Multithreading is currently only supported by\n",
            "        the pyarrow engine.\n",
            "\n",
            "        .. versionadded:: 1.4.0\n",
            "\n",
            "            The 'pyarrow' engine was added as an *experimental* engine, and some features\n",
            "            are unsupported, or may not work correctly, with this engine.\n",
            "    converters : dict of {Hashable : Callable}, optional\n",
            "        Functions for converting values in specified columns. Keys can either\n",
            "        be column labels or column indices.\n",
            "    true_values : list, optional\n",
            "        Values to consider as ``True`` in addition to case-insensitive variants of 'True'.\n",
            "    false_values : list, optional\n",
            "        Values to consider as ``False`` in addition to case-insensitive variants of 'False'.\n",
            "    skipinitialspace : bool, default False\n",
            "        Skip spaces after delimiter.\n",
            "    skiprows : int, list of int or Callable, optional\n",
            "        Line numbers to skip (0-indexed) or number of lines to skip (``int``)\n",
            "        at the start of the file.\n",
            "\n",
            "        If callable, the callable function will be evaluated against the row\n",
            "        indices, returning ``True`` if the row should be skipped and ``False`` otherwise.\n",
            "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
            "    skipfooter : int, default 0\n",
            "        Number of lines at bottom of file to skip (Unsupported with ``engine='c'``).\n",
            "    nrows : int, optional\n",
            "        Number of rows of file to read. Useful for reading pieces of large files.\n",
            "    na_values : Hashable, Iterable of Hashable or dict of {Hashable : Iterable}, optional\n",
            "        Additional strings to recognize as ``NA``/``NaN``. If ``dict`` passed, specific\n",
            "        per-column ``NA`` values.  By default the following values are interpreted as\n",
            "        ``NaN``: \" \", \"#N/A\", \"#N/A N/A\", \"#NA\", \"-1.#IND\", \"-1.#QNAN\", \"-NaN\", \"-nan\",\n",
            "        \"1.#IND\", \"1.#QNAN\", \"<NA>\", \"N/A\", \"NA\", \"NULL\", \"NaN\", \"None\",\n",
            "        \"n/a\", \"nan\", \"null \".\n",
            "\n",
            "    keep_default_na : bool, default True\n",
            "        Whether or not to include the default ``NaN`` values when parsing the data.\n",
            "        Depending on whether ``na_values`` is passed in, the behavior is as follows:\n",
            "\n",
            "        * If ``keep_default_na`` is ``True``, and ``na_values`` are specified, ``na_values``\n",
            "          is appended to the default ``NaN`` values used for parsing.\n",
            "        * If ``keep_default_na`` is ``True``, and ``na_values`` are not specified, only\n",
            "          the default ``NaN`` values are used for parsing.\n",
            "        * If ``keep_default_na`` is ``False``, and ``na_values`` are specified, only\n",
            "          the ``NaN`` values specified ``na_values`` are used for parsing.\n",
            "        * If ``keep_default_na`` is ``False``, and ``na_values`` are not specified, no\n",
            "          strings will be parsed as ``NaN``.\n",
            "\n",
            "        Note that if ``na_filter`` is passed in as ``False``, the ``keep_default_na`` and\n",
            "        ``na_values`` parameters will be ignored.\n",
            "    na_filter : bool, default True\n",
            "        Detect missing value markers (empty strings and the value of ``na_values``). In\n",
            "        data without any ``NA`` values, passing ``na_filter=False`` can improve the\n",
            "        performance of reading a large file.\n",
            "    verbose : bool, default False\n",
            "        Indicate number of ``NA`` values placed in non-numeric columns.\n",
            "\n",
            "        .. deprecated:: 2.2.0\n",
            "    skip_blank_lines : bool, default True\n",
            "        If ``True``, skip over blank lines rather than interpreting as ``NaN`` values.\n",
            "    parse_dates : bool, list of Hashable, list of lists or dict of {Hashable : list}, default False\n",
            "        The behavior is as follows:\n",
            "\n",
            "        * ``bool``. If ``True`` -> try parsing the index. Note: Automatically set to\n",
            "          ``True`` if ``date_format`` or ``date_parser`` arguments have been passed.\n",
            "        * ``list`` of ``int`` or names. e.g. If ``[1, 2, 3]`` -> try parsing columns 1, 2, 3\n",
            "          each as a separate date column.\n",
            "        * ``list`` of ``list``. e.g.  If ``[[1, 3]]`` -> combine columns 1 and 3 and parse\n",
            "          as a single date column. Values are joined with a space before parsing.\n",
            "        * ``dict``, e.g. ``{'foo' : [1, 3]}`` -> parse columns 1, 3 as date and call\n",
            "          result 'foo'. Values are joined with a space before parsing.\n",
            "\n",
            "        If a column or index cannot be represented as an array of ``datetime``,\n",
            "        say because of an unparsable value or a mixture of timezones, the column\n",
            "        or index will be returned unaltered as an ``object`` data type. For\n",
            "        non-standard ``datetime`` parsing, use :func:`~pandas.to_datetime` after\n",
            "        :func:`~pandas.read_csv`.\n",
            "\n",
            "        Note: A fast-path exists for iso8601-formatted dates.\n",
            "    infer_datetime_format : bool, default False\n",
            "        If ``True`` and ``parse_dates`` is enabled, pandas will attempt to infer the\n",
            "        format of the ``datetime`` strings in the columns, and if it can be inferred,\n",
            "        switch to a faster method of parsing them. In some cases this can increase\n",
            "        the parsing speed by 5-10x.\n",
            "\n",
            "        .. deprecated:: 2.0.0\n",
            "            A strict version of this argument is now the default, passing it has no effect.\n",
            "\n",
            "    keep_date_col : bool, default False\n",
            "        If ``True`` and ``parse_dates`` specifies combining multiple columns then\n",
            "        keep the original columns.\n",
            "    date_parser : Callable, optional\n",
            "        Function to use for converting a sequence of string columns to an array of\n",
            "        ``datetime`` instances. The default uses ``dateutil.parser.parser`` to do the\n",
            "        conversion. pandas will try to call ``date_parser`` in three different ways,\n",
            "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
            "        (as defined by ``parse_dates``) as arguments; 2) concatenate (row-wise) the\n",
            "        string values from the columns defined by ``parse_dates`` into a single array\n",
            "        and pass that; and 3) call ``date_parser`` once for each row using one or\n",
            "        more strings (corresponding to the columns defined by ``parse_dates``) as\n",
            "        arguments.\n",
            "\n",
            "        .. deprecated:: 2.0.0\n",
            "           Use ``date_format`` instead, or read in as ``object`` and then apply\n",
            "           :func:`~pandas.to_datetime` as-needed.\n",
            "    date_format : str or dict of column -> format, optional\n",
            "        Format to use for parsing dates when used in conjunction with ``parse_dates``.\n",
            "        The strftime to parse time, e.g. :const:`\"%d/%m/%Y\"`. See\n",
            "        `strftime documentation\n",
            "        <https://docs.python.org/3/library/datetime.html\n",
            "        #strftime-and-strptime-behavior>`_ for more information on choices, though\n",
            "        note that :const:`\"%f\"` will parse all the way up to nanoseconds.\n",
            "        You can also pass:\n",
            "\n",
            "        - \"ISO8601\", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_\n",
            "            time string (not necessarily in exactly the same format);\n",
            "        - \"mixed\", to infer the format for each element individually. This is risky,\n",
            "            and you should probably use it along with `dayfirst`.\n",
            "\n",
            "        .. versionadded:: 2.0.0\n",
            "    dayfirst : bool, default False\n",
            "        DD/MM format dates, international and European format.\n",
            "    cache_dates : bool, default True\n",
            "        If ``True``, use a cache of unique, converted dates to apply the ``datetime``\n",
            "        conversion. May produce significant speed-up when parsing duplicate\n",
            "        date strings, especially ones with timezone offsets.\n",
            "\n",
            "    iterator : bool, default False\n",
            "        Return ``TextFileReader`` object for iteration or getting chunks with\n",
            "        ``get_chunk()``.\n",
            "    chunksize : int, optional\n",
            "        Number of lines to read from the file per chunk. Passing a value will cause the\n",
            "        function to return a ``TextFileReader`` object for iteration.\n",
            "        See the `IO Tools docs\n",
            "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
            "        for more information on ``iterator`` and ``chunksize``.\n",
            "\n",
            "    compression : str or dict, default 'infer'\n",
            "        For on-the-fly decompression of on-disk data. If 'infer' and 'filepath_or_buffer' is\n",
            "        path-like, then detect compression from the following extensions: '.gz',\n",
            "        '.bz2', '.zip', '.xz', '.zst', '.tar', '.tar.gz', '.tar.xz' or '.tar.bz2'\n",
            "        (otherwise no compression).\n",
            "        If using 'zip' or 'tar', the ZIP file must contain only one data file to be read in.\n",
            "        Set to ``None`` for no decompression.\n",
            "        Can also be a dict with key ``'method'`` set\n",
            "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``, ``'xz'``, ``'tar'``} and\n",
            "        other key-value pairs are forwarded to\n",
            "        ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
            "        ``bz2.BZ2File``, ``zstandard.ZstdDecompressor``, ``lzma.LZMAFile`` or\n",
            "        ``tarfile.TarFile``, respectively.\n",
            "        As an example, the following could be passed for Zstandard decompression using a\n",
            "        custom compression dictionary:\n",
            "        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
            "\n",
            "        .. versionadded:: 1.5.0\n",
            "            Added support for `.tar` files.\n",
            "\n",
            "        .. versionchanged:: 1.4.0 Zstandard support.\n",
            "\n",
            "    thousands : str (length 1), optional\n",
            "        Character acting as the thousands separator in numerical values.\n",
            "    decimal : str (length 1), default '.'\n",
            "        Character to recognize as decimal point (e.g., use ',' for European data).\n",
            "    lineterminator : str (length 1), optional\n",
            "        Character used to denote a line break. Only valid with C parser.\n",
            "    quotechar : str (length 1), optional\n",
            "        Character used to denote the start and end of a quoted item. Quoted\n",
            "        items can include the ``delimiter`` and it will be ignored.\n",
            "    quoting : {0 or csv.QUOTE_MINIMAL, 1 or csv.QUOTE_ALL, 2 or csv.QUOTE_NONNUMERIC, 3 or csv.QUOTE_NONE}, default csv.QUOTE_MINIMAL\n",
            "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Default is\n",
            "        ``csv.QUOTE_MINIMAL`` (i.e., 0) which implies that only fields containing special\n",
            "        characters are quoted (e.g., characters defined in ``quotechar``, ``delimiter``,\n",
            "        or ``lineterminator``.\n",
            "    doublequote : bool, default True\n",
            "       When ``quotechar`` is specified and ``quoting`` is not ``QUOTE_NONE``, indicate\n",
            "       whether or not to interpret two consecutive ``quotechar`` elements INSIDE a\n",
            "       field as a single ``quotechar`` element.\n",
            "    escapechar : str (length 1), optional\n",
            "        Character used to escape other characters.\n",
            "    comment : str (length 1), optional\n",
            "        Character indicating that the remainder of line should not be parsed.\n",
            "        If found at the beginning\n",
            "        of a line, the line will be ignored altogether. This parameter must be a\n",
            "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
            "        fully commented lines are ignored by the parameter ``header`` but not by\n",
            "        ``skiprows``. For example, if ``comment='#'``, parsing\n",
            "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in ``'a,b,c'`` being\n",
            "        treated as the header.\n",
            "    encoding : str, optional, default 'utf-8'\n",
            "        Encoding to use for UTF when reading/writing (ex. ``'utf-8'``). `List of Python\n",
            "        standard encodings\n",
            "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
            "\n",
            "    encoding_errors : str, optional, default 'strict'\n",
            "        How encoding errors are treated. `List of possible values\n",
            "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
            "\n",
            "        .. versionadded:: 1.3.0\n",
            "\n",
            "    dialect : str or csv.Dialect, optional\n",
            "        If provided, this parameter will override values (default or not) for the\n",
            "        following parameters: ``delimiter``, ``doublequote``, ``escapechar``,\n",
            "        ``skipinitialspace``, ``quotechar``, and ``quoting``. If it is necessary to\n",
            "        override values, a ``ParserWarning`` will be issued. See ``csv.Dialect``\n",
            "        documentation for more details.\n",
            "    on_bad_lines : {'error', 'warn', 'skip'} or Callable, default 'error'\n",
            "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
            "        Allowed values are :\n",
            "\n",
            "        - ``'error'``, raise an Exception when a bad line is encountered.\n",
            "        - ``'warn'``, raise a warning when a bad line is encountered and skip that line.\n",
            "        - ``'skip'``, skip bad lines without raising or warning when they are encountered.\n",
            "\n",
            "        .. versionadded:: 1.3.0\n",
            "\n",
            "        .. versionadded:: 1.4.0\n",
            "\n",
            "            - Callable, function with signature\n",
            "              ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
            "              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
            "              If the function returns ``None``, the bad line will be ignored.\n",
            "              If the function returns a new ``list`` of strings with more elements than\n",
            "              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
            "              Only supported when ``engine='python'``\n",
            "\n",
            "        .. versionchanged:: 2.2.0\n",
            "\n",
            "            - Callable, function with signature\n",
            "              as described in `pyarrow documentation\n",
            "              <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ParseOptions.html\n",
            "              #pyarrow.csv.ParseOptions.invalid_row_handler>`_ when ``engine='pyarrow'``\n",
            "\n",
            "    delim_whitespace : bool, default False\n",
            "        Specifies whether or not whitespace (e.g. ``' '`` or ``'\\t'``) will be\n",
            "        used as the ``sep`` delimiter. Equivalent to setting ``sep='\\s+'``. If this option\n",
            "        is set to ``True``, nothing should be passed in for the ``delimiter``\n",
            "        parameter.\n",
            "\n",
            "        .. deprecated:: 2.2.0\n",
            "            Use ``sep=\"\\s+\"`` instead.\n",
            "    low_memory : bool, default True\n",
            "        Internally process the file in chunks, resulting in lower memory use\n",
            "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
            "        types either set ``False``, or specify the type with the ``dtype`` parameter.\n",
            "        Note that the entire file is read into a single :class:`~pandas.DataFrame`\n",
            "        regardless, use the ``chunksize`` or ``iterator`` parameter to return the data in\n",
            "        chunks. (Only valid with C parser).\n",
            "    memory_map : bool, default False\n",
            "        If a filepath is provided for ``filepath_or_buffer``, map the file object\n",
            "        directly onto memory and access the data directly from there. Using this\n",
            "        option can improve performance because there is no longer any I/O overhead.\n",
            "    float_precision : {'high', 'legacy', 'round_trip'}, optional\n",
            "        Specifies which converter the C engine should use for floating-point\n",
            "        values. The options are ``None`` or ``'high'`` for the ordinary converter,\n",
            "        ``'legacy'`` for the original lower precision pandas converter, and\n",
            "        ``'round_trip'`` for the round-trip converter.\n",
            "\n",
            "    storage_options : dict, optional\n",
            "        Extra options that make sense for a particular storage connection, e.g.\n",
            "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
            "        are forwarded to ``urllib.request.Request`` as header options. For other\n",
            "        URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
            "        forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
            "        details, and for more examples on storage options refer `here\n",
            "        <https://pandas.pydata.org/docs/user_guide/io.html?\n",
            "        highlight=storage_options#reading-writing-remote-files>`_.\n",
            "\n",
            "    dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
            "        Back-end data type applied to the resultant :class:`DataFrame`\n",
            "        (still experimental). Behaviour is as follows:\n",
            "\n",
            "        * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
            "          (default).\n",
            "        * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
            "          DataFrame.\n",
            "\n",
            "        .. versionadded:: 2.0\n",
            "\n",
            "    Returns\n",
            "    -------\n",
            "    DataFrame or TextFileReader\n",
            "        A comma-separated values (csv) file is returned as two-dimensional\n",
            "        data structure with labeled axes.\n",
            "\n",
            "    See Also\n",
            "    --------\n",
            "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
            "    read_table : Read general delimited file into DataFrame.\n",
            "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(pd.read_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b128b8-189d-4145-a0f1-9bf4d943e1a9",
      "metadata": {
        "id": "02b128b8-189d-4145-a0f1-9bf4d943e1a9"
      },
      "source": [
        "> La lettura di un file *csv* rappresenta spesso una delle **prime attivit√†** di un Data Scientist e di un notebook. Sebbene essa possa sembrare banale, spesso invece rappresenta uno **scoglio iniziale**, fonte di **non poche frustrazioni**, per due ragioni:\n",
        "> - i molti e non banali argomenti della funzione `read_csv`\n",
        "> - le **irregoilarit√†** nei dati dei file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5394f7a9-8061-400e-a1af-3a9a87099765",
      "metadata": {
        "id": "5394f7a9-8061-400e-a1af-3a9a87099765"
      },
      "source": [
        "## Il mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1c16f43-e516-48ac-95b9-1534a96430a2",
      "metadata": {
        "id": "f1c16f43-e516-48ac-95b9-1534a96430a2"
      },
      "source": [
        "La funzione `pd.read_csv` fa un **mapping** automatico dei dati CSV in pandas, come qui descritto:\n",
        "\n",
        "![](how_pandas_infers_CSV_datatypes.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4e93810-ba9e-41ed-be16-49f71508751d",
      "metadata": {
        "id": "a4e93810-ba9e-41ed-be16-49f71508751d"
      },
      "source": [
        "C'√® un problema, non citato nella slide: la funzione `read_csv` **non riesce spesso a inferire le variabili categoriche** (se presenti nel file CSV come stringhe), che vengono perci√≤ importate come `object`, il generico data type *stringa* di pandas. Come si vede, infatti:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9292db96-52aa-4873-ae2b-79451efae594",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T09:32:27.750380Z",
          "iopub.status.busy": "2025-10-21T09:32:27.750139Z",
          "iopub.status.idle": "2025-10-21T09:32:27.754749Z",
          "shell.execute_reply": "2025-10-21T09:32:27.754389Z",
          "shell.execute_reply.started": "2025-10-21T09:32:27.750364Z"
        },
        "id": "9292db96-52aa-4873-ae2b-79451efae594",
        "outputId": "0dc7062c-bade-4afc-9dfe-2b8d559a6321"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Income       float64\n",
              "Limit          int64\n",
              "Rating         int64\n",
              "Cards          int64\n",
              "Age            int64\n",
              "Education      int64\n",
              "Gender        object\n",
              "Student       object\n",
              "Married       object\n",
              "Ethnicity     object\n",
              "Balance        int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_credit.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99baa579-39ee-4f63-9bfc-4e8c39b72841",
      "metadata": {
        "id": "99baa579-39ee-4f63-9bfc-4e8c39b72841"
      },
      "source": [
        "Le variabili `Gender`, `Student`, `Married` e `Ethnicity` sono **categoriche** perch√®, al di l√† del loro formato, possono assumere un numero finito e piccolo di valori, a differenza delle variabili **numeriche**, che possono assumere (almeno potenzialmente) un numero infinito di valori.\n",
        "\n",
        "Ogni cella della variabile, se importata come `object`, **punta a una stringa in memoria, spesso duplicata pi√π volte**.\n",
        "\n",
        "Occorre dunque **convertire** queste variabili in formato `category`, un data type atomico reso disponibile da *pandas* (non c'√® in Python base), nel seguente modo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65df6bda-6ddc-4f71-a4f3-c5374269103f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T09:40:41.996556Z",
          "iopub.status.busy": "2025-10-21T09:40:41.996253Z",
          "iopub.status.idle": "2025-10-21T09:40:42.001137Z",
          "shell.execute_reply": "2025-10-21T09:40:42.000829Z",
          "shell.execute_reply.started": "2025-10-21T09:40:41.996542Z"
        },
        "id": "65df6bda-6ddc-4f71-a4f3-c5374269103f",
        "outputId": "9bfc1afa-b4bc-4f74-cc46-88085e8ba564"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Income        float64\n",
              "Limit           int64\n",
              "Rating          int64\n",
              "Cards           int64\n",
              "Age             int64\n",
              "Education       int64\n",
              "Gender       category\n",
              "Student        object\n",
              "Married        object\n",
              "Ethnicity      object\n",
              "Balance         int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_credit['Gender'] = df_credit['Gender'].astype('category')\n",
        "df_credit.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b894adb0-45e6-47ef-b6d2-993f4163a6de",
      "metadata": {
        "id": "b894adb0-45e6-47ef-b6d2-993f4163a6de"
      },
      "source": [
        "Il metodo *pandas* `astype('category')`:\n",
        "- crea una **tabella di codifica interna** (i ‚Äúlevels‚Äù o ‚Äúcategories‚Äù),\n",
        "- rappresenta la colonna come **interi** interni (0, 1, 2, ‚Ä¶) invece che stringhe ripetute.\n",
        "\n",
        "Il funzionamento di `category` √® simile a quello del **fattore** di R.\n",
        "\n",
        "üöÄ <font size=\"4\">**Vantaggi principali** (di `category`)</font><br>\n",
        "\n",
        "üîπ **Efficienza in memoria**<br>\n",
        "Ogni valore diventa **un intero**, e la stringa viene **memorizzata una sola volta nella tabella delle categorie**.<br>\n",
        "üëâ Su grandi dataset, il risparmio pu√≤ arrivare al 70‚Äì90% di RAM.\n",
        "Esempio:\n",
        "```python\n",
        "df['citt√†'].memory_usage(deep=True)\n",
        "df['citt√†'].astype('category').memory_usage(deep=True)\n",
        "```\n",
        "La seconda occupa molto meno spazio.\n",
        "\n",
        "üîπ **Velocit√† di elaborazione**<br>\n",
        "Molte operazioni pandas (`groupby`, `sort`, `value_counts`, `merges`) diventano **molto pi√π veloci**; infatti:\n",
        "- confrontare interi √® pi√π rapido che confrontare stringhe,\n",
        "- gli algoritmi di raggruppamento e join lavorano sui codici numerici.\n",
        "üí° Tipico: `df.groupby('categoria').agg(...)` √® molto pi√π rapido se categoria √® `category`.\n",
        "\n",
        "üîπ **Significato semantico**<br>\n",
        "Una variabile categorica ha **un numero finito e noto di livelli**.<br>\n",
        "Questo √® utile per:\n",
        "- garantire che non compaiano valori ‚Äúfuori lista‚Äù (es. ‚ÄòFemmina‚Äô vs ‚ÄòF‚Äô),\n",
        "- mantenere l‚Äôordine logico o gerarchico (es. Basso < Medio < Alto).<br>\n",
        "\n",
        "Si pu√≤ anche definire esplicitamente l‚Äôordine, in questo modo:\n",
        "```python\n",
        "df['livello'] = pd.Categorical(df['livello'], categories=['basso','medio','alto'], ordered=True)\n",
        "```\n",
        "\n",
        "‚Üí utile per confronti, ordinamenti o encoding nel machine learning.\n",
        "\n",
        "üîπ **Compatibilit√† ML e preprocessing**<br>\n",
        "Molti algoritmi di machine learning o encoder (es. `sklearn.preprocessing.OrdinalEncoder`, `OneHotEncoder`) riconoscono category e la trattano subito come variabile discreta, **senza doverla prima convertire da object**.\n",
        "\n",
        "---\n",
        "\n",
        "‚ö†Ô∏è **Quando `category` non conviene?**\n",
        "- se la colonna ha **molti valori unici** (es. un codice univoco o un ID cliente), la conversione non porta benefici: la tabella delle categorie sarebbe grande quanto la colonna stessa.\n",
        "- se si modificano spesso i valori (aggiungendo nuove categorie), il tipo `category` √® meno flessibile.\n",
        "\n",
        "üîç **Esempio pratico**:\n",
        "```python\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame({\n",
        "        'sesso': ['M','F','M','F','F']*100000\n",
        "    })\n",
        "    print(df['sesso'].memory_usage(deep=True))   # object\n",
        "    df['sesso'] = df['sesso'].astype('category')\n",
        "    print(df['sesso'].memory_usage(deep=True))   # category (molto meno!)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "![](sintesi_category.png)<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aefdf89b-2214-4ed7-8b59-49939285ecfe",
      "metadata": {
        "id": "aefdf89b-2214-4ed7-8b59-49939285ecfe"
      },
      "source": [
        "## Gli argomenti della funzione `pd.read_csv`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1fd02f-b8fe-4d70-9bbe-235863bef8d5",
      "metadata": {
        "id": "9b1fd02f-b8fe-4d70-9bbe-235863bef8d5"
      },
      "source": [
        "Ecco un ottimo notebook di [illustrazione dei vari argomenti](https://github.com/nikitaprasad21/ML-Cheat-Codes/blob/main/Data-Gathering/CSV-(Comma-Separated-Values)-Files/csv_file_cheatcodes.ipynb) per `pd.read_csv` - **scaricato** nella directory di questo notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20a0786d-2382-40f2-8247-5b828130b23d",
      "metadata": {
        "id": "20a0786d-2382-40f2-8247-5b828130b23d"
      },
      "source": [
        "## La colonna `Unnamed: 0`\n",
        "Vedi [questa chat](https://chatgpt.com/share/68f74bca-554c-8012-a844-7260ce18391d) di chatGPT."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36889a3b-f5e6-406e-8905-83d23e15a66b",
      "metadata": {
        "id": "36889a3b-f5e6-406e-8905-83d23e15a66b"
      },
      "source": [
        "# Problemi frequenti nel caricamento dei file CSV in pandas.\n",
        "\n",
        "Ecco un elenco dei **problemi** pi√π comuni che si incontrano caricando **file CSV** con `pandas.read_csv()`, insieme a **cause** e **soluzioni tipiche**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea061b7-be45-4711-af0a-04cbac19db08",
      "metadata": {
        "id": "bea061b7-be45-4711-af0a-04cbac19db08"
      },
      "source": [
        "üß© **1. Colonne ‚ÄúUnnamed: 0‚Äù o ‚ÄúUnnamed: n‚Äù** - gi√† visto prima\n",
        "\n",
        "<u>Problema</u>: appare una colonna indesiderata chiamata `Unnamed: 0`.<br>\n",
        "<u>Causa</u>: spesso il CSV include un indice salvato da un precedente `DataFrame.to_csv()` (cio√® `index=True` di default).<br>\n",
        "<u>Soluzione</u>:\n",
        "```python\n",
        "    pd.read_csv(\"file.csv\", index_col=0)\n",
        "    # oppure\n",
        "    pd.read_csv(\"file.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ca7d0b-38e8-4a23-99b8-5a8ff7749428",
      "metadata": {
        "id": "11ca7d0b-38e8-4a23-99b8-5a8ff7749428"
      },
      "source": [
        "‚öôÔ∏è **2. Delimitatori sbagliati** - gi√† visto prima\n",
        "\n",
        "<u>Problema</u>: il file non viene separato correttamente (tutte le colonne finiscono in una sola).<br>\n",
        "<u>Causa</u>: il separatore non √® la virgola ma punto e virgola ;, tab \\t o altro.<br>\n",
        "<u>Soluzione</u>:\n",
        "```python\n",
        "    pd.read_csv(\"file.csv\", sep=\";\")      # per CSV europei\n",
        "    pd.read_csv(\"file.csv\", sep=\"\\t\")     # per file TSV\n",
        "```\n",
        "\n",
        "---\n",
        "Un TSV (*Tab-Separated Values*) √® praticamente un CSV, ma invece del separatore `,` oppure `;`, usa il tabulatore `\\t` come separatore di campo.\n",
        "Si pu√† anche rilevare automaticamente:\n",
        "```python\n",
        "    pd.read_csv(\"file.csv\", sep=None, engine=\"python\")\n",
        "```\n",
        "\n",
        "üß© Quando si usa un TSV?\n",
        "- quando i dati contengono molte virgole o punti e virgola (es. descrizioni di testo).\n",
        "- quando il file √® esportato da sistemi Unix o database (es. PostgreSQL COPY TO, Excel ‚Üí ‚ÄúTesto con tabulazioni‚Äù).\n",
        "- quando si vuole evitare ambiguit√† tra separatori decimali e di campo.\n",
        "\n",
        "---\n",
        "\n",
        "Il parametro `engine` in `pandas.read_csv()` serve a dire quale ‚Äúmotore di parsing‚Äù usare per leggere e interpretare il file CSV.<brr>\n",
        "In pratica, Pandas ha **due diversi ‚Äúparser engine‚Äù** che fanno lo stesso lavoro (cio√® leggere il file e trasformarlo in DataFrame), ma **con caratteristiche e prestazioni diverse**.\n",
        "\n",
        "1Ô∏è‚É£ **`engine=\"c\"`** ‚Üí il parser ‚Äúveloce‚Äù (default)\n",
        "- scritto in linguaggio C ‚Üí molto veloce\n",
        "- √® quello usato di default in quasi tutti i casi.\n",
        "- √® ottimo per file puliti e regolari.\n",
        "- ma... √® meno flessibile: non supporta tutte le opzioni, e pu√≤ fallire su CSV ‚Äúsporchi‚Äù o complessi.\n",
        "\n",
        "Esempio di uso:\n",
        "```python\n",
        "    pd.read_csv(\"file.csv\", engine=\"c\")\n",
        "```\n",
        "\n",
        "2Ô∏è‚É£ **`engine=\"python\"`** ‚Üí il parser ‚Äúrobusto‚Äù\n",
        "- scritto in puro Python ‚Üí pi√π lento, ma pi√π tollerante.\n",
        "- supporta opzioni che il parser C non gestisce bene, come:\n",
        "    - `sep=None` (cio√® **il rilevamento automatico del separatore**),\n",
        "    - delimitatori multipli o irregolari,\n",
        "    - linee malformate (on_bad_lines),\n",
        "    - quote e caratteri speciali complessi.\n",
        "\n",
        "Esempio d'uso:\n",
        "```python\n",
        "    pd.read_csv(\"file.csv\", sep=None, engine=\"python\")\n",
        "```\n",
        "\n",
        "üëâ Qui Pandas prova automaticamente a indovinare il separatore (',', ';', '\\t', ecc.) analizzando le prime righe.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a39c444-17bf-457e-aed9-9ba843517807",
      "metadata": {
        "id": "4a39c444-17bf-457e-aed9-9ba843517807"
      },
      "source": [
        "Torniamo all'elenco dei  problemi e soluzioni di `read_csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ee6a66-6045-429c-abfc-2e04eae898e9",
      "metadata": {
        "id": "f0ee6a66-6045-429c-abfc-2e04eae898e9"
      },
      "source": [
        "üî§ **3. Encoding errato**\n",
        "\n",
        "<u>Problema</u>: caratteri accentati o simboli speciali appaiono come ÔøΩ o danno errore `UnicodeDecodeError`.<br>\n",
        "<u>Causa</u>: il file non √® in `UTF-8` ma in `latin1`, `cp1252`, ecc.<br>\n",
        "<u>Soluzione</u>:\n",
        "```python\n",
        "    pd.read_csv(\"file.csv\", encoding=\"latin1\")\n",
        "```\n",
        "\n",
        "> Cos'√® `latin1`?\n",
        "> - `latin1` (o `ISO-8859-1`) √® una codifica a 1 byte (8 bit) **molto usata in Europa occidentale** prima che UTF-8 diventasse standard.\n",
        "> - contiene caratteri come:\n",
        ">   - lettere accentate italiane: `√†`, `√®`, `√©`, `√¨`, `√≤`, `√π`, ...\n",
        ">   - lettere spagnole/francesi/portoghesi: `√±`, `√ß`, `√°`, `√©`, `√µ`, ...\n",
        ">   - vocali con dieresi tedesche: `√§`, `√∂`, `√º`, ...\n",
        ">   - `√∏`, `√•` nordiche\n",
        "\n",
        "> **MA** latin1 NON supporta:\n",
        "> - emoji\n",
        "> - simbolo euro `‚Ç¨`\n",
        "> - caratteri greci, cirillici, arabi, cinesi, ecc.\n",
        "\n",
        "> **Quindi `latin `√® \"Europa occidentale anni '90\".** Se incontra caratteri greci, cirillici, ecc spesso usa il famoso simbolo ÔøΩ o d√† l'errore `UnicodeDecodeError`.\n",
        "\n",
        "\n",
        "<img src=\"ascii_latin1_utf_8.png\" alt=\"immagine\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6b34055",
      "metadata": {
        "id": "f6b34055"
      },
      "source": [
        "Test di **alcuni errori**, in vari passi:<br>\n",
        "1. creiamo un `DataFrame` con caratteri tipici `latin1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25fe2780",
      "metadata": {
        "id": "25fe2780"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"ID\": [1, 2, 3],\n",
        "    \"Nome\": [\"Andr√©\", \"Jos√©\", \"Ana Mar√≠a\"],\n",
        "    \"Citt√†\": [\"Torino\", \"M√°laga\", \"Z√ºrich\"],\n",
        "    \"Note\": [\n",
        "        \"pagato 50$ gi√† fatturato\",              # 'latin1' estende ASCII, che conteneva il carattere '$', dunque anche 'latin1' lo accetta\n",
        "        \"a√±o siguiente -> revisi√≥n t√©cnica\",\n",
        "        \"pi√π vecchio -> gi√† sostituito\"\n",
        "    ]\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1558aca4",
      "metadata": {
        "id": "1558aca4"
      },
      "source": [
        "2. salviamo il `DataFrame` in CSV `latin1` (ISO-8859-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1330ba6",
      "metadata": {
        "id": "b1330ba6",
        "outputId": "0282d025-b24a-41e8-c90d-36d2d411e565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creato file clienti_latin1.csv in encoding latin1\n"
          ]
        }
      ],
      "source": [
        "file_name = \"clienti_latin1.csv\"\n",
        "df.to_csv(\n",
        "    file_name,\n",
        "    index=False,\n",
        "    sep=\";\",             # mettiamo anche il separatore ; cos√¨ √® ancora pi√π realistico \"alla europea\"\n",
        "    encoding=\"latin1\"    # <-- punto chiave\n",
        ")\n",
        "\n",
        "print(f\"Creato file {file_name} in encoding latin1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "970238e5",
      "metadata": {
        "id": "970238e5"
      },
      "source": [
        "3. proviamo a rileggerlo SENZA specificare encoding.<br>\n",
        "Questo √® quello che di solito fa l'utente distratto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1726235f",
      "metadata": {
        "id": "1726235f",
        "outputId": "3ed59a07-93ef-482c-acbd-f950644644f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Errore di decodifica previsto leggendo senza encoding esplicito:\n",
            "'utf-8' codec can't decode byte 0xe0 in position 12: invalid continuation byte\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    df_fail = pd.read_csv(file_name, sep=\";\")  # nessun encoding passato, cio√® default = UTF-8\n",
        "    print(\"Letto senza errori?! Ecco le prime righe:\")\n",
        "    print(df_fail.head())\n",
        "except UnicodeDecodeError as e:\n",
        "    print(\"‚ö†Ô∏è Errore di decodifica previsto leggendo senza encoding esplicito:\")\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae0545da",
      "metadata": {
        "id": "ae0545da"
      },
      "source": [
        "4. d√† errore: vogliamo leggere un file `latin1` come `utf-8`.<br>\n",
        "soluzione corretta: leggiamo specificando `encoding=latin1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d90885c5",
      "metadata": {
        "id": "d90885c5",
        "outputId": "2e42539d-4742-4268-f81b-da3c9d1d3edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üí° Lettura corretta con encoding='latin1':\n",
            "   ID       Nome   Citt√†                               Note\n",
            "0   1      Andr√©  Torino           pagato 50$ gi√† fatturato\n",
            "1   2       Jos√©  M√°laga  a√±o siguiente -> revisi√≥n t√©cnica\n",
            "2   3  Ana Mar√≠a  Z√ºrich      pi√π vecchio -> gi√† sostituito\n"
          ]
        }
      ],
      "source": [
        "df_ok = pd.read_csv(file_name, sep=\";\", encoding=\"latin1\")\n",
        "print(\"\\nüí° Lettura corretta con encoding='latin1':\")\n",
        "print(df_ok.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02c27f7",
      "metadata": {
        "id": "e02c27f7"
      },
      "source": [
        "5. e cosa succede con il seguente dataframe, che contiene il carattere `‚Ç¨` (anzich√® `$`), che non fa parte n√® di `Ascii` n√® di `latin1`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e670c2b",
      "metadata": {
        "id": "4e670c2b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"ID\": [1, 2, 3],\n",
        "    \"Nome\": [\"Andr√©\", \"Jos√©\", \"Ana Mar√≠a\"],\n",
        "    \"Citt√†\": [\"Torino\", \"M√°laga\", \"Z√ºrich\"],\n",
        "    \"Note\": [\n",
        "        \"pagato 50‚Ç¨ gi√† fatturato\",\n",
        "        \"a√±o siguiente -> revisi√≥n t√©cnica\",\n",
        "        \"pi√π vecchio -> gi√† sostituito\"\n",
        "    ]\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb7752b6",
      "metadata": {},
      "source": [
        "Il seguente codice va in errore:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e1adf1fb",
      "metadata": {
        "id": "e1adf1fb",
        "outputId": "b38a7ab8-e8a3-4b43-908c-e10cfa350af2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclienti_latin1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m      3\u001b[0m     file_name,\n\u001b[0;32m      4\u001b[0m     index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m,             \u001b[38;5;66;03m# mettiamo anche il separatore ; cos√¨ √® ancora pi√π realistico \"alla europea\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m\"\u001b[39m    \u001b[38;5;66;03m# <-- punto chiave\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreato file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in encoding latin1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "file_name = \"clienti_latin1.csv\"\n",
        "df.to_csv(\n",
        "    file_name,\n",
        "    index=False,\n",
        "    sep=\";\",             # mettiamo anche il separatore ; cos√¨ √® ancora pi√π realistico \"alla europea\"\n",
        "    encoding=\"latin1\"    # <-- punto chiave\n",
        ")\n",
        "\n",
        "print(f\"Creato file {file_name} in encoding latin1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23514c60",
      "metadata": {
        "id": "23514c60"
      },
      "source": [
        "6. d√† errore, perch√® `latin1` non contiene il carattere `‚Ç¨`.<br>\n",
        "Occorre scrivere in `utf-8`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bb81607",
      "metadata": {
        "id": "7bb81607",
        "outputId": "33c0e3e5-7a32-4cb8-ff8f-3fd2a96f0b74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creato file clienti_latin1.csv in encoding latin1\n"
          ]
        }
      ],
      "source": [
        "file_name = \"clienti_latin1.csv\"\n",
        "df.to_csv(\n",
        "    file_name,\n",
        "    index=False,\n",
        "    sep=\";\",             # mettiamo anche il separatore ; cos√¨ √® ancora pi√π realistico \"alla europea\"\n",
        "    encoding=\"utf-8\"    # <-- punto chiave\n",
        ")\n",
        "\n",
        "print(f\"Creato file {file_name} in encoding latin1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e36a52b-1484-4355-9b4d-c85f6a1d80b6",
      "metadata": {
        "id": "7e36a52b-1484-4355-9b4d-c85f6a1d80b6"
      },
      "source": [
        "üìâ **4. Tipo di dato non corretto**\n",
        "\n",
        "<u>Problema</u>: colonne numeriche importate come stringhe (`object`).<br>\n",
        "<u>Causa</u>: presenza di separatori migliaia, simboli, o celle vuote.<br>\n",
        "<u>Soluzione</u>:\n",
        "```python\n",
        "    pd.read_csv(\"file.csv\", thousands=\".\", decimal=\",\")\n",
        "```\n",
        "oppure dopo la lettura:\n",
        "```python\n",
        "    df[\"col\"] = pd.to_numeric(df[\"col\"], errors=\"coerce\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e15596c",
      "metadata": {
        "id": "0e15596c",
        "outputId": "a85ec4d1-90a5-41e3-f779-013590f79954"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Creato file CSV 'prezzi_legacy.csv' con separatori migliaia '.' e decimali ','.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "################################################\n",
        "# 1. CREAZIONE CSV CON FORMATTAZIONE \"EUROPEA\" #\n",
        "################################################\n",
        "\n",
        "# Nota:\n",
        "# - \"1.234,50\" = mille duecentotrentaquattro virgola cinquanta\n",
        "# - \"2.000\"    = duemila (intero)\n",
        "# - \"\"         = cella vuota -> valore mancante\n",
        "\n",
        "df_orig = pd.DataFrame({\n",
        "    \"Prodotto\": [\"A123\", \"B777\", \"C900\", \"D010\"],\n",
        "    \"PrezzoUnitario\": [\"1.234,50\", \"99,99\", \"\", \"2.000,00\"],\n",
        "    \"Quantit√†\": [\"1.000\", \"250\", \"\", \"1.500\"]\n",
        "})\n",
        "\n",
        "csv_name = \"prezzi_legacy.csv\"\n",
        "\n",
        "# Salviamo come CSV con ';' perch√© √® molto comune negli export amministrativi italiani\n",
        "df_orig.to_csv(\n",
        "    csv_name,\n",
        "    index=False,\n",
        "    sep=\";\",\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "print(f\"[OK] Creato file CSV '{csv_name}' con separatori migliaia '.' e decimali ','.\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb500b0",
      "metadata": {
        "id": "5bb500b0",
        "outputId": "b2cfea58-9393-4ad8-9e77-f3ca316c23fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LETTURA SBAGLIATA (senza thousands/decimal) ===\n",
            "\n",
            "DataFrame letto (sbagliato):\n",
            "  Prodotto PrezzoUnitario  Quantit√†\n",
            "0     A123       1.234,50       1.0\n",
            "1     B777          99,99     250.0\n",
            "2     C900            NaN       NaN\n",
            "3     D010       2.000,00       1.5\n",
            "\n",
            "Tipi di dato dopo lettura sbagliata:\n",
            "Prodotto           object\n",
            "PrezzoUnitario     object\n",
            "Quantit√†          float64\n",
            "dtype: object\n",
            "\n",
            "Provo a sommare la colonna Quantit√† (che √® testo):\n",
            "252.5\n"
          ]
        }
      ],
      "source": [
        "###################################\n",
        "# 2. LETTURA SBAGLIATA (DEFAULT)  #\n",
        "###################################\n",
        "\n",
        "print(\"=== LETTURA SBAGLIATA (senza thousands/decimal) ===\")\n",
        "\n",
        "df_bad = pd.read_csv(\n",
        "    csv_name,\n",
        "    sep=\";\"          # leggiamo correttamente il separatore di colonna\n",
        "    # ma NON diciamo a pandas come interpretare i numeri (migliaia e decimali)\n",
        ")\n",
        "\n",
        "print(\"\\nDataFrame letto (sbagliato):\")\n",
        "print(df_bad)\n",
        "\n",
        "print(\"\\nTipi di dato dopo lettura sbagliata:\")\n",
        "print(df_bad.dtypes)\n",
        "\n",
        "# Prova operazioni numeriche: qui 'Quantit√†' e 'PrezzoUnitario' sono ancora stringhe (object)\n",
        "print(\"\\nProvo a sommare la colonna Quantit√† (che √® testo):\")\n",
        "try:\n",
        "    print(df_bad[\"Quantit√†\"].sum())\n",
        "except Exception as e:\n",
        "    print(\"Errore durante la somma:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e73f674",
      "metadata": {
        "id": "3e73f674"
      },
      "source": [
        "**Perch√© `PrezzoUnitario` √® `object` in `df_bad.dtypes` (anzich√® `float`)?**\n",
        "\n",
        "Per tre motivi insieme:<br>\n",
        "\n",
        "**1. Il separatore decimale √® una virgola, non un punto**<br>\n",
        "*Esempio*: *1.234,50*<br>\n",
        "per pandas (senza istruzioni extra), *1.234,50* non √® un numero valido, **√® una stringa**.<br>\n",
        "pandas si aspetta infatti *1234.50* (punto per i decimali, niente separatore migliaia).<br>\n",
        "Quindi lo lascia come stringa di testo (`object`).\n",
        "\n",
        "**2. C'√® il separatore delle migliaia `.`**<br>\n",
        "Guarda *2.000,00*:<br>\n",
        "l'ideale per pandas sarebbe *2000.00*<br>\n",
        "invece trova *2.000,00*, che sembra ‚Äú2 punto 000 virgola 00‚Äù.<br>\n",
        "Per il parser standard questo non √® un float valido ‚Üí resta stringa (`object`).\n",
        "\n",
        "Stessa cosa per *1.000* nella colonna `Quantit√†`: pandas non sa se √® ‚Äúmille‚Äù oppure ‚Äúuno virgola zero zero zero‚Äù.<br>\n",
        "Quindi preferisce NON indovinare e la tiene testo (`object`).\n",
        "\n",
        "**3. Ci sono celle vuote**<br>\n",
        "Nella colonna hai valori tipo \"\" (stringa vuota).<br>\n",
        "Quindi dentro la stessa colonna hai:\n",
        "- *1.234,50* (testo)\n",
        "- *99,99* (testo)\n",
        "- \"\" (testo vuoto)\n",
        "- *2.000,00* (testo)\n",
        "\n",
        "Colonna eterogenea ‚Üí pandas dice: ‚Äúok, tutto `object` (stringhe) e non parliamone pi√π‚Äù.\n",
        "\n",
        "Se tutti i valori fossero numeri chiari in stile inglese (1234.50, 99.99, 2000.00, ecc.) allora pandas avrebbe inferito `float64` da solo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2560874",
      "metadata": {
        "id": "b2560874",
        "outputId": "8280c62e-f112-4ec7-d19e-2ff5567cddab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "=== LETTURA CORRETTA (thousands='.', decimal=',') ===\n",
            "\n",
            "DataFrame letto (corretto):\n",
            "  Prodotto  PrezzoUnitario  Quantit√†\n",
            "0     A123         1234.50    1000.0\n",
            "1     B777           99.99     250.0\n",
            "2     C900             NaN       NaN\n",
            "3     D010         2000.00    1500.0\n",
            "\n",
            "Tipi di dato dopo lettura corretta:\n",
            "Prodotto           object\n",
            "PrezzoUnitario    float64\n",
            "Quantit√†          float64\n",
            "dtype: object\n",
            "\n",
            "Somma Quantit√† (ora numerica):\n",
            "2750.0\n",
            "\n",
            "Somma PrezzoUnitario (notare i NaN dove c'erano celle vuote):\n",
            "3334.49\n"
          ]
        }
      ],
      "source": [
        "#############################################\n",
        "# 3. LETTURA CORRETTA (parsing in ingresso) #\n",
        "#############################################\n",
        "\n",
        "print(\"\\n\\n=== LETTURA CORRETTA (thousands='.', decimal=',') ===\")\n",
        "\n",
        "df_good = pd.read_csv(\n",
        "    csv_name,\n",
        "    sep=\";\",\n",
        "    thousands=\".\",  # rimuovi separatore migliaia\n",
        "    decimal=\",\"     # interpreta la virgola come separatore decimale\n",
        ")\n",
        "\n",
        "print(\"\\nDataFrame letto (corretto):\")\n",
        "print(df_good)\n",
        "\n",
        "print(\"\\nTipi di dato dopo lettura corretta:\")\n",
        "print(df_good.dtypes)\n",
        "\n",
        "print(\"\\nSomma Quantit√† (ora numerica):\")\n",
        "print(df_good[\"Quantit√†\"].sum())\n",
        "\n",
        "print(\"\\nSomma PrezzoUnitario (notare i NaN dove c'erano celle vuote):\")\n",
        "print(df_good[\"PrezzoUnitario\"].sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84574a93-10bb-434d-8295-e9164b1e51a5",
      "metadata": {
        "id": "84574a93-10bb-434d-8295-e9164b1e51a5"
      },
      "source": [
        "üßæ **5. Header non nella prima riga** - gi√† visto prima\n",
        "\n",
        "<u>Problema</u>: i nomi di colonna non vengono letti correttamente.<br>\n",
        "<u>Causa</u>: il file ha righe descrittive o metadati iniziali.<br>\n",
        "<u>Soluzione</u>:\n",
        "```python\n",
        "    pd.read_csv(\"file.csv\", header=2)   # se l‚Äôintestazione √® alla terza riga\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63024a49",
      "metadata": {
        "id": "63024a49",
        "outputId": "04f41998-1b44-471d-e5ab-da20931dd9b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creato file dati_commentati.csv\n"
          ]
        }
      ],
      "source": [
        "# creazione del file csv\n",
        "import csv\n",
        "\n",
        "file_name = \"dati_commentati.csv\"\n",
        "\n",
        "with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    # Scrivo manualmente due righe iniziali commentate\n",
        "    f.write(\"# Questo file contiene dati di esempio\\n\")\n",
        "    f.write(\"# Formato: ID,Nome,Valore\\n\")\n",
        "\n",
        "    writer = csv.writer(f, delimiter=\";\")\n",
        "\n",
        "    # Header vero e proprio\n",
        "    writer.writerow([\"ID\", \"Nome\", \"Valore\"])\n",
        "\n",
        "    # 3 righe di dati\n",
        "    writer.writerow([1, \"Alpha\", 10.5])\n",
        "    writer.writerow([2, \"Beta\", 20.0])\n",
        "    writer.writerow([3, \"Gamma\", 7.25])\n",
        "\n",
        "print(f\"Creato file {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f1f8539e",
      "metadata": {
        "id": "f1f8539e",
        "outputId": "03939264-2b8b-48ea-b5e8-00899482436d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# lettura errata\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_csv(file_name)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# lettura errata\n",
        "pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19dcfd25",
      "metadata": {
        "id": "19dcfd25",
        "outputId": "2f5f4568-dac4-4b87-a455-32c4b78db711"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Nome</th>\n",
              "      <th>Valore</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Alpha</td>\n",
              "      <td>10.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Beta</td>\n",
              "      <td>20.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Gamma</td>\n",
              "      <td>7.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID   Nome  Valore\n",
              "0   1  Alpha   10.50\n",
              "1   2   Beta   20.00\n",
              "2   3  Gamma    7.25"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# lettura corretta\n",
        "pd.read_csv(\n",
        "    file_name,\n",
        "    sep=\";\",        # separatore di colonna\n",
        "    header=2)       # l'header √® nella terza riga (python conta da 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "977f70f4",
      "metadata": {
        "id": "977f70f4",
        "outputId": "8b6c9fec-b7f0-4de1-de1d-a45da1e93ce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ID   Nome  Valore\n",
            "0   1  Alpha   10.50\n",
            "1   2   Beta   20.00\n",
            "2   3  Gamma    7.25\n",
            "ID          int64\n",
            "Nome       object\n",
            "Valore    float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# lettura elegante\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"dati_commentati.csv\",\n",
        "    sep=\";\",        # separatore di campo\n",
        "    comment=\"#\"     # ignora tutte le righe che iniziano con '#'\n",
        ")\n",
        "\n",
        "print(df)\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a34b8c9-0a5b-4333-82b7-1d87f085d1a5",
      "metadata": {
        "id": "3a34b8c9-0a5b-4333-82b7-1d87f085d1a5"
      },
      "source": [
        "ü™ì **6. File troppo grande**\n",
        "\n",
        "<u>Problema</u>: `MemoryError` o caricamento lentissimo.<br>\n",
        "<u>Causa</u>: CSV molto grande rispetto alla RAM.<br>\n",
        "<u>Soluzione</u>:\n",
        "\n",
        "Caricamento a **chunk**:\n",
        "```python\n",
        "    for chunk in pd.read_csv(\"file.csv\", chunksize=100000):\n",
        "        process(chunk)                                        # 'process' √® una funzione utente\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd48b972",
      "metadata": {
        "id": "dd48b972"
      },
      "source": [
        "Vediamo il **funzionamento dei *chunk*** in <u>due parti</u>:\n",
        "\n",
        "**1. la lettura interna**:\n",
        "```python\n",
        "    pd.read_csv(..., chunksize=100000)\n",
        "```\n",
        "\n",
        "Normalmente la funzione `pd.read_csv(\"file.csv\")` **legge tutto il file in RAM** e restituisce **un unico `DataFrame`**.<br>\n",
        "Con `chunksize=100000`, invece, pandas NON carica tutto.<br>\n",
        "Restituisce un **`iterator`** (un generatore) che produce un `DataFrame` alla volta, ciascuno con **al massimo 100000 righe**.\n",
        "\n",
        "Quindi:\n",
        "- primo ciclo ‚Üí righe 0‚Äì99999\n",
        "- secondo ciclo ‚Üí righe 100000‚Äì199999\n",
        "- terzo ciclo ‚Üí ecc.\n",
        "\n",
        "‚Ä¶fino a fine file.\n",
        "\n",
        "‚ö†Ô∏è Questo vuol dire che **in memoria, in ogni momento, ci sono solo 100k righe**, non milioni/miliardi. E' perfetto se **il file √® troppo grande per stare tutto in RAM**.\n",
        "\n",
        "**2. il ciclo esterno**:\n",
        "```python\n",
        "    for chunk in ... :\n",
        "```\n",
        "\n",
        "`chunk` √® un `DataFrame` pandas ‚Äúparziale‚Äù, cio√® una fetta del CSV.<br>\n",
        "Il `for` cicla su tutte le fette del file, una dopo l‚Äôaltra.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8773d63e",
      "metadata": {
        "id": "8773d63e"
      },
      "source": [
        "üß™ Vediamo **un esempio concreto**, in due passi:\n",
        "- definiamo una **funzione per creare il file csv**, <u>in due varianti</u>:\n",
        "    - la versione <u>piccola</u> della funzione (10 righe) ‚Äî utile da guardare a occhio\n",
        "    - la versione <u>grande</u> della funzione (1_000_000 righe) ‚Äî utile per i test seri su `chunk`\n",
        "    - si pu√≤ scegliere quale versione usare cambiando solo l'argomento `n_righe` alla chiamata.\n",
        "- processiamo a chunk per **calcolare la somma globale della colonna `Importo`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e27792",
      "metadata": {
        "id": "28e27792",
        "outputId": "0433c720-6aad-4cbb-f10f-b79360153a10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creato file CSV 'transazioni_demo.csv' con 10 righe.\n",
            "Creato file CSV 'transazioni_grandi.csv' con 1000000 righe.\n"
          ]
        }
      ],
      "source": [
        "# PASSO 1: definizione di una funzione che serve a creare un CSV di grandi dimensioni per testare il caricamento a chunk\n",
        "\n",
        "import csv\n",
        "import random\n",
        "import datetime\n",
        "\n",
        "def crea_csv_grande(\n",
        "    file_name=\"transazioni_grandi.csv\",      # default\n",
        "    n_righe=1_000_000,                       # default  (1_000_000: sintassi sugar per rendere il numero pi√π leggibile)\n",
        "    seed=42                                  # default\n",
        "):\n",
        "    \"\"\"\n",
        "    Crea un CSV con molte righe, con le colonne:\n",
        "    ID, DataOperazione, Categoria, Importo\n",
        "\n",
        "    - ID: intero progressivo\n",
        "    - DataOperazione: data fittizia\n",
        "    - Categoria: tipo transazione (es. Vendita / Rimborso / Spesa)\n",
        "    - Importo: float positivo o negativo\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "\n",
        "    categorie = [\n",
        "        \"Vendita\",\n",
        "        \"Rimborso\",\n",
        "        \"Spesa Marketing\",\n",
        "        \"Spesa Fornitore\",\n",
        "        \"Abbonamento\",\n",
        "        \"Servizio\"\n",
        "    ]\n",
        "\n",
        "    start_date = datetime.date(2024, 1, 1)\n",
        "\n",
        "    # Creo il file CSV\n",
        "    with open(file_name, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f, delimiter=\",\")\n",
        "\n",
        "        # Header\n",
        "        writer.writerow([\"ID\", \"DataOperazione\", \"Categoria\", \"Importo\"])\n",
        "\n",
        "        for i in range(1, n_righe + 1):\n",
        "            # data = start_date + offset giorni\n",
        "            data_operazione = start_date + datetime.timedelta(days=i % 365)\n",
        "\n",
        "            categoria = random.choice(categorie)\n",
        "\n",
        "            # Importo:\n",
        "            # - vendite positive tra 10 e 500\n",
        "            # - rimborsi negativi tra -200 e -5\n",
        "            # - spese negative tra -1000 e -20\n",
        "            if categoria == \"Vendita\" or categoria == \"Abbonamento\" or categoria == \"Servizio\":\n",
        "                importo = round(random.uniform(10, 500), 2)\n",
        "            elif categoria == \"Rimborso\":\n",
        "                importo = round(random.uniform(-200, -5), 2)\n",
        "            else:\n",
        "                # Spesa Marketing / Spesa Fornitore\n",
        "                importo = round(random.uniform(-1000, -20), 2)\n",
        "\n",
        "            writer.writerow([\n",
        "                i,\n",
        "                data_operazione.isoformat(),  # tipo 2024-03-15\n",
        "                categoria,\n",
        "                importo\n",
        "            ])\n",
        "\n",
        "    print(f\"Creato file CSV '{file_name}' con {n_righe} righe.\")\n",
        "\n",
        "# Esempio di utilizzo della funzione (il MAIN)\n",
        "# if __name__ == \"__main__\": serve per dire:\n",
        "# - \"esegui questo blocco di codice solo se sto lanciando direttamente questo file, e NON se lo sto importando da un altro file\".\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # VERSIONE DEMO PICCOLA (per guardarlo a occhio)\n",
        "    crea_csv_grande(\"transazioni_demo.csv\", n_righe=10)\n",
        "\n",
        "    # VERSIONE GROSSA (per testare chunksize ecc.)\n",
        "    # ATTENZIONE: questo crea ~1 milione di righe.\n",
        "    # Cambia questo numero come vuoi.\n",
        "    crea_csv_grande(\"transazioni_grandi.csv\", n_righe=1_000_000)   # 1_000_000: sintassi sugar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e97687",
      "metadata": {
        "id": "a1e97687",
        "outputId": "1f7bdbda-f5d3-4093-97c7-3cfcb7838110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Totale Importo: -59772293.91\n"
          ]
        }
      ],
      "source": [
        "# PASSO 2: processo a chunk:\n",
        "\n",
        "totale = 0.0\n",
        "\n",
        "for chunk in pd.read_csv(\"transazioni_grandi.csv\", chunksize=100_000):   # sintassi numerica sugar\n",
        "    totale += chunk[\"Importo\"].sum()\n",
        "\n",
        "print(\"Totale Importo:\", totale)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab4ce36e",
      "metadata": {
        "id": "ab4ce36e"
      },
      "source": [
        "Il codice della cella precedente d√† **la somma globale della colonna `Importo`** senza mai caricare tutto il milione di righe in RAM in un solo colpo ‚úÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3cb71a",
      "metadata": {
        "id": "8b3cb71a"
      },
      "source": [
        "---\n",
        "\n",
        "**Commento al risultato NUMERICO ottenuto**<br>\n",
        "\n",
        "Il dataset che abbiamo creato prima ha sia entrate che uscite.<br>\n",
        "Nel generatore CSV avevamo questa logica:\n",
        "- Categorie tipo `Vendita`, `Abbonamento`, `Servizio` ‚Üí importi **positivi** (ricavi, +10 a +500)\n",
        "- Categorie tipo `\"Rimborso\"` ‚Üí importi **negativi** (rimborsi al cliente, -5 a -200)\n",
        "- `Spesa Marketing` e `Spesa Fornitore` ‚Üí importi **negativi grandi** (costi, tra -1000 e -20)\n",
        "\n",
        "Quindi:\n",
        "- le vendite portano soldi dentro,\n",
        "- le spese e i fornitori drenano soldi fuori,\n",
        "- e spesso i costi sono in valore assoluto pi√π grandi delle vendite.\n",
        "\n",
        "Se nel campione ci sono tante righe di costo rispetto alle vendite, il saldo finale va gi√π pesante ‚Üí da qui il totale molto negativo tipo `-59,772,293.91.`\n",
        "\n",
        "Tradotto in business: **stiamo spendendo pi√π di quanto incassiamo** üòÖ.\n",
        "\n",
        "√à un risultato ‚Äúnormale‚Äù?<br>\n",
        "**S√¨, √® coerente con la generazione casuale**:\n",
        "- gli importi negativi possibili arrivano fino a -1000\n",
        "- gli importi positivi arrivano solo fino a +500\n",
        "\n",
        "Quindi anche se met√† righe fossero vendite e met√† spese, la parte spese vince comunque in valore assoluto.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86ac0d14-6fb4-4909-9c4d-1935bc98df71",
      "metadata": {
        "id": "86ac0d14-6fb4-4909-9c4d-1935bc98df71"
      },
      "source": [
        "üßÆ **7. Colonne con valori mancanti o disallineati**\n",
        "\n",
        "<u>Problema</u>: righe con numero diverso di colonne, errore tipo `ParserError: Error tokenizing data`.<br>\n",
        "<u>Causa</u>: virgolette non chiuse o separatori dentro i campi.<br>\n",
        "<u>Soluzione</u>:\n",
        "```python\n",
        "pd.read_csv(\"file.csv\", on_bad_lines=\"skip\", quoting=csv.QUOTE_NONE)\n",
        "```\n",
        "\n",
        "Oppure controllare i delimitatori."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20fc8974",
      "metadata": {
        "id": "20fc8974"
      },
      "source": [
        "Segue il codice in 3 passi (per ognuno dei due errori):\n",
        "- creazione del file csv\n",
        "- lettura errata (voluta)\n",
        "- lettura robusta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "705f99a3",
      "metadata": {
        "id": "705f99a3",
        "outputId": "c6a78c3c-3fa8-4f8a-a09f-d86ea9a08794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ creati i due file CSV di test\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------\n",
        "# 1) CSV con VIRGOLETTE NON CHIUSE  -\n",
        "# -----------------------------------\n",
        "content_unclosed = \"\"\"id,name,amount,notes\n",
        "1,Mario Rossi,1200,OK\n",
        "2,Luigi Bianchi,950,pagato\n",
        "3,Carla Verdi,800,\"nota con virgolette non chiuse\n",
        "4,Paolo Neri,700,ok\n",
        "\"\"\"\n",
        "\n",
        "file_unclosed = \"csv_virgolette_non_chiuse.csv\"\n",
        "with open(file_unclosed, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    f.write(content_unclosed)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2) CSV con SEPARATORI DENTRO I CAMPI (non quotati) -\n",
        "# ----------------------------------------------------\n",
        "# header: 4 colonne\n",
        "content_separators = \"\"\"id,name,city,amount\n",
        "1,Mario Rossi,Milano,1200\n",
        "2,Luigi Bianchi,Roma,900\n",
        "3,Carla Verdi,Milano, Italia,800\n",
        "4,Paolo Neri,Torino,700\n",
        "\"\"\"\n",
        "\n",
        "file_separators = \"csv_separatori_dentro_campi.csv\"\n",
        "with open(file_separators, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    f.write(content_separators)\n",
        "\n",
        "print(\"‚úÖ creati i due file CSV di test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a19bd7b",
      "metadata": {
        "id": "8a19bd7b",
        "outputId": "6058669c-063d-4a13-b2e3-10d7adba7550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 1) TEST: virgolette non chiuse ===\n",
            "‚ùå errore atteso (virgolette non chiuse):\n",
            "Error tokenizing data. C error: EOF inside string starting at row 3\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# TEST di LETTURA file 1   =\n",
        "# ==========================\n",
        "\n",
        "print(\"\\n=== 1) TEST: virgolette non chiuse ===\")\n",
        "try:\n",
        "    df1 = pd.read_csv(file_unclosed)\n",
        "    print(df1)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå errore atteso (virgolette non chiuse):\")\n",
        "    print(e)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d30c4ef6",
      "metadata": {
        "id": "d30c4ef6",
        "outputId": "b4e9d0d8-fe58-4965-8482-18e6a0abc34a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ lettura robusta (file 1):\n",
            "   id           name  amount                            notes\n",
            "0   1    Mario Rossi    1200                               OK\n",
            "1   2  Luigi Bianchi     950                           pagato\n",
            "2   3    Carla Verdi     800  \"nota con virgolette non chiuse\n",
            "3   4     Paolo Neri     700                               ok\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# lettura 'robusta' del file 1  =\n",
        "# ===============================\n",
        "df1_ok = pd.read_csv(\n",
        "    file_unclosed,\n",
        "    on_bad_lines=\"skip\",\n",
        "    quoting=csv.QUOTE_NONE,\n",
        "    engine=\"python\",\n",
        ")\n",
        "print(\"\\n‚úÖ lettura robusta (file 1):\")\n",
        "print(df1_ok)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23db78eb",
      "metadata": {
        "id": "23db78eb"
      },
      "source": [
        "`quoting = csv.QUOTE_NONE`:<br>\n",
        "serve proprio a dirgli: ‚Äúnon trattare le virgolette (\") come qualcosa di speciale, considerale testo normale‚Äù.\n",
        "\n",
        "Perch√© abbiamo messo anche `engine=\"python\"`?<br>\n",
        "Perch√© con CSV sporchi, il parser C √® **molto veloce ma meno tollerante**; quello Python **√® pi√π ‚Äúpaziente‚Äù** e, <u>combinato con on_bad_lines=\"skip\" e quoting=csv.QUOTE_NONE</u>, permette di saltare solo le righe storte e caricare il resto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e797e8be",
      "metadata": {
        "id": "e797e8be",
        "outputId": "aecbf25c-1cbc-441f-c1ac-066d15b1dea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 2) TEST: separatori dentro i campi ===\n",
            "‚ùå errore atteso (troppi separatori):\n",
            "Error tokenizing data. C error: Expected 4 fields in line 4, saw 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================\n",
        "# TEST di LETTURA file 2   =\n",
        "# ==========================\n",
        "\n",
        "print(\"\\n=== 2) TEST: separatori dentro i campi ===\")\n",
        "try:\n",
        "    df2 = pd.read_csv(file_separators)\n",
        "    print(df2)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå errore atteso (troppi separatori):\")\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc2d57bb",
      "metadata": {
        "id": "dc2d57bb",
        "outputId": "3676d4e9-d253-416f-cd8e-ff2abb6a94f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ lettura robusta (file 2):\n",
            "   id           name    city  amount\n",
            "0   1    Mario Rossi  Milano    1200\n",
            "1   2  Luigi Bianchi    Roma     900\n",
            "2   4     Paolo Neri  Torino     700\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# lettura 'robusta' del file 2  =\n",
        "# ===============================\n",
        "\n",
        "df2_ok = pd.read_csv(\n",
        "    file_separators,\n",
        "    on_bad_lines=\"skip\",\n",
        "    quoting=csv.QUOTE_NONE,\n",
        "    engine=\"python\",\n",
        ")\n",
        "print(\"\\n‚úÖ lettura robusta (file 2):\")\n",
        "print(df2_ok)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caab5064-1469-451a-8732-786123690fa4",
      "metadata": {
        "id": "caab5064-1469-451a-8732-786123690fa4"
      },
      "source": [
        "üß† **8. Date non interpretate correttamente**\n",
        "\n",
        "<u>Problema</u>: le date restano stringhe o sono nel formato errato.<br>\n",
        "<u>Soluzione</u>:\n",
        "```python\n",
        "pd.read_csv(\"file.csv\", parse_dates=[\"data\"])\n",
        "```\n",
        "\n",
        "oppure\n",
        "```python\n",
        "df[\"data\"] = pd.to_datetime(df[\"data\"], dayfirst=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c384078",
      "metadata": {
        "id": "2c384078"
      },
      "source": [
        "Creiamo un CSV con deliberatamente dentro date mischiate (italiane, americane, con testo, con ore) cos√¨ `read_csv` non le riconosce e le lascia come `object`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1a34ec",
      "metadata": {
        "id": "ac1a34ec",
        "outputId": "5fa1ec7e-7713-46d7-a41a-3087316baf8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ creato date_mischiate.csv\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Creazione CSV con date \"brutte\"\n",
        "# CSV \"cattivo\": formati data diversi ‚Üí pandas non riesce a unificarli\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "csv_text = \"\"\"data;descrizione;importo\n",
        "01/02/2025;Fattura cliente A;120.50\n",
        "2025-02-01;Fattura cliente B;85.00\n",
        "12/31/2024;Formato USA;15.75\n",
        "31/12/2024;Chiusura anno;999.99\n",
        "2025/02/01 14:30;Con orario;50.00\n",
        ";Data mancante;0.00\n",
        "non-data;Valore sporco;5.25\n",
        "\"\"\"\n",
        "\n",
        "with open(\"date_mischiate.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    f.write(csv_text)\n",
        "\n",
        "print(\"‚úÖ creato date_mischiate.csv\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7adf127f",
      "metadata": {
        "id": "7adf127f"
      },
      "source": [
        "E' un file CSV con date miste.\n",
        "\n",
        "Ora facciamo la lettura ‚Äúingenua‚Äù (pandas non capisce le date e le lascia `object`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e92d4f6",
      "metadata": {
        "id": "2e92d4f6",
        "outputId": "3f316ca0-a12d-49e6-c1cf-876d248629c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¥ LETTURA INGENUA\n",
            "data            object\n",
            "descrizione     object\n",
            "importo        float64\n",
            "dtype: object\n",
            "               data        descrizione  importo\n",
            "0        01/02/2025  Fattura cliente A   120.50\n",
            "1        2025-02-01  Fattura cliente B    85.00\n",
            "2        12/31/2024        Formato USA    15.75\n",
            "3        31/12/2024      Chiusura anno   999.99\n",
            "4  2025/02/01 14:30         Con orario    50.00\n",
            "5               NaN      Data mancante     0.00\n",
            "6          non-data      Valore sporco     5.25 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. Lettura \"sbagliata\" / ingenua\n",
        "df_raw = pd.read_csv(\"date_mischiate.csv\", sep=\";\")\n",
        "print(\"üî¥ LETTURA INGENUA\")\n",
        "print(df_raw.dtypes)\n",
        "print(df_raw, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4da2ad3",
      "metadata": {
        "id": "f4da2ad3"
      },
      "source": [
        "`data` √® `object` ‚Üí cio√® stringa.<br>\n",
        "Adesso le due soluzioni:\n",
        "\n",
        "Soluzione 1 ‚Äì direttamente in `read_csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47731e86",
      "metadata": {
        "id": "47731e86",
        "outputId": "c577319e-3795-4600-fcaf-d05e8d66a613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üü† LETTURA CON parse_dates (pandas non ce la fa)\n",
            "data            object\n",
            "descrizione     object\n",
            "importo        float64\n",
            "dtype: object\n",
            "               data        descrizione  importo\n",
            "0        01/02/2025  Fattura cliente A   120.50\n",
            "1        2025-02-01  Fattura cliente B    85.00\n",
            "2        12/31/2024        Formato USA    15.75\n",
            "3        31/12/2024      Chiusura anno   999.99\n",
            "4  2025/02/01 14:30         Con orario    50.00\n",
            "5               NaN      Data mancante     0.00\n",
            "6          non-data      Valore sporco     5.25 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 3) LETTURA con parse_dates\n",
        "#    ‚Üí con queste date miste, pandas si arrende\n",
        "#    ‚Üí data RESTA object\n",
        "# =========================================\n",
        "df_auto = pd.read_csv(\n",
        "    \"date_mischiate.csv\",\n",
        "    sep=\";\",\n",
        "    parse_dates=[\"data\"],\n",
        "    dayfirst=True\n",
        ")\n",
        "print(\"üü† LETTURA CON parse_dates (pandas non ce la fa)\")\n",
        "print(df_auto.dtypes)\n",
        "print(df_auto, \"\\n\")\n",
        "# üëâ data = object\n",
        "#    perch√© nel file ci sono: dd/mm/yyyy, ISO, mm/dd/yyyy, con orario, vuote, testo...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97cff7fc",
      "metadata": {
        "id": "97cff7fc",
        "outputId": "de08422a-dffb-4447-986e-4c2d6965281a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üü¢ LETTURA ROBUSTA (dopo apply)\n",
            "data           datetime64[ns]\n",
            "descrizione            object\n",
            "importo               float64\n",
            "dtype: object\n",
            "                 data        descrizione  importo\n",
            "0 2025-02-01 00:00:00  Fattura cliente A   120.50\n",
            "1 2025-02-01 00:00:00  Fattura cliente B    85.00\n",
            "2 2024-12-31 00:00:00        Formato USA    15.75\n",
            "3 2024-12-31 00:00:00      Chiusura anno   999.99\n",
            "4 2025-02-01 14:30:00         Con orario    50.00\n",
            "5                 NaT      Data mancante     0.00\n",
            "6                 NaT      Valore sporco     5.25\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# 4) LETTURA ROBUSTA (quella che DEVE riuscire)\n",
        "#    ‚Üí leggiamo come stringa\n",
        "#    ‚Üí convertiamo noi una per una\n",
        "# =========================================\n",
        "\n",
        "# definizione di una funzione di parsing \"flessibile\"\n",
        "def parse_flessibile(x: str):\n",
        "    if pd.isna(x) or x == \"\":\n",
        "        return pd.NaT\n",
        "    # proviamo pi√π formati noti\n",
        "    for fmt in (\"%d/%m/%Y\", \"%Y-%m-%d\", \"%m/%d/%Y\", \"%Y/%m/%d %H:%M\", \"%d/%m/%Y %H:%M\"):\n",
        "        try:\n",
        "            return datetime.strptime(x, fmt)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return pd.NaT   # quello che proprio non √® data\n",
        "\n",
        "#la apply sulla colonna \"data\"\n",
        "df_ok = pd.read_csv(\"date_mischiate.csv\", sep=\";\")\n",
        "df_ok[\"data\"] = df_ok[\"data\"].apply(parse_flessibile)\n",
        "\n",
        "print(\"üü¢ LETTURA ROBUSTA (dopo apply)\")\n",
        "print(df_ok.dtypes)\n",
        "print(df_ok)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c825068",
      "metadata": {
        "id": "6c825068"
      },
      "source": [
        "Vediamo **riga per riga** cosa fa il codice della cella precedente:\n",
        "\n",
        "1. `def parse_flessibile(x: str):`<br>\n",
        "definisce una funzione che riceve **una sola cella** (una stringa) e restituisce una **data** oppure `NaT`.\n",
        "\n",
        "2. `if pd.isna(x) or x == \"\":`<br>\n",
        "se la cella √® vuota (`\"\"`) oppure √® un NA (`NaN` letto da pandas) ‚Üí non prova nemmeno ‚Üí **restituisce** `pd.NaT`.<br>\n",
        "(`pd.NaT` = Not A Time, l‚Äôequivalente di `NaN` ma per le date.)\n",
        "\n",
        "3. `for fmt in (...):`<br>\n",
        "qui c‚Äô√® la lista dei **formati che vuole provare**, in ordine:\n",
        "    - `\"%d/%m/%Y\"` ‚Üí 31/12/2024 (italiano)\n",
        "    - `\"%Y-%m-%d\"` ‚Üí 2025-02-01 (ISO)\n",
        "    - `\"%m/%d/%Y\"` ‚Üí 12/31/2024 (americano)\n",
        "    - `\"%Y/%m/%d %H:%M\"` ‚Üí 2025/02/01 14:30\n",
        "    - `\"%d/%m/%Y %H:%M\"` ‚Üí 31/12/2024 09:15\n",
        "\n",
        "    L‚Äôidea √®: ‚Äúnon sa il formato ‚Üí li prova tutti‚Äù.\n",
        "\n",
        "4. `try: return datetime.strptime(x, fmt)`<br>\n",
        "prova a convertire *quella singola stringa* col formato corrente.\n",
        "- se **funziona** ‚Üí esce subito dalla funzione (`return`) ed abbiamo la `datetime`\n",
        "- se **non funziona** ‚Üí scatta il `ValueError` ‚Üí va nell‚Äô`except`\n",
        "\n",
        "5. `except ValueError: continue`<br>\n",
        "cio√®: ‚Äúok, con questo formato non andava ‚Üí prova il prossimo‚Äù.\n",
        "\n",
        "6. `return pd.NaT` (**alla fine**)<br>\n",
        "se provati **tutti** i formati e nessuno ha funzionato ‚Üí quella riga non √® una data ‚Üí la si marca come vuota (`NaT`).\n",
        "\n",
        "---\n",
        "\n",
        "**Perch√© c‚Äô√® l‚Äôapply**??\n",
        "```python\n",
        "df_ok = pd.read_csv(\"date_mischiate.csv\", sep=\";\")\n",
        "df_ok[\"data\"] = df_ok[\"data\"].apply(parse_flessibile)\n",
        "```\n",
        "\n",
        "- `read_csv(...)` legge tutta la colonna come stringhe (perch√© erano tutte diverse).\n",
        "- `df_ok[\"data\"].apply(parse_flessibile)` vuol dire:<br>\n",
        "    ‚Äúper **ogni riga** della colonna `data` esegue `parse_flessibile(...)`‚Äù.\n",
        "- il risultato √® **una nuova Series di tipo `datetime`** (con dentro anche dei `NaT`).\n",
        "- la riassegna a `df_ok[\"data\"]` ‚Üí la colonna diventa davvero `datetime64[ns]`.\n",
        "\n",
        "√à il trucco classico: **quando `parse_dates` non basta** ‚Üí si fa la `apply`.\n",
        "\n",
        "---\n",
        "\n",
        "**Perch√© non abbiamo usato solo `pd.to_datetime(...)`?**\n",
        "\n",
        "Potevamo fare:\n",
        "```python\n",
        "df_ok[\"data\"] = pd.to_datetime(df_ok[\"data\"], dayfirst=True, errors=\"coerce\")\n",
        "```\n",
        "\n",
        "e in molti casi va bene.<br>\n",
        "Qui per√≤ avevamo formati sia italiani sia americani, sia con orario che no. `to_datetime` da solo a volte indovina, a volte no.<br>\n",
        "Con questa funzione di parsing, invece, decidiamo noi l‚Äôordine dei formati.<br>\n",
        "Esempio: prima prova italiano, poi americano ‚Üí cos√¨ non sbaglia 03/04/2025.\n",
        "\n",
        "---\n",
        "\n",
        "**Cosa succede alle righe ‚Äúsporche‚Äù?**\n",
        "- `\"\"` ‚Üí `NaT`\n",
        "- `\"non-data\"` ‚Üí nessun formato lo capisce ‚Üí `NaT`\n",
        "- `\"2025/02/01 14:30\"` ‚Üí lo becca al 4¬∞ formato ‚Üí diventa una vera `datetime`\n",
        "- `\"12/31/2024\"` ‚Üí lo becca al 3¬∞ formato ‚Üí ok\n",
        "- `\"31/12/2024\"` ‚Üí lo becca al 1¬∞ formato ‚Üí ok\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "print(df_ok.dtypes)\n",
        "```\n",
        "\n",
        "data           datetime64[ns]<br>\n",
        "descrizione            object<br>\n",
        "importo               float64<br>\n",
        "dtype: object<br>\n",
        "\n",
        "E' questo il risultato che volevamo fin dall‚Äôinizio: la colonna non √® pi√π `object`, √® una colonna di date üí™\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a007798",
      "metadata": {
        "id": "5a007798"
      },
      "source": [
        "**RIASSUNTO FINALE del punto 8**\n",
        "\n",
        "Se le date sono **tutte nello stesso formato** ‚Üí<br>\n",
        "`pd.read_csv(..., parse_dates=[\"data\"])` va benissimo.\n",
        "\n",
        "Se le date hanno **formati diversi ma ‚Äúsimili‚Äù** (tutte europee, o tutte ISO) ‚Üí<br>\n",
        "si pu√≤ leggere normalmente e poi fare:<br>\n",
        "    ```python\n",
        "    df[\"data\"] = pd.to_datetime(df[\"data\"], dayfirst=True, errors=\"coerce\")\n",
        "    ```\n",
        "\n",
        "spesso basta.\n",
        "\n",
        "Se le date sono proprio **eterogenee** (eu, usa, con ora, vuote, testo) ‚Üí<br>\n",
        "`parse_dates` da solo non basta, e a volte nemmeno `to_datetime(...)` indovinato;<br>\n",
        "allora conviene una **funzione di parsing flessibile** che provi pi√π formati.\n",
        "\n",
        "**Regola pratica:**<br>\n",
        "1 formato ‚Üí `parse_dates`<br>\n",
        "pochi formati ‚Üí `to_datetime`<br>\n",
        "formati misti/sporchi ‚Üí **funzione custom** ‚úÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c6f29c-cd33-4d11-b3e1-0b6c97d9b118",
      "metadata": {
        "id": "f2c6f29c-cd33-4d11-b3e1-0b6c97d9b118"
      },
      "source": [
        "‚¨ú **9. Duplicati o whitespace nei nomi colonna**\n",
        "\n",
        "<u>Problema</u>: nomi con spazi o duplicati (`'Nome '` ‚â† `'Nome'`).<br>\n",
        "<u>Soluzione</u>:\n",
        "```python\n",
        "df.columns = df.columns.str.strip()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "329743cc",
      "metadata": {
        "id": "329743cc",
        "outputId": "8d8b1f71-50f2-4df7-87cc-2a8bf1557613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ creato con_spazi.csv\n"
          ]
        }
      ],
      "source": [
        "# 1) creo un CSV con nomi colonna con spazi\n",
        "csv_text = \"\"\"  data  ;  nome cliente ; importo ;  note\n",
        "01/02/2025;Mario Rossi;120.50;pagato\n",
        "02/02/2025;  Anna Bianchi ;89.00;ritardo\n",
        "03/02/2025;ACME S.p.A.;250.00;\n",
        "\"\"\"\n",
        "\n",
        "file_name = \"con_spazi.csv\"\n",
        "with open(file_name, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    f.write(csv_text)\n",
        "\n",
        "print(f\"‚úÖ creato {file_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d66f6fd",
      "metadata": {
        "id": "6d66f6fd",
        "outputId": "e95a748b-15c5-460f-f5e6-152afa9ecf47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîé colonne lette (sporche):\n",
            "['  data  ', '  nome cliente ', ' importo ', '  note ']\n"
          ]
        }
      ],
      "source": [
        "# 2) lettura \"normale\" ‚Üí i nomi sono sporchi\n",
        "df = pd.read_csv(file_name, sep=\";\")\n",
        "print(\"üîé colonne lette (sporche):\")\n",
        "print(repr(df.columns.tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d47684",
      "metadata": {
        "id": "23d47684",
        "outputId": "40b44fd3-68db-4f07-d848-600cbc825c9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ colonne dopo strip():\n",
            "['data', 'nome cliente', 'importo', 'note']\n",
            "\n",
            "üìÑ dataframe finale:\n",
            "         data     nome cliente  importo     note\n",
            "0  01/02/2025      Mario Rossi    120.5   pagato\n",
            "1  02/02/2025    Anna Bianchi      89.0  ritardo\n",
            "2  03/02/2025      ACME S.p.A.    250.0      NaN\n"
          ]
        }
      ],
      "source": [
        "# 3) pulizia nomi colonna\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "print(\"\\n‚úÖ colonne dopo strip():\")\n",
        "print(repr(df.columns.tolist()))\n",
        "\n",
        "print(\"\\nüìÑ dataframe finale:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d89d151",
      "metadata": {
        "id": "9d89d151"
      },
      "source": [
        "**Cosa √® successo?**\n",
        "- prima di `strip()` le colonne sono tipo:<br>\n",
        "[*'  data  ', '  nome cliente ', ' importo ', '  note ']*<br>\n",
        "- dopo:<br>\n",
        "*['data', 'nome cliente', 'importo', 'note']*<br>\n",
        "\n",
        "Quindi se si vuole fare:\n",
        "    ```python\n",
        "    df[\"data\"]\n",
        "    ```\n",
        "ora funziona, mentre prima si sarebbe dovuto scrivere `df[\" data \"]` e non √® bello üòÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dfd8c9",
      "metadata": {
        "id": "42dfd8c9"
      },
      "source": [
        "üß± **10. Quote e caratteri speciali**\n",
        "\n",
        "<u>Problema</u>: CSV con virgolette interne, doppie virgolette, ecc.<br>\n",
        "<u>Soluzione</u>:\n",
        "```python\n",
        "pd.read_csv(\"file.csv\", quotechar='\"', escapechar='\\\\')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa07a51",
      "metadata": {
        "id": "3fa07a51"
      },
      "source": [
        "Vediamo il solito flusso (come per gli errori precedenti), in questo caso con 4 passi:\n",
        "- creiamo un CSV **volutamente sporco, ma non abbastanza --> riesce a leggerlo\n",
        "(con virgolette dentro, virgolette raddoppiate, backslash‚Ä¶)**\n",
        "- proviamo la **lettura ingenua** ‚Üí si incarta / va in errore / spezza le colonne\n",
        "- facciamo la **lettura robusta** ‚Üí `quotechar='\"'`, `escapechar='\\\\'`, e volendo anche `engine=\"python\"` per stare larghi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc81c93a",
      "metadata": {
        "id": "cc81c93a",
        "outputId": "134acd68-fe56-4566-b867-1f09435be9d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Scritto csv_sporco_ok.csv\n",
            "id,descrizione,note\n",
            "1,\"Martello, 500g\",\"tutto ok\"\n",
            "2,\"Cacciavite \\\"piatto\\\"\",\"virgolette con backslash (\\\")\"\n",
            "3,\"Set \"\"professionale\"\" 24 pz\",\"virgolette raddoppiate nel campo descrizione\"\n",
            "4,\"C:\\\\attrezzi\\\\nuovo\",\"percorso Windows con backslash\"\n",
            "\n",
            "\n",
            "=== CASO A - LETTURA INGENUA (funziona) ===\n",
            "   id                descrizione                                          note\n",
            "0   1             Martello, 500g                                      tutto ok\n",
            "1   2      Cacciavite \\piatto\\\"\"                 virgolette con backslash (\\)\"\n",
            "2   3  Set \"professionale\" 24 pz  virgolette raddoppiate nel campo descrizione\n",
            "3   4        C:\\\\attrezzi\\\\nuovo                percorso Windows con backslash\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# =========================================================\n",
        "# CASO A - CSV \"sporco ma leggibile\"\n",
        "# ---------------------------------------------------------\n",
        "# Qui vogliamo mostrare che: anche se ci sono virgolette interne\n",
        "# e backslash, la lettura ingenua *potrebbe* funzionare comunque.\n",
        "# =========================================================\n",
        "\n",
        "file_ok = \"csv_sporco_ok.csv\"\n",
        "\n",
        "csv_ok = (\n",
        "    # riga 1\n",
        "    'id,descrizione,note\\n'\n",
        "    # riga 2\n",
        "    '1,\"Martello, 500g\",\"tutto ok\"\\n'\n",
        "    # riga 3 - qui c'√® il backslash + virgolette: \\\"piatto\\\"\n",
        "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",\"virgolette con backslash (\\\\\")\"\\n'\n",
        "    # riga 4 - virgolette raddoppiate stile CSV\n",
        "    '3,\"Set \"\"professionale\"\" 24 pz\",\"virgolette raddoppiate nel campo descrizione\"\\n'\n",
        "    # riga 5 - percorso Windows\n",
        "    '4,\"C:\\\\\\\\attrezzi\\\\\\\\nuovo\",\"percorso Windows con backslash\"\\n'\n",
        ")\n",
        "\n",
        "Path(file_ok).write_text(csv_ok, encoding=\"utf-8\")\n",
        "print(f\"‚úÖ Scritto {file_ok}\")\n",
        "print(csv_ok)\n",
        "\n",
        "print(\"\\n=== CASO A - LETTURA INGENUA (funziona) ===\")\n",
        "# üëâ QUI *NON* mettiamo n√© quotechar n√© escapechar\n",
        "df_ok_naive = pd.read_csv(file_ok)\n",
        "print(df_ok_naive)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f513c7b",
      "metadata": {
        "id": "0f513c7b"
      },
      "source": [
        "Come si vede:\n",
        "- la lettura ingenua del file `csv_sporco_ok.csv` **ha funzionato** (anche senza `quotechar` + `escapechar` )\n",
        "- cio√®, la lettura ‚Äúnon sempre d√† errore, ma √® meglio specificare‚Äù con `quotechar` + `escapechar`\n",
        "\n",
        "Ovviamente, a fortiori la lettura robusta funziona anch'essa, come si vede dalla cella seguente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "514d002b",
      "metadata": {
        "id": "514d002b",
        "outputId": "001bd2ca-5664-4ce4-8e90-5cf975e021e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== CASO A - LETTURA ROBUSTA ===\n",
            "   id                descrizione                                          note\n",
            "0   1             Martello, 500g                                      tutto ok\n",
            "1   2        Cacciavite \"piatto\"                  virgolette con backslash (\")\n",
            "2   3  Set \"professionale\" 24 pz  virgolette raddoppiate nel campo descrizione\n",
            "3   4          C:\\attrezzi\\nuovo                percorso Windows con backslash\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== CASO A - LETTURA ROBUSTA ===\")\n",
        "df_ok_safe = pd.read_csv(\n",
        "    file_ok,\n",
        "    quotechar='\"',\n",
        "    escapechar='\\\\',\n",
        "    engine=\"python\",\n",
        ")\n",
        "print(df_ok_safe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1eee3d8",
      "metadata": {
        "id": "d1eee3d8"
      },
      "source": [
        "Creaimo ora, invece, un file CSV \"rotto\" in altro modo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c72305",
      "metadata": {
        "id": "89c72305",
        "outputId": "08d5d9b7-ce82-40ae-dff9-0f11c028333f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Scritto csv_sporco_rotto.csv\n",
            "id,descrizione,prezzo\n",
            "1,\"Martello\",12.5\n",
            "2,\"Cacciavite \\\"piatto\\\"\",8.9\n",
            "3,\"Set \"\"professionale\"\" 24 pz\",49.0\n",
            "4,\"Pinza con \"virgolette\" dentro, con virgola\",15.0\n",
            "\n",
            "\n",
            "=== CASO B - LETTURA INGENUA (DEVE FALLIRE) ===\n",
            "‚ùå Lettura ingenua fallita: Error tokenizing data. C error: Expected 3 fields in line 5, saw 4\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =========================================================\n",
        "# CASO B - CSV ROTTO APPOSTA (virgolette non bilanciate)\n",
        "# ---------------------------------------------------------\n",
        "# Qui vogliamo mostrare il caso nel quale la lettura\n",
        "# ingenua NON funziona.\n",
        "# L'idea √® mettere una riga con: \"Pinza con \"virgolette\" dentro\"\n",
        "# ma SENZA escape e con una virgola dentro ‚Üí il parser C salta.\n",
        "# =========================================================\n",
        "\n",
        "file_bad = \"csv_sporco_rotto.csv\"\n",
        "\n",
        "csv_bad = (\n",
        "    'id,descrizione,prezzo\\n'                    # riga 1 (header)\n",
        "    '1,\"Martello\",12.5\\n'                        # riga 2 ok\n",
        "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",8.9\\n'          # riga 3 ok (ha \\\")\n",
        "    '3,\"Set \"\"professionale\"\" 24 pz\",49.0\\n'     # riga 4 ok (ha \"\")\n",
        "    # riga 5 - QUESTA ROMPE:\n",
        "    #   - campo quotato che contiene altre virgolette NON escape\n",
        "    #   - e contiene anche una virgola ‚Üí il parser pensa che inizi/finisca un altro campo\n",
        "    '4,\"Pinza con \"virgolette\" dentro, con virgola\",15.0\\n'\n",
        ")\n",
        "\n",
        "Path(file_bad).write_text(csv_bad, encoding=\"utf-8\")\n",
        "print(f\"\\n‚úÖ Scritto {file_bad}\")\n",
        "print(csv_bad)\n",
        "\n",
        "\n",
        "print(\"\\n=== CASO B - LETTURA INGENUA (DEVE FALLIRE) ===\")\n",
        "try:\n",
        "    df_bad_naive = pd.read_csv(file_bad)\n",
        "    print(df_bad_naive)\n",
        "except Exception as e:\n",
        "    # qui ti aspetti qualcosa tipo:\n",
        "    # ParserError: Error tokenizing data. C error: Expected 3 fields in line 5, saw 4\n",
        "    print(\"‚ùå Lettura ingenua fallita:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d74c684",
      "metadata": {
        "id": "1d74c684"
      },
      "source": [
        "Con i due argomenti invece la lettura robusta (con `quotechar='\"'` e `escapechar='\\\\'` **funziona**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a43170b",
      "metadata": {
        "id": "3a43170b",
        "outputId": "c157ab6e-6e67-4867-dfdc-d00580de075c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== CASO B - LETTURA ROBUSTA  ===\n",
            "   id                descrizione  prezzo\n",
            "0   1                   Martello    12.5\n",
            "1   2        Cacciavite \"piatto\"     8.9\n",
            "2   3  Set \"professionale\" 24 pz    49.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_17000\\3188488824.py:2: ParserWarning: Skipping line 5: ',' expected after '\"'\n",
            "\n",
            "  df_bad_safe = pd.read_csv(\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== CASO B - LETTURA ROBUSTA  ===\")\n",
        "df_bad_safe = pd.read_csv(\n",
        "    file_bad,\n",
        "    quotechar='\"',\n",
        "    escapechar='\\\\',\n",
        "    engine=\"python\",\n",
        "    on_bad_lines=\"warn\",   # oppure \"skip\" per saltare via le righe rotte\n",
        ")\n",
        "print(df_bad_safe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c73e41b",
      "metadata": {
        "id": "5c73e41b"
      },
      "source": [
        "Ha letto, ma ha ottenuto un **warning**!<br>\n",
        "Ci sta dicendo una cosa molto precisa: anche col parser ‚Äúpi√π tollerante‚Äù (engine=\"python\") e anche con `quotechar` / `escapechar`, la riga 5 √® proprio rotta a livello di CSV. Non √® solo ‚Äúdifficile‚Äù, √® sintatticamente sbagliata: ci sono virgolette aperte e non chiuse, e in pi√π dentro c‚Äô√® una virgola.\n",
        "\n",
        "Quindi: √® normale che la salti. L‚Äôabbiamo fatta cos√¨ apposta per mostrare il caso in cui ‚Äúneanche la lettura robusta la salva‚Äù e pandas dice ‚Äúok, la butto via e vado avanti‚Äù.\n",
        "\n",
        "Adesso ci sono **due modi** per evitare il warning (il salto riga):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f807e8d3",
      "metadata": {
        "id": "f807e8d3"
      },
      "source": [
        "<u>Primo modo</u>: stile CSV **standard** ‚Üí **raddoppiamo le virgolette**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac79cb36",
      "metadata": {
        "id": "ac79cb36",
        "outputId": "a1dae2af-00b8-4841-d4e6-7280f712d684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== LETTURA ROBUSTA (CSV VALIDO, virgolette raddoppiate) ===\n",
            "   id                                 descrizione  prezzo\n",
            "0   1                                    Martello    12.5\n",
            "1   2                         Cacciavite \"piatto\"     8.9\n",
            "2   3                   Set \"professionale\" 24 pz    49.0\n",
            "3   4  Pinza con \"virgolette\" dentro, con virgola    15.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "file_good = \"csv_sporco_riparato_doppie.csv\"\n",
        "\n",
        "csv_good = (\n",
        "    'id,descrizione,prezzo\\n'\n",
        "    '1,\"Martello\",12.5\\n'\n",
        "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",8.9\\n'\n",
        "    '3,\"Set \"\"professionale\"\" 24 pz\",49.0\\n'\n",
        "    # üëá qui √® corretto: le virgolette interne sono raddoppiate\n",
        "    '4,\"Pinza con \"\"virgolette\"\" dentro, con virgola\",15.0\\n'\n",
        ")\n",
        "\n",
        "Path(file_good).write_text(csv_good, encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\n=== LETTURA ROBUSTA (CSV VALIDO, virgolette raddoppiate) ===\")\n",
        "df_ok = pd.read_csv(\n",
        "    file_good,\n",
        "    quotechar='\"',\n",
        "    escapechar='\\\\',\n",
        "    engine=\"python\",\n",
        ")\n",
        "print(df_ok)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7b1eedd",
      "metadata": {
        "id": "f7b1eedd"
      },
      "source": [
        "Non c'√® pi√π il warning! Perch√©?<br> Perch√©:\n",
        "- il campo √® quotato `\"...\"`,\n",
        "- dentro ci sono virgolette ‚Üí le abbiamo riscritte come `\"\"`,\n",
        "- dentro c‚Äô√® anche la virgola ‚Üí ma siccome il campo √® quotato, la virgola √® ok."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b68cf89",
      "metadata": {
        "id": "8b68cf89"
      },
      "source": [
        "<u>Secondo modo</u>: stile **‚Äú`escapechar`‚Äù** ‚Üí usiamo il backslash dentro (se proprio vogliamo mostrare l‚Äôuso di `escapechar='\\\\'`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17297188",
      "metadata": {
        "id": "17297188",
        "outputId": "d5c09e41-1f45-4564-b711-b7cd675d93ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== LETTURA ROBUSTA (CSV VALIDO, escape con backslash) ===\n",
            "   id                                 descrizione  prezzo\n",
            "0   1                                    Martello    12.5\n",
            "1   2                         Cacciavite \"piatto\"     8.9\n",
            "2   3                   Set \"professionale\" 24 pz    49.0\n",
            "3   4  Pinza con \"virgolette\" dentro, con virgola    15.0\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "file_good2 = \"csv_sporco_riparato_escape.csv\"\n",
        "\n",
        "csv_good2 = (\n",
        "    'id,descrizione,prezzo\\n'\n",
        "    '1,\"Martello\",12.5\\n'\n",
        "    '2,\"Cacciavite \\\\\"piatto\\\\\"\",8.9\\n'\n",
        "    '3,\"Set \"\"professionale\"\" 24 pz\",49.0\\n'\n",
        "    # üëá qui ESCAPE TUTTE le virgolette interne\n",
        "    '4,\"Pinza con \\\\\"virgolette\\\\\" dentro, con virgola\",15.0\\n'\n",
        ")\n",
        "\n",
        "Path(file_good2).write_text(csv_good2, encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\n=== LETTURA ROBUSTA (CSV VALIDO, escape con backslash) ===\")\n",
        "df_ok2 = pd.read_csv(\n",
        "    file_good2,\n",
        "    quotechar='\"',\n",
        "    escapechar='\\\\',   # üëà adesso serve davvero\n",
        "    engine=\"python\",\n",
        ")\n",
        "print(df_ok2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "249bd460",
      "metadata": {
        "id": "249bd460"
      },
      "source": [
        "Come si vede, di nuovo: niente warning ‚úÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55ea55dd",
      "metadata": {
        "id": "55ea55dd"
      },
      "source": [
        "**RIASSUNTO del punto 10**:\n",
        "- ‚Äúlettura ingenua‚Äù: `pd.read_csv(\"file.csv\")` ‚Üí se il CSV √® formalmente giusto, legge; se √® un po‚Äô sporco, a volte legge; se √® proprio rotto, lancia `ParserError`.\n",
        "- ‚Äúlettura robusta‚Äù: `pd.read_csv(\"file.csv\", quotechar='\"', escapechar='\\\\', engine=\"python\", on_bad_lines=\"warn\")` ‚Üí **legge di pi√π, ma non pu√≤ inventare virgolette che non ci sono** ‚Üí e quindi d√† il **warning** visto prima.\n",
        "- se NON vogliamo il warning ‚Üí le due modalit√† di lettura viste (oppure riscriviamo il CSV in modo valido (raddoppio \"\" oppure escape \\\"))."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d418a88b",
      "metadata": {
        "id": "d418a88b"
      },
      "source": [
        "**RIASSUNTO dei 10 casi**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a303a02e-92b9-43e7-8b9f-39df6858cc3f",
      "metadata": {
        "id": "a303a02e-92b9-43e7-8b9f-39df6858cc3f"
      },
      "source": [
        "![](problemi_tipici_read_csv.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a31fa67-aee3-4f8d-b74c-fdcd27295ea4",
      "metadata": {
        "id": "1a31fa67-aee3-4f8d-b74c-fdcd27295ea4"
      },
      "source": [
        "**Segue ora un secondo set di esempi** con **la correzione dati**:<br>\n",
        "\n",
        "1Ô∏è‚É£ creazione di un file CSV **‚Äúsporco‚Äù** con vari **errori e inconsistenze reali**;<br>\n",
        "2Ô∏è‚É£ il codice Python completo per leggerlo correttamente con `pandas.read_csv()`;<br>\n",
        "3Ô∏è‚É£ **<u>il codice Python per pulire il dataframe</u>** ‚ùó"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb74fea9-7ab2-4c45-816a-2e5c326fe786",
      "metadata": {
        "id": "cb74fea9-7ab2-4c45-816a-2e5c326fe786"
      },
      "source": [
        "1Ô∏è‚É£ Il file `dati_sporchi.csv`<br>\n",
        "üëâ Contiene:\n",
        "- delimitatore `;` invece di `,`\n",
        "- encoding misto (accenti e caratteri speciali)\n",
        "- separatori decimali confusi (`,`, `.`)\n",
        "- valori mancanti o `N/A`\n",
        "- riga con virgolette interne e una virgola nel nome\n",
        "- header con spazi\n",
        "- colonna `Unnamed: 0` inutile\n",
        "- righe con colonne disallineate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6285db6a-1d17-412c-900e-25bbad202383",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T12:04:25.200361Z",
          "iopub.status.busy": "2025-10-21T12:04:25.200077Z",
          "iopub.status.idle": "2025-10-21T12:04:25.204300Z",
          "shell.execute_reply": "2025-10-21T12:04:25.203949Z",
          "shell.execute_reply.started": "2025-10-21T12:04:25.200345Z"
        },
        "id": "6285db6a-1d17-412c-900e-25bbad202383",
        "outputId": "333cdf73-4d74-4d50-d338-46297f3a4f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ File 'dati_sporchi.csv' creato.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# 1. CREA IL FILE CSV \"SPORCO\"\n",
        "# =========================\n",
        "\n",
        "csv_content = \"\"\"# Dati di esempio esportati da sistema legacy\n",
        "# Contengono errori di formato, encoding e separatori\n",
        "ID; Nome ; Et√† ; Data_nascita ; Stipendio ; Note\n",
        "0; \"Mario Rossi\"; 35 ; 12/05/1989 ; \"2.500,50\" ; \"Lavora a Roma, ottimo rendimento\"\n",
        "1; \"Anna Bianchi\"; 29 ; 01/09/1995 ; \"3.200,00\" ; \"Milano, nuovi progetti\"\n",
        "2; \"Jos√© √Ålvarez\"; 40 ; 15/02/1984 ; \"4.000,75\" ; \"Problemi di encoding √†√®√¨√≤√π\"\n",
        "3; \"Luigi Verdi\"; \"?\" ; 03/11/1990 ; \"2,800.00\" ; \"Errore nei separatori decimali\"\n",
        "4; \"Giulia Rossi\" ; 27 ; 31-08-1997 ; \"3.000,00\" ; \"Riga OK\"\n",
        "5; \"Paolo Bianchi\" ; 33 ; 02/04/1991 ; \"N/A\" ; \"Valore mancante stipendio\"\n",
        "6; \"Marco, Test\"; 38 ; 07/07/1986 ; \"2.900,00\" ; \"Virgola nel nome\"\n",
        "7 \"Sara Neri\" ; 31 ; 10/10/1993 ; \"3.200,00\" ; Riga con separatore mancante\n",
        "8; \"Laura Verdi\"; 25 ; 21/06/1999 ; \"3.000,00\"\n",
        "9; \"Andrea Neri\" ; ; ; ; \"Campi mancanti\"\n",
        "Unnamed: 0; \"Extra colonna inutile\"; ; ; ;\n",
        "\"\"\"\n",
        "\n",
        "with open(\"dati_sporchi.csv\", \"w\", encoding=\"latin1\") as f:\n",
        "    f.write(csv_content)\n",
        "\n",
        "print(\"‚úÖ File 'dati_sporchi.csv' creato.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02b80af",
      "metadata": {
        "id": "b02b80af"
      },
      "source": [
        "2Ô∏è‚É£ La lettura robusta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f7bc6d9-36b7-443d-9fd2-1c8485f44be5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T12:04:31.824870Z",
          "iopub.status.busy": "2025-10-21T12:04:31.824652Z",
          "iopub.status.idle": "2025-10-21T12:04:31.831028Z",
          "shell.execute_reply": "2025-10-21T12:04:31.830405Z",
          "shell.execute_reply.started": "2025-10-21T12:04:31.824854Z"
        },
        "id": "8f7bc6d9-36b7-443d-9fd2-1c8485f44be5",
        "outputId": "a36cf5be-bcfc-40f1-fc26-17b5a400371f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colonne originali: ['ID', 'Nome ', 'Et√† ', 'Data_nascita ', 'Stipendio ', 'Note'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# 2. LETTURA ROBUSTA\n",
        "# =========================\n",
        "df = pd.read_csv(\n",
        "    \"dati_sporchi.csv\",\n",
        "    sep=\";\",                     # separatore europeo\n",
        "    comment=\"#\",                 # ignora righe di commento\n",
        "    engine=\"python\",             # parser pi√π flessibile\n",
        "    encoding=\"latin1\",           # gestisce accenti\n",
        "    on_bad_lines=\"skip\",         # salta righe errate\n",
        "    skip_blank_lines=True,       # ignora righe vuote\n",
        "    skipinitialspace=True        # rimuove spazi dopo ;\n",
        ")\n",
        "\n",
        "print(\"Colonne originali:\", df.columns.tolist(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b471617-f89e-476b-835a-271212a655cf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T12:04:39.705683Z",
          "iopub.status.busy": "2025-10-21T12:04:39.705409Z",
          "iopub.status.idle": "2025-10-21T12:04:39.713430Z",
          "shell.execute_reply": "2025-10-21T12:04:39.712972Z",
          "shell.execute_reply.started": "2025-10-21T12:04:39.705656Z"
        },
        "id": "0b471617-f89e-476b-835a-271212a655cf",
        "outputId": "0a4751d7-3952-4d59-f2fe-2fdeaea2fce6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Nome</th>\n",
              "      <th>Et√†</th>\n",
              "      <th>Data_nascita</th>\n",
              "      <th>Stipendio</th>\n",
              "      <th>Note</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>Laura Verdi</td>\n",
              "      <td>25.0</td>\n",
              "      <td>21/06/1999</td>\n",
              "      <td>3.000,00</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Unnamed: 0</td>\n",
              "      <td>Extra colonna inutile</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           ID                  Nome   Et√†  Data_nascita  Stipendio   Note\n",
              "0           8            Laura Verdi  25.0   21/06/1999    3.000,00   NaN\n",
              "1  Unnamed: 0  Extra colonna inutile   NaN           NaN        NaN   NaN"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "230a7a7c",
      "metadata": {
        "id": "230a7a7c"
      },
      "source": [
        "3Ô∏è‚É£  La pulizia del dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "118730db-3a3f-4d62-8bf5-f9b1e5ab942f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T12:04:48.491769Z",
          "iopub.status.busy": "2025-10-21T12:04:48.491555Z",
          "iopub.status.idle": "2025-10-21T12:04:48.496177Z",
          "shell.execute_reply": "2025-10-21T12:04:48.495747Z",
          "shell.execute_reply.started": "2025-10-21T12:04:48.491754Z"
        },
        "id": "118730db-3a3f-4d62-8bf5-f9b1e5ab942f"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3.1 PULIZIA NOMI COLONNE\n",
        "# =========================\n",
        "df.columns = df.columns.str.strip()                          # rimuove spazi\n",
        "df.columns = df.columns.str.replace(\"√É\", \"√†\", regex=False)   # corregge accenti errati\n",
        "df = df.loc[:, ~df.columns.str.contains(\"^Unnamed\", case=False)]  # rimuove colonne Unnamed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745a6018-7cb7-4435-9db7-46705faef766",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T12:04:56.004391Z",
          "iopub.status.busy": "2025-10-21T12:04:56.004064Z",
          "iopub.status.idle": "2025-10-21T12:04:56.011034Z",
          "shell.execute_reply": "2025-10-21T12:04:56.010686Z",
          "shell.execute_reply.started": "2025-10-21T12:04:56.004375Z"
        },
        "id": "745a6018-7cb7-4435-9db7-46705faef766",
        "outputId": "e83cfdd0-1c2b-428e-8ffa-d3ff5e42ade6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Nome</th>\n",
              "      <th>Et√†</th>\n",
              "      <th>Data_nascita</th>\n",
              "      <th>Stipendio</th>\n",
              "      <th>Note</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>Laura Verdi</td>\n",
              "      <td>25.0</td>\n",
              "      <td>21/06/1999</td>\n",
              "      <td>3.000,00</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Unnamed: 0</td>\n",
              "      <td>Extra colonna inutile</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           ID                   Nome   Et√† Data_nascita Stipendio  Note\n",
              "0           8            Laura Verdi  25.0  21/06/1999   3.000,00   NaN\n",
              "1  Unnamed: 0  Extra colonna inutile   NaN          NaN       NaN   NaN"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4789159d-f4a0-46c6-a439-ab4ce656d8c5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T12:05:45.040706Z",
          "iopub.status.busy": "2025-10-21T12:05:45.040481Z",
          "iopub.status.idle": "2025-10-21T12:05:45.055201Z",
          "shell.execute_reply": "2025-10-21T12:05:45.054688Z",
          "shell.execute_reply.started": "2025-10-21T12:05:45.040693Z"
        },
        "id": "4789159d-f4a0-46c6-a439-ab4ce656d8c5"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3.2 TRASFORMAZIONI TIPICHE\n",
        "# =========================\n",
        "\n",
        "# -- Colonna Et√†\n",
        "if \"Et√†\" in df.columns:\n",
        "    df[\"Et√†\"] = pd.to_numeric(df[\"Et√†\"], errors=\"coerce\")\n",
        "\n",
        "# -- Colonna Stipendio\n",
        "if \"Stipendio\" in df.columns:\n",
        "    df[\"Stipendio\"] = (\n",
        "        df[\"Stipendio\"]\n",
        "        .astype(str)\n",
        "        .str.replace(\".\", \"\", regex=False)  # rimuove i punti (migliaia)\n",
        "        .str.replace(\",\", \".\", regex=False) # converte virgola in punto\n",
        "    )\n",
        "    df[\"Stipendio\"] = pd.to_numeric(df[\"Stipendio\"], errors=\"coerce\")\n",
        "\n",
        "# -- Colonna Data_nascita\n",
        "if \"Data_nascita\" in df.columns:\n",
        "    df[\"Data_nascita\"] = pd.to_datetime(df[\"Data_nascita\"], dayfirst=True, errors=\"coerce\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4892219e-17e2-4df9-b29c-3fde757bac0c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T12:05:53.083670Z",
          "iopub.status.busy": "2025-10-21T12:05:53.083365Z",
          "iopub.status.idle": "2025-10-21T12:05:53.090487Z",
          "shell.execute_reply": "2025-10-21T12:05:53.090121Z",
          "shell.execute_reply.started": "2025-10-21T12:05:53.083652Z"
        },
        "id": "4892219e-17e2-4df9-b29c-3fde757bac0c",
        "outputId": "6fc40745-a405-4de2-9e43-61a02d2db86d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Nome</th>\n",
              "      <th>Et√†</th>\n",
              "      <th>Data_nascita</th>\n",
              "      <th>Stipendio</th>\n",
              "      <th>Note</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>Laura Verdi</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1999-06-21</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Unnamed: 0</td>\n",
              "      <td>Extra colonna inutile</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           ID                   Nome   Et√† Data_nascita  Stipendio  Note\n",
              "0           8            Laura Verdi  25.0   1999-06-21     3000.0   NaN\n",
              "1  Unnamed: 0  Extra colonna inutile   NaN          NaT        NaN   NaN"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf1ebe6-5367-40e4-8e63-bac6b1be4704",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T12:06:12.546286Z",
          "iopub.status.busy": "2025-10-21T12:06:12.546096Z",
          "iopub.status.idle": "2025-10-21T12:06:12.551472Z",
          "shell.execute_reply": "2025-10-21T12:06:12.551153Z",
          "shell.execute_reply.started": "2025-10-21T12:06:12.546272Z"
        },
        "id": "bcf1ebe6-5367-40e4-8e63-bac6b1be4704",
        "outputId": "564e153e-671b-4e2e-ed5b-da939fff589b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ File caricato e pulito correttamente!\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Nome</th>\n",
              "      <th>Et√†</th>\n",
              "      <th>Data_nascita</th>\n",
              "      <th>Stipendio</th>\n",
              "      <th>Note</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>Laura Verdi</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1999-06-21</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Unnamed: 0</td>\n",
              "      <td>Extra colonna inutile</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           ID                   Nome   Et√† Data_nascita  Stipendio  Note\n",
              "0           8            Laura Verdi  25.0   1999-06-21     3000.0   NaN\n",
              "1  Unnamed: 0  Extra colonna inutile   NaN          NaT        NaN   NaN"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tipi di dato:\n",
            " ID                      object\n",
            "Nome                    object\n",
            "Et√†                    float64\n",
            "Data_nascita    datetime64[ns]\n",
            "Stipendio              float64\n",
            "Note                   float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# 3.3 RISULTATO FINALE\n",
        "# =========================\n",
        "print(\"‚úÖ File caricato e pulito correttamente!\\n\")\n",
        "display(df)\n",
        "print(\"\\nTipi di dato:\\n\", df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a9a845",
      "metadata": {
        "id": "75a9a845"
      },
      "source": [
        "# Accelerare il caricamento in pandas di un file csv di grandi dimensioni\n",
        "\n",
        "Come accelerare il caricamento in pandas con read_csv di un file csv molto grande?<br>\n",
        "Ecco il **workflow consigliato**:\n",
        "```python\n",
        "    import pandas as pd\n",
        "\n",
        "    # Prima lettura, con ottimizzazioni di base\n",
        "    df_iter = pd.read_csv(\n",
        "        \"bigdata.csv\",\n",
        "        usecols=[\"A\", \"B\", \"C\"],\n",
        "        dtype={\"A\": \"int32\", \"B\": \"float32\"},\n",
        "        chunksize=1_000_000,\n",
        "        engine=\"pyarrow\"\n",
        "    )\n",
        "\n",
        "    # Elaborazione incrementale\n",
        "    df = pd.concat(df_iter)\n",
        "\n",
        "    # Salva in formato ottimizzato\n",
        "    df.to_parquet(\"bigdata.parquet\")\n",
        "```\n",
        "üëâ Le letture successive da Parquet o Feather saranno **fino a 50√ó pi√π rapide**.\n",
        "\n",
        "**Piccoli trucchi pratici**\n",
        "- pre-carica in RAM i file (es. `cat file.csv > /dev/null` su Linux) se il collo di bottiglia √® il disco.\n",
        "- se lavori spesso con gli stessi dati ‚Üí converti subito a Parquet.\n",
        "- se il file √® remoto ‚Üí usa `storage_options` (es. S3 o GDrive) per lettura diretta.\n",
        "- se non serve l‚Äôindice ‚Üí `index_col=False` o `index_col=None`.\n",
        "- per misurare l‚Äôeffetto: usa `%%time` in Jupyter o VSC oppure `timeit`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "085db616",
      "metadata": {
        "id": "085db616"
      },
      "source": [
        "**Ecco una guida pratica per velocizzare la lettura** üëá\n",
        "\n",
        "Come velocizzare pandas.read_csv() su file grandi\n",
        "\n",
        "1Ô∏è‚É£ **Specifica i tipi di dato (dtype)**<br>\n",
        "Pandas, se non li dichiari, deve ‚Äúindovinare‚Äù i tipi scorrendo le righe ‚Üí lento e dispendioso in memoria.\n",
        "```python\n",
        "    dtypes = {\n",
        "        \"id\": \"int32\",\n",
        "        \"categoria\": \"category\",\n",
        "        \"prezzo\": \"float32\",\n",
        "        \"quantita\": \"int16\"\n",
        "}\n",
        "df = pd.read_csv(\"file.csv\", dtype=dtypes)\n",
        "```\n",
        "\n",
        "‚úÖ Vantaggi: caricamento molto pi√π veloce e dataframe pi√π leggero.\n",
        "\n",
        "2Ô∏è‚É£ **Leggi solo alcune colonne**<br>\n",
        "Se non ti servono tutte, dichiara usecols:\n",
        "```python\n",
        "    df = pd.read_csv(\"file.csv\", usecols=[\"id\", \"prezzo\", \"quantita\"])\n",
        "```\n",
        "\n",
        "‚úÖ Risparmi tempo e memoria.\n",
        "\n",
        "3Ô∏è‚É£ **Disattiva ci√≤ che non serve**\n",
        "\n",
        "Niente indice:\n",
        "```python\n",
        "    index_col=False\n",
        "```\n",
        "\n",
        "Niente analisi di numeri mancanti complessa:\n",
        "```python\n",
        "    keep_default_na=False\n",
        "na_values=[\"\"]\n",
        "```\n",
        "Niente conversione automatica di date:\n",
        "```python\n",
        "    parse_dates=False\n",
        "```\n",
        "\n",
        "‚úÖ Tutto ci√≤ evita inferenze costose.\n",
        "\n",
        "4Ô∏è‚É£ **Usa un chunking (lettura a blocchi)**\n",
        "\n",
        "Se il file √® troppo grande per la RAM, leggilo a pezzi:\n",
        "```python\n",
        "    chunks = pd.read_csv(\"file.csv\", chunksize=1_000_000)\n",
        "    for chunk in chunks:\n",
        "        # elabora il chunk\n",
        "        process(chunk)\n",
        "```\n",
        "\n",
        "‚úÖ Mantieni basso il consumo di memoria e puoi elaborare in streaming.\n",
        "\n",
        "5Ô∏è‚É£ **Specifica l‚Äôengine**\n",
        "\n",
        "pandas pu√≤ usare due engine:\n",
        "- `engine='c'` (default, scritto in C) ‚Üí pi√π veloce\n",
        "- `engine='python'` ‚Üí pi√π flessibile ma pi√π lento\n",
        "\n",
        "Assicurati di usare:\n",
        "```python\n",
        "    pd.read_csv(\"file.csv\", engine=\"c\")\n",
        "```\n",
        "\n",
        "6Ô∏è‚É£ **‚ÄúScalda‚Äù la cache del disco**\n",
        "\n",
        "Su Linux:\n",
        "\n",
        "cat file.csv > /dev/null\n",
        "\n",
        "‚Üí cos√¨ il file √® gi√† in cache RAM e il successivo read_csv sar√† pi√π rapido.\n",
        "(non migliora la prima lettura, ma le successive s√¨)\n",
        "\n",
        "7Ô∏è‚É£ **Converti in Parquet appena puoi**\n",
        "\n",
        "CSV ‚Üí Parquet una volta sola, poi lavora sempre in Parquet:\n",
        "```python\n",
        "    df = pd.read_csv(\"file.csv\")\n",
        "    df.to_parquet(\"file.parquet\")\n",
        "```\n",
        "\n",
        "e successivamente:\n",
        "```python\n",
        "    df = pd.read_parquet(\"file.parquet\")\n",
        "```\n",
        "\n",
        "‚úÖ Spesso 5‚Äì10√ó pi√π veloce in lettura e 3‚Äì4√ó meno spazio su disco.\n",
        "\n",
        "8Ô∏è‚É£ Alternativa: usa Dask o Polars\n",
        "\n",
        "Se il file √® enorme (decine di GB):\n",
        "```python\n",
        "    dask.dataframe.read_csv()\n",
        "```\n",
        " ‚Üí lettura parallela su pi√π core;\n",
        "```python\n",
        "    polars.read_csv()\n",
        "```\n",
        " ‚Üí motore Rust super veloce (anche 10√ó pi√π rapido di pandas).\n",
        "\n",
        "9Ô∏è‚É£ Misura sempre con %%time\n",
        "\n",
        "In Jupyter o VS Code:\n",
        "```python\n",
        "    %%time\n",
        "    df = pd.read_csv(\"file.csv\", dtype=dtypes, usecols=cols)\n",
        "```\n",
        "\n",
        "Confronta varie versioni e scegli la pi√π rapida nel tuo contesto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f510b8c8-a527-4e6b-bfbd-47b128111ca6",
      "metadata": {
        "id": "f510b8c8-a527-4e6b-bfbd-47b128111ca6"
      },
      "source": [
        "# Applicazione al file finanziario `FinancialIndicators`\n",
        "Il file *Credit_ISLR* √® molto piccolo. Usiamo il pi√π corposo file csv *FinancialIndicators.csv*:\n",
        "- circa 7000 righe\n",
        "- 73 colonne\n",
        "- circa 2.4 GB\n",
        "- separatore = ',' (file americano)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4e2971-36fe-4ace-a716-9e0a34b5118c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T19:06:43.677547Z",
          "iopub.status.busy": "2025-10-24T19:06:43.677309Z",
          "iopub.status.idle": "2025-10-24T19:06:43.739704Z",
          "shell.execute_reply": "2025-10-24T19:06:43.739205Z",
          "shell.execute_reply.started": "2025-10-24T19:06:43.677533Z"
        },
        "id": "ab4e2971-36fe-4ace-a716-9e0a34b5118c",
        "outputId": "450114b3-416f-4123-91b6-427fc1cd4f88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tempo totale di esecuzione:  0.04415559768676758\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Industry Name</th>\n",
              "      <th>SIC</th>\n",
              "      <th>Exchange</th>\n",
              "      <th>Country</th>\n",
              "      <th>Stock Price</th>\n",
              "      <th>% Chg in last year</th>\n",
              "      <th>Trading Volume</th>\n",
              "      <th># of shares outstanding</th>\n",
              "      <th>Market Cap</th>\n",
              "      <th>...</th>\n",
              "      <th>Trailing Net Income</th>\n",
              "      <th>Dividends</th>\n",
              "      <th>Intangible Assets/Total Assets</th>\n",
              "      <th>Fixed Assets/Total Assets</th>\n",
              "      <th>Market D/E</th>\n",
              "      <th>Market Debt to Capital</th>\n",
              "      <th>Book Debt to Capital</th>\n",
              "      <th>Dividend Yield</th>\n",
              "      <th>Insider Holdings</th>\n",
              "      <th>Institutional Holdings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@Road Inc</td>\n",
              "      <td>Telecom. Services</td>\n",
              "      <td>4810</td>\n",
              "      <td>NDQ</td>\n",
              "      <td>US</td>\n",
              "      <td>5.23</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>236397</td>\n",
              "      <td>54.8</td>\n",
              "      <td>319.60</td>\n",
              "      <td>...</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1-800 Contacts Inc</td>\n",
              "      <td>Medical Supplies</td>\n",
              "      <td>8060</td>\n",
              "      <td>NDQ</td>\n",
              "      <td>US</td>\n",
              "      <td>11.70</td>\n",
              "      <td>0.03</td>\n",
              "      <td>57921</td>\n",
              "      <td>13.3</td>\n",
              "      <td>151.90</td>\n",
              "      <td>...</td>\n",
              "      <td>3.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1-800-ATTORNEY Inc</td>\n",
              "      <td>Publishing</td>\n",
              "      <td>2700</td>\n",
              "      <td>NDQ</td>\n",
              "      <td>US</td>\n",
              "      <td>1.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1-800-FLOWERS.COM</td>\n",
              "      <td>Internet</td>\n",
              "      <td>7370</td>\n",
              "      <td>NDQ</td>\n",
              "      <td>US</td>\n",
              "      <td>6.42</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>197850</td>\n",
              "      <td>65.2</td>\n",
              "      <td>422.90</td>\n",
              "      <td>...</td>\n",
              "      <td>7.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1mage Software Inc</td>\n",
              "      <td>Computer Software/Svcs</td>\n",
              "      <td>3579</td>\n",
              "      <td>NDQ</td>\n",
              "      <td>US</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10200</td>\n",
              "      <td>3.3</td>\n",
              "      <td>0.03</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>12.12</td>\n",
              "      <td>0.92</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 73 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Company Name           Industry Name   SIC Exchange Country  \\\n",
              "0           @Road Inc       Telecom. Services  4810      NDQ      US   \n",
              "1  1-800 Contacts Inc        Medical Supplies  8060      NDQ      US   \n",
              "2  1-800-ATTORNEY Inc              Publishing  2700      NDQ      US   \n",
              "3   1-800-FLOWERS.COM                Internet  7370      NDQ      US   \n",
              "4  1mage Software Inc  Computer Software/Svcs  3579      NDQ      US   \n",
              "\n",
              "   Stock Price  % Chg in last year  Trading Volume  # of shares outstanding  \\\n",
              "0         5.23               -0.02          236397                     54.8   \n",
              "1        11.70                0.03           57921                     13.3   \n",
              "2         1.01                0.00            1438                      0.0   \n",
              "3         6.42               -0.01          197850                     65.2   \n",
              "4         0.01                0.00           10200                      3.3   \n",
              "\n",
              "   Market Cap  ...  Trailing Net Income  Dividends  \\\n",
              "0      319.60  ...                 27.0        0.0   \n",
              "1      151.90  ...                  3.3        0.0   \n",
              "2        0.00  ...                 -1.0        0.0   \n",
              "3      422.90  ...                  7.8        0.0   \n",
              "4        0.03  ...                 -0.8        0.0   \n",
              "\n",
              "   Intangible Assets/Total Assets  Fixed Assets/Total Assets  Market D/E  \\\n",
              "0                            0.00                       0.02        0.00   \n",
              "1                            0.48                       0.19        0.16   \n",
              "2                             NaN                        NaN         NaN   \n",
              "3                            0.31                       0.20        0.01   \n",
              "4                            0.00                       0.00       12.12   \n",
              "\n",
              "   Market Debt to Capital  Book Debt to Capital  Dividend Yield  \\\n",
              "0                    0.00                  0.00             0.0   \n",
              "1                    0.14                  0.29             0.0   \n",
              "2                     NaN                   NaN             0.0   \n",
              "3                    0.01                  0.03             0.0   \n",
              "4                    0.92                   NaN             0.0   \n",
              "\n",
              "   Insider Holdings  Institutional Holdings  \n",
              "0               NaN                    0.23  \n",
              "1               NaN                    0.39  \n",
              "2               NaN                    0.00  \n",
              "3              0.21                    0.88  \n",
              "4               NaN                    0.00  \n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "df_FI = pd.read_csv('FinancialIndicators.csv')\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print ('Tempo totale di esecuzione: ', end_time - start_time)\n",
        "\n",
        "df_FI.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21d497ca-5f19-479e-97b0-b3317485e133",
      "metadata": {
        "id": "21d497ca-5f19-479e-97b0-b3317485e133"
      },
      "source": [
        " Applichiamo gli argomenti sopra elencati al caricamento di questo file."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fed47c7b-75b6-4196-9eee-6643b567c9f4",
      "metadata": {
        "id": "fed47c7b-75b6-4196-9eee-6643b567c9f4"
      },
      "source": [
        "# Le prestazioni"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425a0074-88bb-4905-bd5d-e3cc60161f54",
      "metadata": {
        "id": "425a0074-88bb-4905-bd5d-e3cc60161f54"
      },
      "source": [
        "Per quanto riguarda le **prestazioni dei vari formati** (come occupazione in memoria, salvtaggio su disco e apertura /lettura) vedi il seguente utile studio.\n",
        "\n",
        "Il messaggio chiave dello studio √® che:\n",
        "- il formato CSV √® molto meglio di excel (neanche preso in considerazione nella comparazione), √® disponibile in tutti gli ambienti di *data management*\n",
        "- per big data (come vedremo) il formato migliore √® il parquet, soprattutto nella occupazione di memoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d609bbdc-af9b-474c-81fb-ebaae9a984d6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T19:21:22.342679Z",
          "iopub.status.busy": "2025-10-24T19:21:22.342407Z",
          "iopub.status.idle": "2025-10-24T19:21:22.346551Z",
          "shell.execute_reply": "2025-10-24T19:21:22.346201Z",
          "shell.execute_reply.started": "2025-10-24T19:21:22.342665Z"
        },
        "id": "d609bbdc-af9b-474c-81fb-ebaae9a984d6",
        "outputId": "13a8c84f-15e8-4964-da92-b2833f299446"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"1000\"\n",
              "            height=\"600\"\n",
              "            src=\"I_O Optimization in Data Projects - by Avi Chawla.pdf\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x2028c82ae90>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Esempio d‚Äôuso:\n",
        "show_pdf(\"I_O Optimization in Data Projects - by Avi Chawla.pdf\")  # vedi ultimo capitolo per la lettura dei PDF in VSC."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89bf0e3-903b-44ee-ab4a-9259cb996949",
      "metadata": {
        "id": "e89bf0e3-903b-44ee-ab4a-9259cb996949"
      },
      "source": [
        "# Il formato dei dati per big data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d931f3-454c-48ce-8d18-e69d831b36a6",
      "metadata": {
        "id": "11d931f3-454c-48ce-8d18-e69d831b36a6"
      },
      "source": [
        "E' possibile caricare big data di 5M di righe in *pandas*? Dipende.\n",
        "\n",
        "La risposta breve √®: s√¨, pandas pu√≤ gestire anche 5 milioni di righe, ma dipende da cosa si intende per ‚Äúgestire‚Äù e da quanta RAM si ha a disposizione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b133258d-da10-4f09-b750-a16307861065",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:48:10.456975Z",
          "iopub.status.busy": "2025-10-24T18:48:10.456716Z",
          "iopub.status.idle": "2025-10-24T18:48:10.460041Z",
          "shell.execute_reply": "2025-10-24T18:48:10.459542Z",
          "shell.execute_reply.started": "2025-10-24T18:48:10.456959Z"
        },
        "id": "b133258d-da10-4f09-b750-a16307861065"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e9d90ee-e5ff-40ff-bc57-722befe5677c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:48:11.173601Z",
          "iopub.status.busy": "2025-10-24T18:48:11.173406Z",
          "iopub.status.idle": "2025-10-24T18:48:11.177957Z",
          "shell.execute_reply": "2025-10-24T18:48:11.177335Z",
          "shell.execute_reply.started": "2025-10-24T18:48:11.173587Z"
        },
        "id": "1e9d90ee-e5ff-40ff-bc57-722befe5677c"
      },
      "outputs": [],
      "source": [
        "# Il prefisso r dice a Python di non interpretare \\ come escape.\n",
        "path = r'C:\\Users\\Utente\\Desktop\\salvataggi\\SALVATAGGIO DATI\\Documents\\Seminari\\Data Science (corsi)\\Corso Python base\\linkage\\file_csv'\n",
        "\n",
        "all_files = glob.glob(os.path.join(path, \"*.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b531d132-3f6b-412d-befc-378ef601c974",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:48:18.240362Z",
          "iopub.status.busy": "2025-10-24T18:48:18.240149Z",
          "iopub.status.idle": "2025-10-24T18:48:21.341634Z",
          "shell.execute_reply": "2025-10-24T18:48:21.341067Z",
          "shell.execute_reply.started": "2025-10-24T18:48:18.240346Z"
        },
        "id": "b531d132-3f6b-412d-befc-378ef601c974"
      },
      "outputs": [],
      "source": [
        "li = []\n",
        "\n",
        "for filename in all_files:\n",
        "    df = pd.read_csv(filename, index_col=None, header=0)\n",
        "    li.append(df)\n",
        "\n",
        "frame = pd.concat(li, axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7eeada6-6166-4d45-a67a-cceb0c92f38d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:48:28.789133Z",
          "iopub.status.busy": "2025-10-24T18:48:28.788903Z",
          "iopub.status.idle": "2025-10-24T18:48:28.793928Z",
          "shell.execute_reply": "2025-10-24T18:48:28.793345Z",
          "shell.execute_reply.started": "2025-10-24T18:48:28.789116Z"
        },
        "id": "e7eeada6-6166-4d45-a67a-cceb0c92f38d",
        "outputId": "277c7c34-d5c1-4f0f-a659-d2e88d62e6cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5749132, 12)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "frame.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "995c22ea-e5cf-49e8-a777-4b1d213c12b1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:48:29.183826Z",
          "iopub.status.busy": "2025-10-24T18:48:29.183572Z",
          "iopub.status.idle": "2025-10-24T18:48:29.202116Z",
          "shell.execute_reply": "2025-10-24T18:48:29.201677Z",
          "shell.execute_reply.started": "2025-10-24T18:48:29.183810Z"
        },
        "id": "995c22ea-e5cf-49e8-a777-4b1d213c12b1",
        "outputId": "af1a84aa-0606-4ea6-f65f-db5acc1ea9c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_1</th>\n",
              "      <th>id_2</th>\n",
              "      <th>cmp_fname_c1</th>\n",
              "      <th>cmp_fname_c2</th>\n",
              "      <th>cmp_lname_c1</th>\n",
              "      <th>cmp_lname_c2</th>\n",
              "      <th>cmp_sex</th>\n",
              "      <th>cmp_bd</th>\n",
              "      <th>cmp_bm</th>\n",
              "      <th>cmp_by</th>\n",
              "      <th>cmp_plz</th>\n",
              "      <th>is_match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37291</td>\n",
              "      <td>53113</td>\n",
              "      <td>0.833333333333333</td>\n",
              "      <td>?</td>\n",
              "      <td>1.0</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39086</td>\n",
              "      <td>47614</td>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>1.0</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70031</td>\n",
              "      <td>70237</td>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>1.0</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84795</td>\n",
              "      <td>97439</td>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>1.0</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36950</td>\n",
              "      <td>42116</td>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id_1   id_2       cmp_fname_c1 cmp_fname_c2  cmp_lname_c1 cmp_lname_c2  \\\n",
              "0  37291  53113  0.833333333333333            ?           1.0            ?   \n",
              "1  39086  47614                  1            ?           1.0            ?   \n",
              "2  70031  70237                  1            ?           1.0            ?   \n",
              "3  84795  97439                  1            ?           1.0            ?   \n",
              "4  36950  42116                  1            ?           1.0            1   \n",
              "\n",
              "   cmp_sex cmp_bd cmp_bm cmp_by cmp_plz  is_match  \n",
              "0        1      1      1      1       0      True  \n",
              "1        1      1      1      1       1      True  \n",
              "2        1      1      1      1       1      True  \n",
              "3        1      1      1      1       1      True  \n",
              "4        1      1      1      1       1      True  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "frame.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59cd49ea-916b-4b5c-a3cb-16cd72aebf25",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:48:29.571961Z",
          "iopub.status.busy": "2025-10-24T18:48:29.571744Z",
          "iopub.status.idle": "2025-10-24T18:48:29.580914Z",
          "shell.execute_reply": "2025-10-24T18:48:29.580336Z",
          "shell.execute_reply.started": "2025-10-24T18:48:29.571945Z"
        },
        "id": "59cd49ea-916b-4b5c-a3cb-16cd72aebf25",
        "outputId": "523b34f3-f0fe-4173-ef10-1b336157609a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_1</th>\n",
              "      <th>id_2</th>\n",
              "      <th>cmp_fname_c1</th>\n",
              "      <th>cmp_fname_c2</th>\n",
              "      <th>cmp_lname_c1</th>\n",
              "      <th>cmp_lname_c2</th>\n",
              "      <th>cmp_sex</th>\n",
              "      <th>cmp_bd</th>\n",
              "      <th>cmp_bm</th>\n",
              "      <th>cmp_by</th>\n",
              "      <th>cmp_plz</th>\n",
              "      <th>is_match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5749127</th>\n",
              "      <td>47892</td>\n",
              "      <td>98941</td>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5749128</th>\n",
              "      <td>53346</td>\n",
              "      <td>74894</td>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5749129</th>\n",
              "      <td>18058</td>\n",
              "      <td>99971</td>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5749130</th>\n",
              "      <td>84934</td>\n",
              "      <td>95688</td>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5749131</th>\n",
              "      <td>20985</td>\n",
              "      <td>57829</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id_1   id_2 cmp_fname_c1 cmp_fname_c2  cmp_lname_c1 cmp_lname_c2  \\\n",
              "5749127  47892  98941            1            ?      0.166667            ?   \n",
              "5749128  53346  74894            1            ?      0.222222            ?   \n",
              "5749129  18058  99971            0            ?      1.000000            ?   \n",
              "5749130  84934  95688            1            ?      0.000000            ?   \n",
              "5749131  20985  57829            1            1      0.000000            ?   \n",
              "\n",
              "         cmp_sex cmp_bd cmp_bm cmp_by cmp_plz  is_match  \n",
              "5749127        1      0      0      1       0     False  \n",
              "5749128        1      0      0      1       0     False  \n",
              "5749129        1      0      0      0       0     False  \n",
              "5749130        1      0      1      0       0     False  \n",
              "5749131        1      0      1      1       0     False  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "frame.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74fdc05b-cd85-45cd-ba2e-2539460ca8ae",
      "metadata": {
        "id": "74fdc05b-cd85-45cd-ba2e-2539460ca8ae"
      },
      "source": [
        "---\n",
        "‚öôÔ∏è **1. Dipende dalla dimensione totale in memoria**\n",
        "\n",
        "Pandas lavora tutto in RAM.\n",
        "\n",
        "Esempio:\n",
        "- 5 milioni di righe √ó 50 colonne\n",
        "- ogni cella occupa ~8 byte (`float64`)<br>\n",
        "üëâ $5.000.000 √ó 50 √ó 8 ‚âà 2 GB$\n",
        "\n",
        "Quindi un file CSV da 200 MB pu√≤ diventare **2‚Äì3 GB in RAM** una volta caricato, per via della conversione in tipi numerici, indici, metadati ecc.\n",
        "\n",
        "Se si ha un PC con **16 GB di RAM**, va bene; se si hanno 8 GB, pandas ci riesce ma sar√† lento e potremmo vedere ‚Äúswap‚Äù o crash per mancanza di memoria.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a67f3b2d-f650-497d-a2d3-d473ba7ba953",
      "metadata": {
        "id": "a67f3b2d-f650-497d-a2d3-d473ba7ba953"
      },
      "source": [
        "üß† **2. Operazioni che pandas gestisce bene anche con 5M di righe**\n",
        "\n",
        "Con hardware \"decente\" (CPU moderna, 16 GB RAM) pandas gestisce tranquillamente:\n",
        "\n",
        "‚úÖ **Caricamento CSV**\n",
        "```python\n",
        "    df = pd.read_csv(\"dati.csv\")\n",
        "```\n",
        "\n",
        "Si pu√≤ anche usare:\n",
        "- `dtype=` per tipizzare meglio le colonne (meno RAM);\n",
        "- `usecols=` per leggere solo alcune colonne;\n",
        "- `chunksize=` per leggere a blocchi.\n",
        "\n",
        "‚úÖ **Operazioni elementari e aggregazioni**\n",
        "- `df.describe()`, `df.mean()`, `df.groupby(\"col\").agg(...)`\n",
        "- `df.sort_values(\"col\")`\n",
        "- `df.query(\"x > 10 and y < 5\")`\n",
        "- `df.sample(100_000)`<br>\n",
        "tutte fattibili.\n",
        "\n",
        "‚úÖ **Join e merge moderati**<br>\n",
        "Fino a qualche milione di righe per tabella:\n",
        "```python\n",
        "    pd.merge(df1, df2, on=\"id\", how=\"inner\")\n",
        "```\n",
        "funziona, ma attenzione ai picchi di memoria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d718ddf-6d75-48f4-86a2-8c2ed09d83d0",
      "metadata": {
        "id": "2d718ddf-6d75-48f4-86a2-8c2ed09d83d0"
      },
      "source": [
        "---\n",
        "üö´ **Operazioni che iniziano a diventare problematiche**\n",
        "\n",
        "Quando il dataset supera **i 5‚Äì10 milioni di righe o supera i 5 GB in RAM**, ecco cosa rallenta o esplode:\n",
        "\n",
        "‚ùå **ordinamenti multipli o sort complessi**\n",
        "```python\n",
        "    df.sort_values([\"col1\", \"col2\"])\n",
        "```\n",
        "Crea una copia in memoria grande quanto il DataFrame stesso.\n",
        "\n",
        "‚ùå **merge / join molto grandi**<br>\n",
        "se le due tabelle insieme superano la RAM disponibile.\n",
        "\n",
        "‚ùå **apply / lambda riga per riga**\n",
        "```python\n",
        "    df.apply(lambda row: f(row.x), axis=1)\n",
        "```\n",
        "\n",
        "Molto lente: infatti sono eseguite in Python puro, non in C.<br>\n",
        "Meglio usare funzioni **vectorized** (`np.where`, `pd.Series.map`, ecc.).\n",
        "\n",
        "‚ùå **operazioni iterative**<br>\n",
        "Cicli `for row in df.itertuples()` su milioni di righe ‚Üí un disastro!\n",
        "\n",
        "‚ùå **Scrittura su CSV/parquet**\n",
        "```python\n",
        "    df.to_csv(\"file.csv\")\n",
        "```\n",
        "\n",
        "Poco efficiente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cbedac-bee5-4376-9f2d-50b5df007d8f",
      "metadata": {
        "id": "a6cbedac-bee5-4376-9f2d-50b5df007d8f"
      },
      "source": [
        "---\n",
        "‚ö° **Alternative e strategie**\n",
        "\n",
        "**1. Usare il *chunking***\n",
        "\n",
        "Il seguente codice √® **rischioso**\n",
        "```python\n",
        "    import pandas as pd\n",
        "\n",
        "    df = pd.read_csv(\"dati.csv\")\n",
        "    df[\"media\"] = df[\"valore\"].mean()\n",
        "\n",
        "```\n",
        "\n",
        "Meglio leggere a blocchi e processare iterativamente:\n",
        "```python\n",
        "    import pandas as pd\n",
        "\n",
        "    chunksize = 500.000   # legge 500 mila righe per volta\n",
        "    risultati = []        # lista dove accumulare i risultati\n",
        "\n",
        "    for chunk in pd.read_csv(\"dati.csv\", chunksize=chunksize):\n",
        "        media_chunk = chunk[\"valore\"].mean()       # calcolo sulla parte letta\n",
        "        risultati.append(media_chunk)              # salvo il risultato parziale\n",
        "\n",
        "    # dopo il ciclo puoi combinare i risultati\n",
        "    media_totale = sum(risultati) / len(risultati)\n",
        "    print(\"Media complessiva:\", media_totale)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "**2. Usare il formato dati *parquet***<br>\n",
        "Vedi il prossimo capitolo.\n",
        "\n",
        "**3. Usare `cuDF`**<br>\n",
        "Utilizza la GPU senza modifiche al codice Pandas\n",
        "\n",
        "**4. Usare `Spark`**<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "660e3a2f-64c1-47f9-bc7c-7c9dbfc9bc41",
      "metadata": {
        "id": "660e3a2f-64c1-47f9-bc7c-7c9dbfc9bc41"
      },
      "source": [
        "# Un file CSV molto, molto grande\n",
        "Riusciamo a caricare un file CSV come [questo](https://www.kaggle.com/datasets/aadimator/nyc-realtime-traffic-speed-data/data)? √® quasi 30GB.<br>\n",
        "*Download* --> *Download dataset as zip (10 GBs)*.<br>\n",
        "Il suo nome √® **DOT_Traffic_Speeds_NBE.csv** ed √® relativo al traffico nella citt√† di NewYorl.\n",
        "\n",
        "Vediamone il significato:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a88614f-4843-4510-b051-561d2d866fff",
      "metadata": {
        "id": "1a88614f-4843-4510-b051-561d2d866fff"
      },
      "source": [
        "---\n",
        "That Kaggle dataset is an **export** of NYC DOT‚Äôs **real-time traffic speed feed** (‚ÄúDOT Traffic Speeds NBE‚Äù).\n",
        "\n",
        "Each row is a **timestamped observation for one road segment (a ‚Äúlink‚Äù)** with the average **speed** and **travel time** between the segment‚Äôs start and end points. It‚Äôs maintained by NYC DOT and mirrored to Kaggle. ([Kaggle][1])\n",
        "\n",
        "Here‚Äôs what the **fields mean** (names may appear in UPPER_CASE on Kaggle):\n",
        "\n",
        "* **ID / LINK_ID**\n",
        "  Unique identifier of the road **segment** (link) from TRANSCOM (regional traffic consortium). `LINK_ID` is the same as `ID`. Use this as **the key to group or join**. ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "* **SPEED**\n",
        "  **Average speed (mph)** vehicles traveled **across the whole segment** during the most recent interval. It‚Äôs not spot speed at a point‚Äîthink ‚Äúsegment travel speed.‚Äù Expect missing or zero values at times. ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "* **TRAVEL_TIME**\n",
        "  **Seconds** the average vehicle took to traverse the segment in that interval. Roughly `TRAVEL_TIME ‚âà segment_length / SPEED` (after converting units). Useful to derive segment length if you have a stable speed sample. ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "* **STATUS**\n",
        "  Marked as an **artifact / not useful** in NYC DOT‚Äôs own metadata. Most people ignore it. ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "* **DATA_AS_OF** (a.k.a. `DataAsOf`)\n",
        "  **Timestamp** when data for that link was last received. The feed updates **every few minutes**. Timezone is local (Eastern). Use this for time-series work and resampling. ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "* **LINK_POINTS**\n",
        "  **Plaintext sequence of lat/long pairs** describing the link geometry (start‚Üíend polyline). **Caveat:** some values are **truncated**‚Äîdon‚Äôt rely on this alone for precise mapping. ([Medium][3])\n",
        "\n",
        "* **ENCODED_POLY_LINE**\n",
        "  **Google-encoded polyline** version of the same geometry. This is usually the better field to decode for maps. (See Google‚Äôs polyline spec referenced by DOT.) ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "* **ENCODED_POLY_LINE_LVLS**\n",
        "  **Polyline ‚Äúlevels‚Äù** for Google‚Äôs legacy rendering (zoom levels). Often unused in modern tooling but included for completeness. ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "* **OWNER**\n",
        "  Owner of the detector producing this link‚Äôs data (administrative/operational). ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "* **TRANSCOM_ID / TRANSCOM_ID (artifact)**\n",
        "  Marked **not useful** by the publisher (redundant with ID). ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "* **BOROUGH**\n",
        "  NYC borough name (**Brooklyn, Bronx, Manhattan, Queens, Staten Island**). It can be blank for some links. Handy for rollups and filtering. ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "* **LINK_NAME / DESCRIPTION**\n",
        "  Human-readable description of the segment (e.g., ‚ÄúBQE N Atlantic Ave ‚Äî BKN Bridge Manhattan Side‚Äù). Note: **links are one-way**, and not every corridor has both directions in the feed. ([Medium][3])\n",
        "\n",
        "### How to interpret the dataset (what a ‚Äúrow‚Äù is)\n",
        "\n",
        "* One **segment (link)** √ó one **timestamp** ‚Üí **avg speed & travel time** for vehicles that **completed** that segment in the interval. It‚Äôs not per-vehicle data; it‚Äôs an **aggregate**. ([Medium][3])\n",
        "* The feed is **real-time / near-real-time**, updated several times per minute, and covers **major arterials & highways** in NYC. ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "### Practical notes / gotchas\n",
        "\n",
        "* **Geometry:** Prefer **`ENCODED_POLY_LINE`** over `LINK_POINTS`; the latter can be cut off. ([Medium][3])\n",
        "* **Aggregation grain:** Links are **directional**; do not assume two-way coverage for a corridor. ([Medium][3])\n",
        "* **Units:** SPEED = mph, TRAVEL_TIME = seconds; `BOROUGH` is a label, not a geometry. ([Sito Ufficiale di New York City][2])\n",
        "* **Quality:** Occasional zeros/missing values; treat **STATUS** as ignorable. ([Sito Ufficiale di New York City][2])\n",
        "\n",
        "### Typical uses\n",
        "\n",
        "* Compute **p50/p90 speeds** by `BOROUGH`/`LINK_ID`/hour; detect slowdowns and incidents.\n",
        "* Map segments by decoding **`ENCODED_POLY_LINE`**; join with borough boundaries for choropleths.\n",
        "* Derive **segment length** via `median(SPEED)*median(TRAVEL_TIME)` (unit-converted) if length isn‚Äôt separately available.\n",
        "\n",
        "[1]: https://www.kaggle.com/datasets/aadimator/nyc-realtime-traffic-speed-data?utm_source=chatgpt.com \"NYC Real-Time Traffic Speed Data\"\n",
        "[2]: https://www.nyc.gov/html/dot/downloads/pdf/metadata-trafficspeeds.pdf?utm_source=chatgpt.com \"Traffic Sensors Metadata What does this data set describe? ...\"\n",
        "[3]: https://medium.com/qri-io/new-qri-dataset-s-nyc-real-time-traffic-speeds-c3e4c88f44be \"New Qri Dataset(s): NYC Real-Time Traffic Speeds | by Chris Whong | qri.io | Medium\"\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0514b618",
      "metadata": {
        "id": "0514b618"
      },
      "source": [
        "**NOTE su questa dimensione (circa 30 GB)**\n",
        "\n",
        "28 GB ‚âà 28.000.000.000 byte ‚âà 26 GiB (se lo guardiamo in termini ‚Äúinformatici‚Äù).\n",
        "\n",
        "Un file CSV √® testuale, quindi √® poco denso: gli stessi dati in **Parquet** starebbero spesso in **3‚Äì6 GB**.\n",
        "\n",
        "In RAM questo file, <u>se letto con *pandas*</u>, **occupa ben pi√π spazio di 28 GB**: pandas infatti deve:\n",
        "- leggere il testo,\n",
        "- fare il parsing,\n",
        "- creare gli array interni.\n",
        "\n",
        "Risultato: 28 GB di CSV con la lettura *pandas* possono diventare **50‚Äì80 GB di RAM** senza sforzarsi troppo (dipende da quante colonne stringa ci sono, da quanti NaN, da quanto sono lunghe le etichette, ecc.).\n",
        "\n",
        "**√à frequente in azienda una simile dimensione?**\n",
        "- un singolo CSV da 28 GB non √® la norma nei gestionali/contabilit√†/HR. In questi sistemi troviamo pi√π facilmente **50‚Äì500 MB, massimo 2‚Äì3 GB** quando fanno l‚Äôexport ‚Äúdi tutto‚Äù.\n",
        "- √® per√≤ normalissimo in contesti tipo:\n",
        "    - log applicativi / web / sicurezza,\n",
        "    - telco,\n",
        "    - mobility / trasporti (tipo il tuo caso),\n",
        "    - IoT,\n",
        "    - data lake ‚Äúbuttato gi√π‚Äù da un sistema legacy.\n",
        "\n",
        "Ma‚Ä¶ quasi mai le aziende vogliono avere un unico CSV da 28 GB. Di solito √® un ‚Äúdumpone‚Äù fatto cos√¨ perch√© ‚Äúera l‚Äôopzione di export‚Äù, oppure perch√© qualcuno ha fatto SELECT * su 3 anni e l‚Äôha mandato su S3. In produzione seria si spezza per data o per partizione e si va con il Parquet.\n",
        "\n",
        "**Quindi: non √® strano avere 28 GB di dati. √à un po‚Äô strano averli tutti in un solo CSV.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vgs_xWascu9u",
      "metadata": {
        "id": "Vgs_xWascu9u"
      },
      "source": [
        "## Determinazione dell'ambiente di esecuzione.\n",
        "Il notebook funziona **indifferentemente** sia su Jupyter Notebook / Visual Studio Code che su Google Colab, come detto, <u>a parte due aspetti</u>:\n",
        "- il caricamento dei dataset nel notebook\n",
        "- l'inclusione delle immagini *png* nelle singole celle\n",
        "\n",
        "E' quindi utile **determinare l'ambiente di esecuzione**, impostando una variabile binaria (a `True` se siamo in Google Colab, a `False` se siamo in Jupyter Notebook).\n",
        "\n",
        "Le due operazioni suddette saranno eseguite in modo differente a seconda del valore della variabile binaria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fVONjPrKcxTl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVONjPrKcxTl",
        "outputId": "32789101-9cc6-49c1-9c71-de84f3de4982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on Colab: False\n"
          ]
        }
      ],
      "source": [
        "# impostazione del TOGGLE BINARIO:\n",
        "try:\n",
        "    import google.colab                      # package disponibile SOLO in Google Colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(\"Running on Colab:\", IN_COLAB)\n",
        "\n",
        "\n",
        "# IMPORT dei package necessari (necessari sia in JN che in Colab):\n",
        "from IPython.display import Image, display   # import dei package di incorporamento e visualizzazione immagine (una tantum)\n",
        "                                             # Image e display sono entrambi necessari a Jupyter Notebook\n",
        "                                             # Google Colab utilizza solo Image\n",
        "import os                                    # necessario a Google Colab per vedere da una cella codice\n",
        "                                             # i contenuti del 'content'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H_CJX_iEiFOW",
      "metadata": {
        "id": "H_CJX_iEiFOW"
      },
      "source": [
        "## Caricamento del big *csv* con Google Colab Pro"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XZ1iJB7li9I-",
      "metadata": {
        "id": "XZ1iJB7li9I-"
      },
      "source": [
        "Quanto spazio abbiamo nel *session storage* della VM? (con un run-time **L4-GPU** con 53GB di RAM, 22.5 GB di VRAM e 235.7 GB di disco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fD1FoVI-h98u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD1FoVI-h98u",
        "outputId": "834e970e-ce63-470e-f677-1d845ac790c8"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !df -h"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fKGa5X3zir9F",
      "metadata": {
        "id": "fKGa5X3zir9F"
      },
      "source": [
        "`overlay 236G 40G 197G 17% /`:  √® **la root** dell'ambiente Colab, cio√® quello che in Colab vediamo sotto `/content`.<br>\n",
        "Quello che interessa davvero √® **Avail = 197G**.<br>\n",
        "\n",
        "Tradotto: possiamo creare nuovi file fino a circa 197 GB (poi ovviamente dipende anche da quanto si usa per i notebook, i parquets, ecc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PgNseeIPkMW-",
      "metadata": {
        "id": "PgNseeIPkMW-"
      },
      "source": [
        "E le altre righe (`/dev/root`, `tmpfs`, `/dev/shm‚Ä¶`) dell'output precedente?\n",
        "Sono cose del container Colab.\n",
        "- `/dev/shm 26G` ‚Üí √® la shared memory (utile ad esempio per multiprocessing, ma non per salvare 28 GB).\n",
        "- `tmpfs 27G` ‚Üí memorie temporanee in RAM.\n",
        "- non sono il posto giusto per parcheggiare un CSV da 28 GB!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YbLH32-6kjUu",
      "metadata": {
        "id": "YbLH32-6kjUu"
      },
      "source": [
        "Abbiamo quindi tantissimo spazio locale nella VM: **circa 197 GB liberi** ‚Üí quindi s√¨, un file da 28 GB ci sta tranquillamente.\n",
        "\n",
        "Tuttavia l'upload di un file cos√¨ grande nela *session storage* di Google colab √® lento e a rischio di failure. Molto meglio mettere il file su Google Drive e poi **montare il disco** (autorizzando la connessione Google con i soliti passi):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "pnX91OR0fjFt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnX91OR0fjFt",
        "outputId": "83c3c029-62cd-4a6b-f65c-f66e404872a2"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)  # l'argomento 'force_remount = True' permette il mount multiplo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FC3d8Vedf0l_",
      "metadata": {
        "id": "FC3d8Vedf0l_"
      },
      "source": [
        "Cos√¨ il file resta su Drive, non lo dobbiamo ‚Äúcaricare‚Äù nella sessione, lo leggiamo da l√¨ (`/content/drive/MyDrive/.../big.csv`), Colab non deve tenere 28 GB sul disco locale.\n",
        "\n",
        "‚ö†Ô∏è Attenzione: leggere 28 GB da Drive √® pi√π lento che leggere da disco locale. Per un CSV enorme pu√≤ voler dire **minuti di I/O**.\n",
        "\n",
        "NB. `drive/MyDrive` √® ora **disponibile anche sotto la `content` del *session storage***.\n",
        "\n",
        "Possiamo infatti vederlo rieseguendo il comando `!df -h`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q3WskQszAurS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3WskQszAurS",
        "outputId": "e52fa047-e170-42d8-aff5-52b64d524aaa"
      },
      "outputs": [],
      "source": [
        " if IN_COLAB:\n",
        "    !df -h"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MCKpRa5IAdVw",
      "metadata": {
        "id": "MCKpRa5IAdVw"
      },
      "source": [
        "<u>Domanda</u>: ma ‚Äúoverlay‚Äù e ‚Äúdrive‚Äù hanno la stessa dimensione (236G) ü§î?\n",
        "\n",
        "S√¨, sembra cos√¨ perch√© Colab spesso **mostra lo stesso backing storage o comunque due volumi con taglia simile**. Quello che ci interessa √®: abbiamo ~200 GB liberi localmente e ~187 GB liberi su Drive ‚Üí entrambi > 28 GB ‚Üí siamo al sicuro."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XiFlPeUdgVJW",
      "metadata": {
        "id": "XiFlPeUdgVJW"
      },
      "source": [
        "Per prima cosa, come verifica, facciamo la **lista dei file sul drive**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "xIIb3zi6gRi2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIIb3zi6gRi2",
        "outputId": "819e67f8-10fd-4578-fe53-707e22267377"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    import os\n",
        "    base = \"/content/drive/MyDrive\"\n",
        "\n",
        "    for name in os.listdir(base):\n",
        "        print(name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zv35IUn_ghq1",
      "metadata": {
        "id": "zv35IUn_ghq1"
      },
      "source": [
        "Se vogliamo vedere anche **la dimensione** dei file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "NERI4Rj0gmlV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NERI4Rj0gmlV",
        "outputId": "c5b2a464-e7c0-42a6-c6da-e0ba1d5ee3cc"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    for name in os.listdir(base):\n",
        "        path = os.path.join(base, name)\n",
        "        if os.path.isfile(path):\n",
        "            print(\"FILE \", name, os.path.getsize(path))\n",
        "        else:\n",
        "            print(\"DIR  \", name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K3N3gcTyrW9F",
      "metadata": {
        "id": "K3N3gcTyrW9F"
      },
      "source": [
        "Verifichiamo la dimensione del file `DOT_Traffic_Speeds_NBE.csv`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "-oUOMAR5rI2-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oUOMAR5rI2-",
        "outputId": "3c264c3c-6a21-4beb-9a97-6bf245a93d8e"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !ls -lh /content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eOGXJx70ByxB",
      "metadata": {
        "id": "eOGXJx70ByxB"
      },
      "source": [
        "Ora siamo pronti a leggere il file csv con pandas (`pd.read_csv`) con **cuDF**.\n",
        "\n",
        "---\n",
        "\n",
        "**`cudf` √® una versione di pandas con accelerazione CUDA.**\n",
        "\n",
        "Vedi [questo post con video](https://www.linkedin.com/feed/update/urn:li:activity:7173982894921519105?utm_source=share&utm_medium=member_desktop) e [questo articolo](https://www.blog.dailydoseofds.com/p/nvidias-latest-update-can-make-your).\n",
        "\n",
        "*Steve Nouri*:<br>\n",
        "**NVIDIA made Pandas 50x faster with No code change!**<br>\n",
        "\n",
        "Occorre fare semplicemente questo:<br>\n",
        "```python\n",
        "    %load_ext cudf.pandas\n",
        "    import pandas as pd\n",
        "```\n",
        "\n",
        "Ora `cuDF` sar√† **integrato direttamente in Google Colab** (occore ovviamente avere un **run-time GPU abilitato**).\n",
        "\n",
        "Dai un'occhiata anche qui https://bit.ly/3XX9pgm.\n",
        "\n",
        "Vedi anche [questo ottimo video](https://www.youtube.com/watch?v=8X_IaCNpo7E) tradotto in italiano.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "qv0WrQY1DR1B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv0WrQY1DR1B",
        "outputId": "dc031cd0-1dba-42b9-9074-9c3cbbb9a58a"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    %load_ext cudf.pandas\n",
        "    import pandas as pd\n",
        "    import cudf\n",
        "else:\n",
        "    import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r5z8pQe6h2BF",
      "metadata": {
        "id": "r5z8pQe6h2BF"
      },
      "source": [
        "Il ciclo seguente di **lettura** impiega **610 secondi** su L4-GPU.<br>\n",
        "**L'idea generale**:\n",
        "- abbiamo un CSV enorme **a blocchi da 250k righe** con *pandas*,\n",
        "- lo leggiamo a pezzetti (`chunksize`), \n",
        "- ogni pezzetto lo spostiamo in GPU con cuDF,\n",
        "- qui dentro (in GPU) potremmo filtrare le righe oppure trasformare il file,\n",
        "- volendo lo salviamo subito oppure lo accumuliamo in una lista,\n",
        "- alla fine possiamo concatenare tutto in GPU (solo se ci sta in VRAM),\n",
        "- tutto il blocco di codice seguente √® temporizzato per sapere quanto ci mette."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "udWpoiyAdyyV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udWpoiyAdyyV",
        "outputId": "1b8a54a9-1976-4aef-f71c-e27939d42970"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 3. se si vogliono accumulare i chunk in GPU (solo se li possiamo tenere)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m gdf_parts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(CSV_PATH, chunksize\u001b[38;5;241m=\u001b[39mROWS_PER_CHUNK)):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[pandas] letto chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m con \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m righe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# 4. converte il chunk pandas -> cuDF (qui usiamo la GPU)\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "CHUNK = 200_000_000  # 200 MB per volta\n",
        "offset = 0\n",
        "part = 0\n",
        "\n",
        "import time\n",
        "\n",
        "# l'avviamento del timer\n",
        "start_time = time.time()\n",
        "\n",
        "# // il codice da misurare\n",
        "\n",
        "# 1. percorso del file\n",
        "CSV_PATH = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
        "\n",
        "# 2. dimensione del chunk (si parte basso)\n",
        "ROWS_PER_CHUNK = 250_000   # ~200-300 MB a seconda delle colonne\n",
        "\n",
        "# 3. se si vogliono accumulare i chunk in GPU (solo se li possiamo tenere)\n",
        "gdf_parts = []\n",
        "\n",
        "for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=ROWS_PER_CHUNK)):\n",
        "    print(f\"[pandas] letto chunk {i} con {len(chunk)} righe\")\n",
        "\n",
        "    # 4. converte il chunk pandas -> cuDF (qui usiamo la GPU)\n",
        "    gdf_chunk = cudf.from_pandas(chunk)\n",
        "    print(f\"[cuDF] chunk {i} in GPU con shape {gdf_chunk.shape}\")\n",
        "\n",
        "    # ‚¨áÔ∏è qui possiamo fare le nostre operazioni in GPU\n",
        "    # esempio: filtro\n",
        "    # gdf_chunk = gdf_chunk[gdf_chunk[\"BOROUGH\"] == \"MANHATTAN\"]\n",
        "\n",
        "    # esempio: salviamo subito in parquet per non tenere tutto in GPU\n",
        "    # gdf_chunk.to_parquet(f\"/content/out_part_{i:04d}.parquet\")\n",
        "\n",
        "    # se invece li teniamo per per unirli dopo:\n",
        "    gdf_parts.append(gdf_chunk)\n",
        "\n",
        "# 5. (opzionale) uniamo tutti i pezzi GPU in un unico DataFrame cuDF\n",
        "# ‚ö†Ô∏è facciamo solo se ci sta in VRAM\n",
        "if gdf_parts:\n",
        "    gdf_all = cudf.concat(gdf_parts, ignore_index=True)\n",
        "    print(gdf_all.shape)\n",
        "\n",
        "# // fine del codice da misurare\n",
        "\n",
        "# fine timer e stampa\n",
        "end_time = time.time()\n",
        "print ('Tempo totale di esecuzione: ', end_time - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da6ddab5",
      "metadata": {},
      "source": [
        "Vediamo riga per riga cosa fa il codice della cella precedente.\n",
        "\n",
        "**Timer**\n",
        "```python\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "```\n",
        "dove:\n",
        "- importiamo il package `time`\n",
        "- salviamo l‚Äôistante di partenza\n",
        "- alla fine salviamo l'istante di fine elaborazione per dire ‚Äútutto questo giro ha impiegato X secondi‚Äù.\n",
        "\n",
        "**Parametri di lettura**\n",
        "```python\n",
        "    CSV_PATH = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
        "    ROWS_PER_CHUNK = 250_000\n",
        "    gdf_parts = []\n",
        "```\n",
        "dove:\n",
        "- `CSV_PATH`: dov‚Äô√® il file\n",
        "- `ROWS_PER_CHUNK = 250_000`: invece di leggere 65 milioni di righe in un colpo solo, ne leggiamo **250k alla volta**. <u>√à il modo giusto quando il CSV √® enorme</u>.\n",
        "- `gdf_parts = []`: la lista vuota dove mettiamo i DataFrame cuDF man mano che li convertiamo.\n",
        "\n",
        "**Lettura a chunk con pandas**\n",
        "```python\n",
        "    for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=ROWS_PER_CHUNK)):\n",
        "        print(f\"[pandas] letto chunk {i} con {len(chunk)} righe\")\n",
        "```\n",
        "\n",
        "Qui succede una cosa importante:\n",
        "- non stiamo leggendo direttamente con cuDF.\n",
        "- stiamo usando *pandas* con l‚Äôopzione `chunksize=...` ‚Üí questo fa s√¨ che la funzione `read_csv` diventi un **generatore**: ogni giro del ciclo `for` restituisce **solo un pezzo del file**.\n",
        "- `i` √® l‚Äôindice del `chunk` (0, 1, 2, ‚Ä¶).\n",
        "- `chunk` √® un **DataFrame pandas** con ~250k righe.\n",
        "- stampiamo quante righe abbiamo  letto per vedere che stiamo avanzando.\n",
        "\n",
        "Perch√© facciamo cos√¨? Perch√© spesso pandas √® pi√π ‚Äútollerante‚Äù e la lettura a chunk √® gi√† pronta, e poi usiamo cuDF solo per l‚Äôelaborazione.\n",
        "\n",
        "> <u>Nota su `enumerate`</u><br>\n",
        "> `enumerate` √® una funzione di Python che prende qualcosa di **iterabile** (lista, generator, in questo caso i chunk del `read_csv`) e restituisce coppie:\n",
        "> - il numero del giro (0, 1, 2, 3‚Ä¶)\n",
        "> - l‚Äôelemento vero dell‚Äôiterazione<br>\n",
        "> \n",
        "> Cio√® con `enumerate(...)` stiamo dicendo: ‚Äúper ogni pezzo che ci dai, dacci anche l‚Äôindice del pezzo‚Äù.\n",
        "> - `i` ‚Üí 0 per il primo chunk, 1 per il secondo, 2 per il terzo‚Ä¶\n",
        "> - `chunk` ‚Üí **il DataFrame pandas con quelle 250.000 righe**<br>\n",
        "> \n",
        "> Cos√¨ possiamo stampare:\n",
        "> ```python\n",
        ">     print(f\"[pandas] letto chunk {i} ...\")\n",
        "> ```\n",
        "> e sappiamo a che punto siamo.\n",
        "\n",
        "**Conversione pandas ‚Üí cuDF**\n",
        "```python\n",
        "    gdf_chunk = cudf.from_pandas(chunk)\n",
        "    print(f\"[cuDF] chunk {i} in GPU con shape {gdf_chunk.shape}\")\n",
        "```\n",
        "- prendiamo il pezzo letto in RAM (pandas),\n",
        "- lo spostiamo in GPU convertendolo in un DataFrame cuDF.\n",
        "- stampiamo la `shape` (le dimensioni) giusto per controllo.\n",
        "\n",
        "Questo √® il punto in cui ‚Äúusiamo la GPU‚Äù.\n",
        "\n",
        "**Punto in cui fare le elaborazioni (EVENTUALI)**\n",
        "```python\n",
        "    gdf_chunk = gdf_chunk[gdf_chunk[\"BOROUGH\"] == \"MANHATTAN\"]\n",
        "    gdf_chunk.to_parquet(...)\n",
        "```\n",
        "\n",
        "√à il pattern ‚Äúleggo CSV grossi ‚Üí li trasformo a pezzi ‚Üí li salvo in formato pi√π comodo‚Äù.\n",
        "\n",
        "**Accumulo dei pezzi**\n",
        "```python\n",
        "    gdf_parts.append(gdf_chunk)\n",
        "```\n",
        "\n",
        "Invece di salvare subito, mettiamo il pezzo in una lista.<br>\n",
        "E' comodo se:\n",
        "- vogliamo fare una concatenazione alla fine,\n",
        "- oppure se i chunk sono pochi e stanno in memoria.\n",
        "\n",
        "√à rischioso se il CSV √® davvero enorme e la GPU ha poca VRAM.\n",
        "\n",
        "**Concatenazione finale (opzionale)**\n",
        "```python\n",
        "    if gdf_parts:\n",
        "        gdf_all = cudf.concat(gdf_parts, ignore_index=True)\n",
        "        print(gdf_all.shape)\n",
        "```\n",
        "\n",
        "Se abbiamo almeno un pezzo, li uniamo tutti in un unico DataFrame cuDF.\n",
        "```python\n",
        "    ignore_index=True perch√© dopo una concat i vecchi indici non hanno senso.\n",
        "```\n",
        "‚ö†Ô∏è giustamente abbiamo messo il commento: ‚Äúfallo solo se ci sta in VRAM‚Äù.<br>\n",
        "Questa √® la parte che spesso, sui dataset enormi, non si fa, e ci si ferma al salvataggio per chunk.\n",
        "\n",
        "**Fine timer**\n",
        "```python\n",
        "    end_time = time.time()\n",
        "    print ('Tempo totale di esecuzione: ', end_time - start_time)\n",
        "```\n",
        "Stampiamo quanto ci ha messo l‚Äôintero giro: lettura a chunk + conversione + eventuale concat."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H_M4fMa-FtsD",
      "metadata": {
        "id": "H_M4fMa-FtsD"
      },
      "source": [
        "La versione seguente ha un altro approccio: ‚Äúnon accumula, processa e poi butta‚Äù, √® ancora pi√π sicura (**770 secondi su L4-GPU**) üëá"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZdodFJq4F56x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdodFJq4F56x",
        "outputId": "a2f11623-f636-41a0-b7a5-91bb13606999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tempo totale di esecuzione:  770.3484830856323\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# l'avviamento del timer\n",
        "start_time = time.time()\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
        "ROWS_PER_CHUNK = 250_000\n",
        "\n",
        "# creazione della directory sul session storage\n",
        "out_dir = \"/content/parquet_parts\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=ROWS_PER_CHUNK)):\n",
        "    gdf = cudf.from_pandas(chunk)\n",
        "    # fai le tue operazioni qui...\n",
        "    # e poi NON lo tieni in memoria\n",
        "    gdf.to_parquet(f\"{out_dir}/part_{i:04d}.parquet\")\n",
        "\n",
        "# fine timer e stampa\n",
        "end_time = time.time()\n",
        "print ('Tempo totale di esecuzione: ', end_time - start_time)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kjFBw0SGpDle",
      "metadata": {
        "id": "kjFBw0SGpDle"
      },
      "source": [
        "`chunk` ‚Üí √® il **dataframe pandas** che arriva dal CSV.\n",
        "\n",
        "`gdf` ‚Üí √® il **dataframe cuDF** (quello ‚Äúvero‚Äù su cui si lavora in GPU).\n",
        "\n",
        "si vuole usare pandas? ‚Üí usiamo `chunk`<br>\n",
        "si vuole usare cuDF ‚Üí usiamo `gdf` (√® questo ‚Äúil dataframe‚Äù che ci interessa per la GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7G8P4O6ysyyN",
      "metadata": {
        "id": "7G8P4O6ysyyN"
      },
      "source": [
        "Vogliamo contare le righe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "PYptJ8a8soQ2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYptJ8a8soQ2",
        "outputId": "aec8b167-c6d5-483f-c993-12a205540005"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !wc -l /content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k3ckz0MWtNmu",
      "metadata": {
        "id": "k3ckz0MWtNmu"
      },
      "source": [
        "64.914.524 righe!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xr6vc01At3w2",
      "metadata": {
        "id": "Xr6vc01At3w2"
      },
      "source": [
        "Sapendo la dimensione in righe del file possiamo ora scrivere il codice di lettura del file pi√π efficace (con `chunk` = 1.000.000):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "No59puLbt_B-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No59puLbt_B-",
        "outputId": "23630120-d65a-4ff0-a90d-11bf3aaf3029"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'cudf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(path, chunksize\u001b[38;5;241m=\u001b[39mROWS_PER_CHUNK)):\n\u001b[1;32m---> 10\u001b[0m     gdf \u001b[38;5;241m=\u001b[39m cudf\u001b[38;5;241m.\u001b[39mfrom_pandas(chunk)\n\u001b[0;32m     11\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gdf)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(gdf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m righe, totale: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'cudf' is not defined"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    path = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
        "\n",
        "    ROWS_PER_CHUNK = 1_000_000   # 1 milione: ~65 giri\n",
        "\n",
        "    total = 0\n",
        "    for i, chunk in enumerate(pd.read_csv(path, chunksize=ROWS_PER_CHUNK)):\n",
        "        gdf = cudf.from_pandas(chunk)\n",
        "        total += len(gdf)\n",
        "        print(f\"chunk {i:03d} -> {len(gdf)} righe, totale: {total}\")\n",
        "\n",
        "    print(\"‚úÖ totale letto:\", total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1oyaldXrxaVG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oyaldXrxaVG",
        "outputId": "7028812d-a4d3-4f2f-f691-1cc80b725f90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(914523, 13)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunk.shape # l'ultimo chunk in memoria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cytbmIYoxfSe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cytbmIYoxfSe",
        "outputId": "117c8106-248e-469b-b2d6-6baa987e9359"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(914523, 13)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gdf.shape # l'ultimo "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O7-F9giOy1Si",
      "metadata": {
        "id": "O7-F9giOy1Si"
      },
      "source": [
        "Prima abbiamo letto a chunk.<br>\n",
        "Ora PROVIAMO a leggere tutto il file **in un unico dataframe in memoria** (in 7 minuti su L4-GPU). C'√® il rischio che **esploda la RAM**, ma qui ne abbiamo 53GB!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1BW_EkvBy7V-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "1BW_EkvBy7V-",
        "outputId": "47d2bbc1-dc34-452e-cec5-a67d9b35d40f"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(path, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000_000\u001b[39m):\n\u001b[0;32m      3\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
            "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    686\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
            "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:185\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m         values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 185\u001b[0m     values \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(values)\n\u001b[0;32m    187\u001b[0m     fastpath \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m values\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Utente\\anaconda3\\Lib\\site-packages\\pandas\\core\\construction.py:481\u001b[0m, in \u001b[0;36mensure_wrapped_if_datetimelike\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto_numpy()  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mensure_wrapped_if_datetimelike\u001b[39m(arr):\n\u001b[0;32m    482\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m    Wrap datetime64 and timedelta64 ndarrays in DatetimeArray/TimedeltaArray.\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray):\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    dfs = []\n",
        "    for chunk in pd.read_csv(path, chunksize=1_000_000):\n",
        "        dfs.append(chunk)\n",
        "\n",
        "    df = pd.concat(dfs, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n1xWoleF1GVu",
      "metadata": {
        "id": "n1xWoleF1GVu"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7zZHnjRk1Osu",
      "metadata": {
        "id": "7zZHnjRk1Osu"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-vOgqkll0i6H",
      "metadata": {
        "id": "-vOgqkll0i6H"
      },
      "source": [
        "Se volessimo invece rileggere il primo chunk faremmo cos√¨:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z5ip_NeZ1E2u",
      "metadata": {
        "id": "Z5ip_NeZ1E2u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = \"/content/drive/MyDrive/DOT_Traffic_Speeds_NBE.csv\"\n",
        "\n",
        "reader = pd.read_csv(path, chunksize=1_000_000)  # crea l‚Äôiteratore\n",
        "first_chunk = next(reader)                       # prende SOLO il primo pezzo\n",
        "\n",
        "first_chunk.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1db88df-326e-44f1-b303-f6f1e7e64db3",
      "metadata": {
        "id": "e1db88df-326e-44f1-b303-f6f1e7e64db3"
      },
      "source": [
        "# Il formato *parquet*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b851181-6e65-4abc-ae55-0defc23b36e8",
      "metadata": {
        "id": "6b851181-6e65-4abc-ae55-0defc23b36e8"
      },
      "source": [
        "Useremo la serie storica `usa_stocks_30m.parquet`: √® una serie OHLCV di 514 titoli del Nasdaq del NYSE (dal 1998 al 2024).\n",
        "\n",
        "Il dataset con cui lavoreremo √® un sottoinsieme del dataset [**USA 514 Stocks Prices NASDAQ NYSE**](https://www.kaggle.com/datasets/olegshpagin/usa-stocks-prices-ohlcv/data), anche disponibile su [Kaggle](https://www.kaggle.com/datasets), composto da circa **36 milioni** di elementi.\n",
        "\n",
        "Scarichiamo il dataset NON da Kaggle ma dal \"Public Google Cloud Storage bucket\" di NVIDIA, per garantire velocit√† di download maggiori.\n",
        "\n",
        "Il \"Public Google Cloud Storage bucket\" di NVIDIA √® uno spazio online dove NVIDIA mette a disposizione file pubblici (come dataset, modelli, esempi di codice) che chiunque pu√≤ scaricare.\n",
        "√à un po‚Äô come un grande armadio digitale aperto a tutti, ospitato su Google Cloud.\n",
        "\n",
        "Questo download richiede **circa 60 secondi**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "42a21e07-e9a6-41ad-a212-58c4e9591723",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T11:07:44.454739Z",
          "iopub.status.busy": "2025-10-21T11:07:44.454485Z",
          "iopub.status.idle": "2025-10-21T11:08:48.677259Z",
          "shell.execute_reply": "2025-10-21T11:08:48.676694Z",
          "shell.execute_reply.started": "2025-10-21T11:07:44.454724Z"
        },
        "id": "42a21e07-e9a6-41ad-a212-58c4e9591723",
        "outputId": "b53971e6-1a21-4fa8-b725-a4611586cf0f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musa_stocks_30m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://storage.googleapis.com/rapidsai/colab-data/usa_stocks_30m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScarico il file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m     urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlretrieve(url, file_path)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "# Download della big time series\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"usa_stocks_30m.parquet\"\n",
        "url = \"https://storage.googleapis.com/rapidsai/colab-data/usa_stocks_30m.parquet\"\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    print(f\"Scarico il file {file_path}...\")\n",
        "    urllib.request.urlretrieve(url, file_path)\n",
        "    print(\"Download completato.\")\n",
        "else:\n",
        "    print(f\"{file_path} gi√† presente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b980ec3c-2bb6-48ac-91f9-f3b51bfaf546",
      "metadata": {
        "id": "b980ec3c-2bb6-48ac-91f9-f3b51bfaf546"
      },
      "source": [
        "**Il file `usa_stocks_30m.parquet`**<br>\n",
        "\n",
        "Il file `usa_stocks_30m.parquet` √® un dataset messo a disposizione dal team RAPIDS (NVIDIA) per fare esempi di analisi su big time series finanziarie con librerie GPU-accelerate (tipo `cuDF`).\n",
        "\n",
        "üìå In pratica:\n",
        "- √à un file in **formato *Parquet*** (colonnare, compresso, molto efficiente per big data).\n",
        "- Contiene dati di **prezzi azionari USA** (titoli quotati) registrati con una **frequenza di 30 minuti**.\n",
        "- √à pensato per dimostrazioni: analisi di serie temporali, manipolazione con pandas/cuDF, benchmark CPU vs GPU.\n",
        "\n",
        "üìä Tipicamente include:\n",
        "- ticker ‚Üí il simbolo del titolo (es. AAPL, MSFT).\n",
        "- timestamp ‚Üí la data/ora della rilevazione (ogni 30 min).\n",
        "- open, high, low, close, volume (OHLCV) ‚Üí classici campi di trading.\n",
        "\n",
        "üìê Dimensioni indicative:\n",
        "- Circa **36 milioni di righe**,\n",
        "- Grandezza **~ 600‚Äì700 MB** in formato Parquet,\n",
        "- **Se convertito in CSV diventerebbe molto pi√π pesante (anche diversi GB)**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f2bcc0d",
      "metadata": {
        "id": "0f2bcc0d"
      },
      "source": [
        "üëâ Il formato Parquet **√® molto pi√π efficiente di CSV**:\n",
        "- √® binario e compresso (occupa meno spazio);\n",
        "- √® colonnare ‚Üí pandas pu√≤ leggere solo le colonne necessarie;\n",
        "- conserva i tipi di dato (niente inferenza ogni volta)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f8500e-dc26-4e34-9d62-4c479d76d7fb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T11:10:08.319709Z",
          "iopub.status.busy": "2025-10-21T11:10:08.319486Z",
          "iopub.status.idle": "2025-10-21T11:10:09.816654Z",
          "shell.execute_reply": "2025-10-21T11:10:09.816286Z",
          "shell.execute_reply.started": "2025-10-21T11:10:08.319695Z"
        },
        "id": "c8f8500e-dc26-4e34-9d62-4c479d76d7fb"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"usa_stocks_30m.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38c6387e-944b-4ac2-b893-ac306da0318b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T11:10:15.799215Z",
          "iopub.status.busy": "2025-10-21T11:10:15.799025Z",
          "iopub.status.idle": "2025-10-21T11:10:15.810892Z",
          "shell.execute_reply": "2025-10-21T11:10:15.810518Z",
          "shell.execute_reply.started": "2025-10-21T11:10:15.799201Z"
        },
        "id": "38c6387e-944b-4ac2-b893-ac306da0318b",
        "outputId": "16174d18-617d-43fb-813a-c3db214b535c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "      <th>ticker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1999-11-18 17:00:00</td>\n",
              "      <td>45.56</td>\n",
              "      <td>50.00</td>\n",
              "      <td>45.50</td>\n",
              "      <td>46.00</td>\n",
              "      <td>9275000</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1999-11-18 17:30:00</td>\n",
              "      <td>46.00</td>\n",
              "      <td>47.69</td>\n",
              "      <td>45.82</td>\n",
              "      <td>46.57</td>\n",
              "      <td>3200900</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1999-11-18 18:00:00</td>\n",
              "      <td>46.56</td>\n",
              "      <td>46.63</td>\n",
              "      <td>41.00</td>\n",
              "      <td>41.00</td>\n",
              "      <td>3830500</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1999-11-18 18:30:00</td>\n",
              "      <td>41.00</td>\n",
              "      <td>43.38</td>\n",
              "      <td>40.37</td>\n",
              "      <td>42.38</td>\n",
              "      <td>3688600</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1999-11-18 19:00:00</td>\n",
              "      <td>42.31</td>\n",
              "      <td>42.44</td>\n",
              "      <td>41.56</td>\n",
              "      <td>41.69</td>\n",
              "      <td>1584300</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             datetime   open   high    low  close   volume ticker\n",
              "0 1999-11-18 17:00:00  45.56  50.00  45.50  46.00  9275000      A\n",
              "1 1999-11-18 17:30:00  46.00  47.69  45.82  46.57  3200900      A\n",
              "2 1999-11-18 18:00:00  46.56  46.63  41.00  41.00  3830500      A\n",
              "3 1999-11-18 18:30:00  41.00  43.38  40.37  42.38  3688600      A\n",
              "4 1999-11-18 19:00:00  42.31  42.44  41.56  41.69  1584300      A"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8727b398-e653-4ea0-8325-b267c373595a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-21T11:10:21.336437Z",
          "iopub.status.busy": "2025-10-21T11:10:21.335997Z",
          "iopub.status.idle": "2025-10-21T11:10:21.340274Z",
          "shell.execute_reply": "2025-10-21T11:10:21.339849Z",
          "shell.execute_reply.started": "2025-10-21T11:10:21.336418Z"
        },
        "id": "8727b398-e653-4ea0-8325-b267c373595a",
        "outputId": "69468dbd-03e4-431c-f0df-7bb77d4bfeca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36087094, 7)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c333f1a-b878-481c-9817-64f92d8d11ea",
      "metadata": {
        "id": "7c333f1a-b878-481c-9817-64f92d8d11ea"
      },
      "source": [
        "# Pickle e Feather: poco usati?\n",
        "\n",
        "- `Parquet`: default per dati tabellari grandi ‚Üí colonnare, compresso, schema, partizionabile, cross-linguaggio (Spark, DuckDB, BigQuery, etc.).\n",
        "- `CSV`: scambio umano/‚Äúuniversale‚Äù, ma pesante e lento.\n",
        "- `Feather (Arrow IPC file)`: super-veloce per passaggi temporanei tra Python/R o per caching locale; meno features (no partizionamento, no append, poca ‚Äúevoluzione schema‚Äù).\n",
        "- `Pickle`: solo Python, non sicuro da caricare se non ti fidi della fonte, fragile tra versioni; buono per oggetti Python (modelli sklearn, liste), non per ‚Äúdati‚Äù tabellari duraturi.\n",
        "\n",
        "**`Feather` e `Pickle` sono ‚Äúpoco usati‚Äù?**\n",
        "\n",
        "`Pickle`\n",
        "- üîí Sicurezza: pickle.load pu√≤ eseguire codice ‚Üí sconsigliato per file condivisi.\n",
        "- üß¨ Portabilit√† bassa: Python-only e talvolta legato a versioni/librerie.\n",
        "- üì¶ Dati tabellari: non √® colonnare n√© compresso in modo efficiente; niente predicate pushdown.\n",
        "\n",
        "`Feather`\n",
        "- üß© Niche use: ottimo per interop Python‚ÜîR/Arrow e cache ‚Äúfast‚Äù, ma‚Ä¶\n",
        "- üß± Meno funzionalit√†: niente partizionamento, niente append, gestisce peggio ‚Äúdata lake‚Äù evolutivi.\n",
        "- üåç Ecosistemi: Spark/DB/Cloud tool spingono Parquet come standard di fatto.\n",
        "\n",
        "**Quando ha senso usarli**:\n",
        "\n",
        "`Pickle`\n",
        "- Snapshot veloci di oggetti Python (es. pipeline sklearn) per uso interno e controllato.\n",
        "- Alternative spesso migliori: joblib.dump per modelli sklearn; ONNX / PMML per portabilit√†; per dati tabellari ‚Üí Parquet.\n",
        "\n",
        "`Feather`\n",
        "- Cache locale ‚Äúfast I/O‚Äù (es. salvataggio intermedio in notebook).\n",
        "- Scambio rapido Python‚ÜîR (Arrow): pyarrow.feather / arrow::write_feather in R.\n",
        "\n",
        "Se vuoi massima velocit√† di lettura/scrittura su singolo file e non ti serve partizionare.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e139d34b-1bdd-4a69-b5c0-cf5fd9cac163",
      "metadata": {
        "id": "e139d34b-1bdd-4a69-b5c0-cf5fd9cac163"
      },
      "source": [
        "# File JSON\n",
        "\n",
        "Un formato di file **non tabellare** ma di **frequente uso** in Python √® JSON.\n",
        "\n",
        "I file JSON (estensione *.json*) sono uno dei formati pi√π usati oggi per scambiare dati tra applicazioni, **specialmente sul web e in ambito API**.\n",
        "\n",
        "üí° **In breve**\n",
        "- JSON sta per **JavaScript Object Notation**<br>\n",
        "√à un formato **testuale**, <u>leggibile da umani e facilmente interpretabile dai programmi</u>, nato da JavaScript ma oggi usato in **praticamente tutti i linguaggi** (Python, Java, C#, PHP, ecc.).\n",
        "\n",
        "üì¶ **Struttura di un file JSON**\n",
        "\n",
        "Un file JSON contiene dati organizzati come [**coppie chiave‚Äìvalore**](https://en.wikipedia.org/wiki/Name%E2%80%93value_pair), ad esempio:\n",
        "```json\n",
        "{\n",
        "  \"nome\": \"Antonio\",\n",
        "  \"eta\": 45,\n",
        "  \"iscritti\": [\"Mario\", \"Lucia\", \"Giorgio\"],\n",
        "  \"attivo\": true,\n",
        "  \"dettagli\": {\n",
        "    \"ruolo\": \"Analista\",\n",
        "    \"azienda\": \"ACI\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "üëÜ Questo √® un oggetto JSON, che contiene:\n",
        "- stringhe (\"`Antonio`\", \"`Analista`\")\n",
        "- numeri (`45`)\n",
        "- booleani (`true`)\n",
        "- liste (array) ([\"`Mario`\", \"`Lucia`\", \"`Giorgio`\"])\n",
        "- oggetti annidati (\"`dettagli\": {...}`)\n",
        "\n",
        "**In Python**<br>\n",
        "Puoi leggere o scrivere file JSON facilmente con il modulo json:\n",
        "```python\n",
        "    import json\n",
        "\n",
        "    # Lettura\n",
        "    with open(\"dati.json\", \"r\") as f:\n",
        "        dati = json.load(f)\n",
        "    print(dati[\"nome\"])  # -> Antonio\n",
        "\n",
        "    # Scrittura\n",
        "    nuovi_dati = {\"linguaggio\": \"Python\", \"versione\": 3.12}\n",
        "    with open(\"config.json\", \"w\") as f:\n",
        "        json.dump(nuovi_dati, f, indent=4)\n",
        "```\n",
        "\n",
        "**Dove si usa JSON?**\n",
        "- API REST (quasi tutte le API moderne usano JSON per scambiare dati)\n",
        "- Configurazioni (es. package.json in Node.js)\n",
        "- Database NoSQL come MongoDB (che usa BSON, una versione binaria di JSON)\n",
        "- Applicazioni web e mobile per passare dati tra frontend e backend\n",
        "\n",
        "üÜö Confronto rapido JSON vs CSV vs XML<br>\n",
        "![](json_sintesi_2.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96b2d437-b52a-4fcc-ad51-a50a03a1a58c",
      "metadata": {
        "id": "96b2d437-b52a-4fcc-ad51-a50a03a1a58c"
      },
      "source": [
        "## Conversione di un file CSV o un dizionario Python in JSON e viceversa (cio√® import/export completo)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d06205a1-a18e-4b45-a2ed-209a2f2f774e",
      "metadata": {
        "id": "d06205a1-a18e-4b45-a2ed-209a2f2f774e"
      },
      "source": [
        "\n",
        "1Ô∏è‚É£ **Dizionario Python ‚Üí JSON (scrittura)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf021f1-f963-4e3f-8f6f-c7fb1b3a7506",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:44:03.089523Z",
          "iopub.status.busy": "2025-10-24T18:44:03.089300Z",
          "iopub.status.idle": "2025-10-24T18:44:03.093787Z",
          "shell.execute_reply": "2025-10-24T18:44:03.093434Z",
          "shell.execute_reply.started": "2025-10-24T18:44:03.089509Z"
        },
        "id": "3cf021f1-f963-4e3f-8f6f-c7fb1b3a7506",
        "outputId": "835fd5ec-a5b0-4ddd-c3af-d5bdfb929d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ File JSON creato!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "dati = {\n",
        "    \"nome\": \"Antonio\",\n",
        "    \"eta\": 45,\n",
        "    \"linguaggi\": [\"Python\", \"SQL\", \"R\"],\n",
        "    \"attivo\": True\n",
        "}\n",
        "\n",
        "# Scrivi su file JSON\n",
        "with open(\"dati.json\", \"w\") as f:\n",
        "    json.dump(dati, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"‚úÖ File JSON creato!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c83db7",
      "metadata": {
        "id": "83c83db7"
      },
      "source": [
        "**Risultato** (`dati.json`):\n",
        "```json\n",
        "{\n",
        "    \"nome\": \"Antonio\",\n",
        "    \"eta\": 45,\n",
        "    \"linguaggi\": [\"Python\", \"SQL\", \"R\"],\n",
        "    \"attivo\": true\n",
        "}\n",
        "```\n",
        "\n",
        "üî∏ `indent=4` ‚Üí rende il file leggibile<br>\n",
        "üî∏ `ensure_ascii=False` ‚Üí mantiene i caratteri accentati"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a854eae7-dcde-46e2-9d28-10eb000fc34f",
      "metadata": {
        "id": "a854eae7-dcde-46e2-9d28-10eb000fc34f"
      },
      "source": [
        "2Ô∏è‚É£ **JSON ‚Üí Dizionario Python (lettura)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3b993e-98e6-4a78-8c5d-8f5465430e02",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:44:22.161337Z",
          "iopub.status.busy": "2025-10-24T18:44:22.161108Z",
          "iopub.status.idle": "2025-10-24T18:44:22.165310Z",
          "shell.execute_reply": "2025-10-24T18:44:22.164829Z",
          "shell.execute_reply.started": "2025-10-24T18:44:22.161322Z"
        },
        "id": "4c3b993e-98e6-4a78-8c5d-8f5465430e02",
        "outputId": "85a74128-deec-48ae-e3bb-e947bed84282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Antonio\n",
            "<class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open(\"dati.json\", \"r\") as f:\n",
        "    dati_letti = json.load(f)\n",
        "\n",
        "print(dati_letti[\"nome\"])     # Antonio\n",
        "print(type(dati_letti))       # dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b35add1-62ec-4a31-b0b3-0339b610e752",
      "metadata": {
        "id": "1b35add1-62ec-4a31-b0b3-0339b610e752"
      },
      "source": [
        "3Ô∏è‚É£ **CSV ‚Üí JSON**\n",
        "\n",
        "Utilizziamo il file `Credit_ISLR.csv`.<br>\n",
        "Convertiamolo in JSON:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e3a0db-ca89-418c-98b7-a947e7457f0d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:44:37.515423Z",
          "iopub.status.busy": "2025-10-24T18:44:37.515145Z",
          "iopub.status.idle": "2025-10-24T18:44:37.531262Z",
          "shell.execute_reply": "2025-10-24T18:44:37.530696Z",
          "shell.execute_reply.started": "2025-10-24T18:44:37.515406Z"
        },
        "id": "91e3a0db-ca89-418c-98b7-a947e7457f0d",
        "outputId": "03455685-a179-43ef-fe3f-de6a5c35d1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ CSV convertito in JSON!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "with open(\"Credit_ISLR.csv\", \"r\") as f_csv:\n",
        "    reader = csv.DictReader(f_csv)\n",
        "    dati = list(reader)\n",
        "\n",
        "with open(\"Credit_ISLR.json\", \"w\") as f_json:\n",
        "    json.dump(dati, f_json, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"‚úÖ CSV convertito in JSON!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94fccb97-c1a3-4b7d-95a1-eaff91f594b4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:45:11.763598Z",
          "iopub.status.busy": "2025-10-24T18:45:11.763267Z",
          "iopub.status.idle": "2025-10-24T18:45:11.767654Z",
          "shell.execute_reply": "2025-10-24T18:45:11.767036Z",
          "shell.execute_reply.started": "2025-10-24T18:45:11.763583Z"
        },
        "id": "94fccb97-c1a3-4b7d-95a1-eaff91f594b4"
      },
      "source": [
        "4Ô∏è‚É£ **JSON ‚Üí CSV**\n",
        "\n",
        "Ora facciamo l‚Äôinverso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9ca8e7-8336-41bd-b16b-b597ca3d77f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:46:42.239974Z",
          "iopub.status.busy": "2025-10-24T18:46:42.239453Z",
          "iopub.status.idle": "2025-10-24T18:46:42.247525Z",
          "shell.execute_reply": "2025-10-24T18:46:42.247163Z",
          "shell.execute_reply.started": "2025-10-24T18:46:42.239958Z"
        },
        "id": "af9ca8e7-8336-41bd-b16b-b597ca3d77f6",
        "outputId": "064bbe6f-e0a9-4607-f77a-c52bfc9d7d08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ JSON convertito in CSV!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "with open(\"Credit_ISLR.json\", \"r\") as f_json:\n",
        "    dati = json.load(f_json)\n",
        "\n",
        "with open(\"Credit_ISLR_out.csv\", \"w\", newline=\"\") as f_csv:\n",
        "    writer = csv.DictWriter(f_csv, fieldnames=dati[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(dati)\n",
        "\n",
        "print(\"‚úÖ JSON convertito in CSV!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5bc086-2d2e-4322-b78d-d57ba9b6e637",
      "metadata": {
        "id": "2c5bc086-2d2e-4322-b78d-d57ba9b6e637"
      },
      "source": [
        "![](json_sintesi.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7fb19b5-028d-4477-bae3-cc6149e75317",
      "metadata": {
        "id": "c7fb19b5-028d-4477-bae3-cc6149e75317"
      },
      "source": [
        "# Nota tecnica sui PDF\n",
        "In VSC il rendering dei file PDF √® differente da quello di Jupyter Notebook/Lab e da quello di Google Colab.<br>\n",
        "La seguente funzione `show_pdf` rileva quale IDE √® attiva e \"rende\" il PDF in modo differente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab319e3-b6e2-49d9-9221-66b650ba6f8b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-10-24T18:29:25.166858Z",
          "iopub.status.busy": "2025-10-24T18:29:25.166658Z",
          "iopub.status.idle": "2025-10-24T18:29:25.172138Z",
          "shell.execute_reply": "2025-10-24T18:29:25.171545Z",
          "shell.execute_reply.started": "2025-10-24T18:29:25.166842Z"
        },
        "id": "4ab319e3-b6e2-49d9-9221-66b650ba6f8b"
      },
      "outputs": [],
      "source": [
        "def show_pdf(pdf_path, width=1000, height=600):\n",
        "    \"\"\"\n",
        "    Mostra un PDF nel modo pi√π appropriato per l'ambiente attuale:\n",
        "    - In Jupyter: visualizza inline con IFrame.\n",
        "    - In Colab: usa IFrame (gestisce bene i file caricati).\n",
        "    - In VS Code o altri ambienti: apre nel browser predefinito.\n",
        "    \"\"\"\n",
        "    import os, webbrowser, sys\n",
        "    from pathlib import Path\n",
        "\n",
        "    pdf_path = Path(pdf_path)\n",
        "    if not pdf_path.exists():\n",
        "        raise FileNotFoundError(f\"File non trovato: {pdf_path}\")\n",
        "\n",
        "    # Rileva ambiente\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "    except NameError:\n",
        "        shell = None\n",
        "\n",
        "    if shell == 'ZMQInteractiveShell':  # Jupyter o Colab\n",
        "        from IPython.display import IFrame, display\n",
        "        display(IFrame(str(pdf_path), width=width, height=height))\n",
        "    elif \"vscode\" in sys.executable.lower() or \"vscode\" in os.getcwd().lower():\n",
        "        # Ambiente VS Code ‚Üí apre nel browser\n",
        "        webbrowser.open(pdf_path.resolve().as_uri())\n",
        "        print(f\"üìÇ PDF aperto nel browser: {pdf_path}\")\n",
        "    else:\n",
        "        # Altri ambienti (terminali, script)\n",
        "        webbrowser.open(pdf_path.resolve().as_uri())\n",
        "        print(f\"üìÇ PDF aperto nel browser: {pdf_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7edba68",
      "metadata": {
        "id": "d7edba68"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0934c5-46f1-4c9e-99f3-9e848be3782b",
      "metadata": {
        "id": "6b0934c5-46f1-4c9e-99f3-9e848be3782b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
